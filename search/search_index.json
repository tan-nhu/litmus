{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ROADMAP/","text":"LITMUS ROADMAP \u00b6 This document captures only the high level roadmap items. For the detailed backlog, see issues list . Completed \u00b6 Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure In-Progress (Under Active Development) \u00b6 Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward Backlog \u00b6 Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Roadmap"},{"location":"ROADMAP/#litmus-roadmap","text":"This document captures only the high level roadmap items. For the detailed backlog, see issues list .","title":"LITMUS ROADMAP"},{"location":"ROADMAP/#completed","text":"Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure","title":"Completed"},{"location":"ROADMAP/#in-progress-under-active-development","text":"Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward","title":"In-Progress (Under Active Development)"},{"location":"ROADMAP/#backlog","text":"Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Backlog"},{"location":"experiments/categories/getstarted/","text":"Kubernetes Experiments \u00b6 It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available: Generic \u00b6 Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart Pod Chaos \u00b6 Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss Node Chaos \u00b6 Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint Application Chaos \u00b6 While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete Cloud Platforms \u00b6 Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available: AWS \u00b6 Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag GCP \u00b6 Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss Azure \u00b6 Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop VMWare \u00b6 Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"Contents"},{"location":"experiments/categories/getstarted/#kubernetes-experiments","text":"It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available:","title":"Kubernetes Experiments"},{"location":"experiments/categories/getstarted/#generic","text":"Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart","title":"Generic"},{"location":"experiments/categories/getstarted/#pod-chaos","text":"Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss","title":"Pod Chaos"},{"location":"experiments/categories/getstarted/#node-chaos","text":"Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint","title":"Node Chaos"},{"location":"experiments/categories/getstarted/#application-chaos","text":"While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete","title":"Application Chaos"},{"location":"experiments/categories/getstarted/#cloud-platforms","text":"Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available:","title":"Cloud Platforms"},{"location":"experiments/categories/getstarted/#aws","text":"Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag","title":"AWS"},{"location":"experiments/categories/getstarted/#gcp","text":"Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss","title":"GCP"},{"location":"experiments/categories/getstarted/#azure","text":"Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop","title":"Azure"},{"location":"experiments/categories/getstarted/#vmware","text":"Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"VMWare"},{"location":"experiments/categories/aws/AWS-experiments-tunables/","text":"It contains the AWS specific experiment tunables. Managed Nodegroup \u00b6 It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"AWS experiments tunables"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#managed-nodegroup","text":"It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Managed Nodegroup"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws/ebs-loss-by-id/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_ID Comma separated list of volume IDs subjected to ebs detach chaos Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Detach Volumes By ID \u00b6 It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By ID"},{"location":"experiments/categories/aws/ebs-loss-by-id/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-id/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#detach-volumes-by-id","text":"It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By ID"},{"location":"experiments/categories/aws/ebs-loss-by-tag/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_TAG Provide the common tag for target volumes. It'll be in form of key:value (Ex: 'team:devops') Optional Fields Variables Description Notes VOLUME_AFFECTED_PERC The Percentage of total ebs volumes to target Defaults to 0 (corresponds to 1 volume), provide numeric value only TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single volume \u00b6 It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of volumes \u00b6 It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By Tag"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-single-volume","text":"It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single volume"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-percent-of-volumes","text":"It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of volumes"},{"location":"experiments/categories/aws/ec2-terminate-by-id/","text":"Introduction \u00b6 It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Stop Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#introduction","text":"It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#stop-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/","text":"Introduction \u00b6 It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes INSTANCE_TAG Instance Tag to filter the target ec2 instance. The INSTANCE_TAG should be provided as key:value ex: team:devops Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single instance \u00b6 It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By Tag"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#introduction","text":"It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-single-instance","text":"It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-percent-of-instances","text":"It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/","text":"It contains the aws-ssm specific experiment tunables. CPU Cores \u00b6 It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Percentage \u00b6 It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSM Docs \u00b6 It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers Count \u00b6 It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"AWS SSM experiments tunables"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#cpu-cores","text":"It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"CPU Cores"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#memory-percentage","text":"It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Percentage"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#ssm-docs","text":"It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSM Docs"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#workers-count","text":"It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers Count"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/","text":"Introduction \u00b6 AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Stress Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#introduction","text":"AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#stress-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stress Instances By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/","text":"Introduction \u00b6 AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_TAG Instance Tag to filter the target ec2 instance The EC2_INSTANCE_TAG should be provided as key:value ex: chaos:ssm Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Target single instance \u00b6 It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By Tag"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#introduction","text":"AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-single-instance","text":"It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-percent-of-instances","text":"It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/azure/azure-instance-stop/","text":"Introduction \u00b6 It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations Azure instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes AZURE_INSTANCE_NAME Instance name of the target azure instance RESOURCE_GROUP The resource group of the target instance Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance poweroff. Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop Instances By Name \u00b6 It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azore instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Azure Instance Stop"},{"location":"experiments/categories/azure/azure-instance-stop/#introduction","text":"It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance","title":"Introduction"},{"location":"experiments/categories/azure/azure-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/azure/azure-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/azure/azure-instance-stop/#default-validations","text":"View the default validations Azure instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/azure/azure-instance-stop/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/azure/azure-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#stop-instances-by-name","text":"It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azore instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By Name"},{"location":"experiments/categories/azure/azure-instance-stop/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CASSANDRA_SVC_NAME Cassandra Service Name Defaults value: cassandra KEYSPACE_REPLICATION_FACTOR Value of the Replication factor for the cassandra liveness deploy It needs to create keyspace while checking the livenss of cassandra CASSANDRA_PORT Port of the cassandra statefulset Defaults value: 9042 CASSANDRA_LIVENESS_CHECK It allows to check the liveness of the cassandra statefulset It can be enabled or disabled CASSANDRA_LIVENESS_IMAGE Image of the cassandra liveness deployment Default value: litmuschaos/cassandra-client:latest SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0% (corresponds to 1 replica) CHAOS_INTERVAL Time interval b/w two successive pod failures (sec) Defaults to 5s LIB The chaos lib used to inject the chaos Defaults to litmus . Supported litmus only FORCE Application Pod deletion mode. False indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Cassandra App Details \u00b6 It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Force Delete \u00b6 The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of cassandra \u00b6 The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra Pod Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#introduction","text":"It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod","title":"Introduction"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#default-validations","text":"View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas.","title":"Default Validations"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#cassandra-app-details","text":"It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra App Details"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#force-delete","text":"The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Force Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#liveness-check-of-cassandra","text":"The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of cassandra"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/","text":"It contains tunables, which are common for all the experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Duration of the chaos \u00b6 It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Ramp Time \u00b6 It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60' Sequence of chaos execution \u00b6 It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: - parallel : The chaos is injected in all the targets at once. - serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60' Name of chaos library \u00b6 It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60' Instance ID \u00b6 It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60' Image used by the helper pod \u00b6 It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for all experiments"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#duration-of-the-chaos","text":"It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Duration of the chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#ramp-time","text":"It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Ramp Time"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#sequence-of-chaos-execution","text":"It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: - parallel : The chaos is injected in all the targets at once. - serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Sequence of chaos execution"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#name-of-chaos-library","text":"It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Name of chaos library"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#instance-id","text":"It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Instance ID"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#image-used-by-the-helper-pod","text":"It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Image used by the helper pod"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/","text":"Introduction \u00b6 It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations Disk volumes are attached to their respective instances Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID The ID of the GCP Project of which the disk volumes are a part of All the target disk volumes should belong to a single GCP Project DISK_VOLUME_NAMES Target non-boot persistent disk volume names Multiple disk volume names can be provided as disk1,disk2,... DISK_ZONES The zones of respective target disk volumes Provide the zone for every target disk name as zone1,zone2... in the respective order of DISK_VOLUME_NAMES DEVICE_NAMES The device names of respective target disk volumes Provide the device name for every target disk name as deviceName1,deviceName2... in the respective order of DISK_VOLUME_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between the successive chaos iterations (sec) Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Detach Volumes By Names \u00b6 It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Disk Loss"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#introduction","text":"It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#default-validations","text":"View the default validations Disk volumes are attached to their respective instances","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#detach-volumes-by-names","text":"It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By Names"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/","text":"Introduction \u00b6 It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID GCP project ID to which the VM instances belong All the VM instances must belong to a single GCP project VM_INSTANCE_NAMES Name of target VM instances Multiple instance names can be provided as instance1,instance2,... INSTANCE_ZONES The zones of the target VM instances Zone for every instance name has to be provided as zone1,zone2,... in the same order of VM_INSTANCE_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination Defaults to 30s AUTO_SCALING_GROUP Set to enable if the target instance is the part of a auto-scaling group Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Target GCP Instances \u00b6 It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Autoscaling NodeGroup \u00b6 If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Instance Stop"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#introduction","text":"It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#target-gcp-instances","text":"It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target GCP Instances"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#autoscaling-nodegroup","text":"If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Autoscaling NodeGroup"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored. Default Validations \u00b6 View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes KAFKA_NAMESPACE Namespace of Kafka Brokers May be same as value for spec.appinfo.appns KAFKA_LABEL Unique label of Kafka Brokers May be same as value for spec.appinfo.applabel KAFKA_SERVICE Headless service of the Kafka Statefulset KAFKA_PORT Port of the Kafka ClusterIP service ZOOKEEPER_NAMESPACE Namespace of Zookeeper Cluster May be same as value for KAFKA_NAMESPACE or other ZOOKEEPER_LABEL Unique label of Zokeeper statefulset ZOOKEEPER_SERVICE Headless service of the Zookeeper Statefulset ZOOKEEPER_PORT Port of the Zookeeper ClusterIP service Optional Fields Variables Description Notes KAFKA_BROKER Kafka broker pod (name) to be deleted A target selection mode (random/liveness-based/specific) KAFKA_KIND Kafka deployment type Same as spec.appinfo.appkind . Supported: statefulset KAFKA_LIVENESS_STREAM Kafka liveness message stream Supported: enabled , disabled KAFKA_LIVENESS_IMAGE Image used for liveness message stream Set the liveness image as <registry_url>/<repository>:<image-tag> KAFKA_REPLICATION_FACTOR Number of partition replicas for liveness topic partition Necessary if KAFKA_LIVENESS_STREAM is enabled . The replication factor should be less than or equal to number of Kafka brokers KAFKA_INSTANCE_NAME Name of the Kafka chroot path on zookeeper Necessary if installation involves use of such path KAFKA_CONSUMER_TIMEOUT Kafka consumer message timeout, post which it terminates Defaults to 30000ms, Recommended timeout for EKS platform: 60000 ms TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s CHAOS_INTERVAL Time interval b/w two successive broker failures (sec) Defaults to 5s Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Kafka And Zookeeper App Details \u00b6 It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of kafka \u00b6 The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Kafka Broker Pod Failure"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#introduction","text":"It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod","title":"Introduction"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored.","title":"Prerequisites"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#default-validations","text":"View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken","title":"Default Validations"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#kafka-and-zookeeper-app-details","text":"It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kafka And Zookeeper App Details"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#liveness-check-of-kafka","text":"The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of kafka"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/","text":"It contains tunables, which are common for all the node experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Single Node \u00b6 It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Multiple Nodes \u00b6 It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Nodes With Labels \u00b6 It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Node Affected Percentage \u00b6 It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for node experiments"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-single-node","text":"It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Single Node"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-multiple-nodes","text":"It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Multiple Nodes"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-nodes-with-labels","text":"It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Nodes With Labels"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#node-affected-percentage","text":"It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Affected Percentage"},{"location":"experiments/categories/nodes/docker-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Docker Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Docker Service Kill"},{"location":"experiments/categories/nodes/docker-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/docker-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/docker-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/docker-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/docker-service-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/docker-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#kill-docker-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Docker Service"},{"location":"experiments/categories/nodes/kubelet-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE The lib image used to inject kubelet kill chaos the image should have systemd installed in it. Defaults to ubuntu:16.04 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Kubelet Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kubelet Service Kill"},{"location":"experiments/categories/nodes/kubelet-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/kubelet-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/kubelet-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/kubelet-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/kubelet-service-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/kubelet-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#kill-kubelet-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Kubelet Service"},{"location":"experiments/categories/nodes/node-cpu-hog/","text":"Introduction \u00b6 This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node cpu hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE Image used to run the stress command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before & after injection of chaos in sec Optional NODE_CPU_CORE Number of cores of node CPU to be consumed Defaults to 2 NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Node CPU Cores \u00b6 It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Hog"},{"location":"experiments/categories/nodes/node-cpu-hog/#introduction","text":"This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-cpu-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-cpu-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-cpu-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-cpu-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#node-cpu-cores","text":"It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Cores"},{"location":"experiments/categories/nodes/node-drain/","text":"Introduction \u00b6 It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Drain Node \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Drain"},{"location":"experiments/categories/nodes/node-drain/#introduction","text":"It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-drain/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-drain/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-drain/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-drain/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-drain/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-drain/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-drain/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-drain/#drain-node","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Drain Node"},{"location":"experiments/categories/nodes/node-io-stress/","text":"Introduction \u00b6 This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node io stress chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120 FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. CPU Number of core of CPU to be used Default to 1 NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 VM_WORKERS It is the number vm workers involved in IO disk stress Default to 1 LIB The chaos lib used to inject the chaos Default to litmus LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Limit CPU Utilization \u00b6 The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this: Workers For Stress \u00b6 The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node IO Stress"},{"location":"experiments/categories/nodes/node-io-stress/#introduction","text":"This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node","title":"Introduction"},{"location":"experiments/categories/nodes/node-io-stress/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-io-stress/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-io-stress/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-io-stress/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/nodes/node-io-stress/#limit-cpu-utilization","text":"The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this:","title":"Limit CPU Utilization"},{"location":"experiments/categories/nodes/node-io-stress/#workers-for-stress","text":"The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-memory-hog/","text":"Introduction \u00b6 This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node memory hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in seconds) Optional Defaults to 120 LIB The chaos lib used to inject the chaos Optional Defaults to litmus LIB_IMAGE Image used to run the stress command Optional Defaults to litmuschaos/go-runner:latest MEMORY_CONSUMPTION_PERCENTAGE Percent of the total node memory capacity Optional Defaults to 30 MEMORY_CONSUMPTION_MEBIBYTES The size in Mebibytes of total available memory. When using this we need to keep MEMORY_CONSUMPTION_PERCENTAGE empty as the percentage have more precedence Optional NUMBER_OF_WORKERS It is the number of VM workers involved in IO disk stress Optional Default to 1 RAMP_TIME Period to wait before and after injection of chaos in sec Optional NODES_AFFECTED_PERC The Percentage of total nodes to target Optional Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Memory Consumption Percentage \u00b6 It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Consumption Mebibytes \u00b6 It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Memory Hog"},{"location":"experiments/categories/nodes/node-memory-hog/#introduction","text":"This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-memory-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-memory-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-memory-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-memory-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-percentage","text":"It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Percentage"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-mebibytes","text":"It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Mebibytes"},{"location":"experiments/categories/nodes/node-memory-hog/#workers-for-stress","text":"The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-restart/","text":"Introduction \u00b6 It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment. Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of target node, subjected to chaos. If not provided it will select the random node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes LIB_IMAGE The image used to restart the node Defaults to litmuschaos/go-runner:latest SSH_USER name of ssh user Defaults to root TARGET_NODE_IP Internal IP of the target node, subjected to chaos. If not provided, the experiment will lookup the node IP of the TARGET_NODE node Defaults to empty REBOOT_COMMAND Command used for reboot Defaults to sudo systemctl reboot TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Reboot Command \u00b6 It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSH User \u00b6 It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Node Internal IP \u00b6 It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Restart"},{"location":"experiments/categories/nodes/node-restart/#introduction","text":"It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-restart/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-restart/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment.","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-restart/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-restart/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-restart/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-restart/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-restart/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-restart/#reboot-command","text":"It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Reboot Command"},{"location":"experiments/categories/nodes/node-restart/#ssh-user","text":"It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSH User"},{"location":"experiments/categories/nodes/node-restart/#target-node-internal-ip","text":"It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Node Internal IP"},{"location":"experiments/categories/nodes/node-taint/","text":"Introduction \u00b6 It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE TAINT_LABEL Label and effect to be tainted on application node Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Taint Label \u00b6 It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Taint"},{"location":"experiments/categories/nodes/node-taint/#introduction","text":"It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-taint/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-taint/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-taint/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-taint/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-taint/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-taint/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-taint/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-taint/#taint-label","text":"It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Taint Label"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/","text":"It contains tunables, which are common for all pod-level experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Specific Pods \u00b6 It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pod Affected Percentage \u00b6 It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for pod experiments"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-pods","text":"It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Pods"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#pod-affected-percentage","text":"It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Affected Percentage"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Container"},{"location":"experiments/categories/pods/container-kill/","text":"Introduction \u00b6 It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER The name of container to be killed inside the pod If the TARGET_CONTAINER is not provided it will delete the first container CHAOS_INTERVAL Time interval b/w two successive container kill (in sec) If the CHAOS_INTERVAL is not provided it will take the default value of 10s TOTAL_CHAOS_DURATION The time duration for chaos injection (seconds) Defaults to 20s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only TARGET_PODS Comma separated list of application pod name subjected to container kill chaos If not provided, it will select target pods randomly based on provided appLabels LIB_IMAGE LIB Image used to kill the container Defaults to litmuschaos/go-runner:latest LIB The category of lib use to inject chaos Default value: litmus, supported values: pumba and litmus RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel SIGNAL It contains termination signal used for container kill Default value: SIGKILL SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Kill Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Signal For Kill \u00b6 It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Kill"},{"location":"experiments/categories/pods/container-kill/#introduction","text":"It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container","title":"Introduction"},{"location":"experiments/categories/pods/container-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/container-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/container-kill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/container-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/container-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/container-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/container-kill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/container-kill/#kill-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Specific Container"},{"location":"experiments/categories/pods/container-kill/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/container-kill/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/container-kill/#signal-for-kill","text":"It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Signal For Kill"},{"location":"experiments/categories/pods/container-kill/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/disk-fill/","text":"Introduction \u00b6 It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes FILL_PERCENTAGE Percentage to fill the Ephemeral storage limit Can be set to more than 100 also, to force evict the pod. The ephemeral-storage limits must be set in targeted pod to use this ENV. EPHEMERAL_STORAGE_MEBIBYTES Ephemeral storage which need to fill (unit: MiBi) It is mutually exclusive with the FILL_PERCENTAGE ENV. If both are provided then it will use the FILL_PERCENTAGE Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to disk-fill If not provided, the first container in the targeted pod will be subject to chaos CONTAINER_PATH Storage Location of containers Defaults to '/var/lib/docker/containers' TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 60s TARGET_PODS Comma separated list of application pod name subjected to disk fill chaos If not provided, it will select target pods randomly based on provided appLabels DATA_BLOCK_SIZE It contains data block size used to fill the disk(in KB) Defaults to 256, it supports unit as KB only PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only LIB_IMAGE The image used to fill the disk Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Disk Fill Percentage \u00b6 It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Disk Fill Mebibytes \u00b6 It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Data Block Size \u00b6 It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Path \u00b6 It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill"},{"location":"experiments/categories/pods/disk-fill/#introduction","text":"It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage","title":"Introduction"},{"location":"experiments/categories/pods/disk-fill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/disk-fill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\"","title":"Prerequisites"},{"location":"experiments/categories/pods/disk-fill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/disk-fill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/disk-fill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/disk-fill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/disk-fill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-percentage","text":"It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Percentage"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-mebibytes","text":"It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Mebibytes"},{"location":"experiments/categories/pods/disk-fill/#data-block-size","text":"It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Data Block Size"},{"location":"experiments/categories/pods/disk-fill/#container-path","text":"It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Path"},{"location":"experiments/categories/pods/pod-autoscaler/","text":"Introduction \u00b6 The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes REPLICA_COUNT Number of replicas upto which we want to scale nil Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The timeout for the chaos experiment (in seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Replica counts \u00b6 It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Autoscaler"},{"location":"experiments/categories/pods/pod-autoscaler/#introduction","text":"The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas","title":"Introduction"},{"location":"experiments/categories/pods/pod-autoscaler/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-autoscaler/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-autoscaler/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-autoscaler/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-autoscaler/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#replica-counts","text":"It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Replica counts"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Default to litmus TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CHAOS_INJECT_COMMAND The command to inject the cpu chaos Default to md5sum /dev/zero CHAOS_KILL_COMMAND The command to kill the chaos process Default to kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}') . Another useful one that generally works (in case the default doesn't) is kill -9 \\((ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print\\) 1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Chaos Inject and Kill Commands \u00b6 It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: - CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog Exec"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#chaos-inject-and-kill-commands","text":"It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: - CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Inject and Kill Commands"},{"location":"experiments/categories/pods/pod-cpu-hog/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Default to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog"},{"location":"experiments/categories/pods/pod-cpu-hog/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-cpu-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-delete/","text":"Introduction \u00b6 It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod Uses \u00b6 View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application. Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in sec) Defaults to 15s, NOTE: Overall run duration of the experiment may exceed the TOTAL_CHAOS_DURATION by a few min CHAOS_INTERVAL Time interval b/w two successive pod failures (in sec) Defaults to 5s RANDOMNESS Introduces randomness to pod deletions with a minimum period defined by CHAOS_INTERVAL It supports true or false. Default value: false FORCE Application Pod deletion mode. false indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 TARGET_PODS Comma separated list of application pod name subjected to pod delete chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Force Delete \u00b6 The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Random Interval \u00b6 The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. - If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. - If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Pod Delete"},{"location":"experiments/categories/pods/pod-delete/#introduction","text":"It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-delete/#uses","text":"View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application.","title":"Uses"},{"location":"experiments/categories/pods/pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-delete/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-delete/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-delete/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-delete/#force-delete","text":"The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Force Delete"},{"location":"experiments/categories/pods/pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/pod-delete/#random-interval","text":"The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. - If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. - If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Random Interval"},{"location":"experiments/categories/pods/pod-dns-error/","text":"Introduction \u00b6 Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns-error None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_HOSTNAMES List of the target hostnames or keywords eg. '[\"litmuschaos\",\"chaosnative.com\"]' If not provided, all hostnames/domains will be targeted MATCH_SCHEME Determines whether the dns query has to match exactly with one of the targets or can have any of the targets as substring. Can be either exact or substring if not provided, it will be set as exact PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Target Host Names \u00b6 It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60' Match Scheme \u00b6 It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Error"},{"location":"experiments/categories/pods/pod-dns-error/#introduction","text":"Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-error/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-error/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-error/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-error/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-error/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-error/#target-host-names","text":"It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Target Host Names"},{"location":"experiments/categories/pods/pod-dns-error/#match-scheme","text":"It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Match Scheme"},{"location":"experiments/categories/pods/pod-dns-error/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-dns-spoof/","text":"Introduction \u00b6 Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns spoof None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) SPOOF_MAP Map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. If not provided, no hostnames/domains will be spoofed PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Spoof Map \u00b6 It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Spoof"},{"location":"experiments/categories/pods/pod-dns-spoof/#introduction","text":"Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-spoof/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-spoof/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-spoof/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-spoof/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-spoof/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#spoof-map","text":"It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Spoof Map"},{"location":"experiments/categories/pods/pod-dns-spoof/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/","text":"Introduction \u00b6 This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120s VOLUME_MOUNT_PATH Fill the given volume mount path LIB The chaos lib used to inject the chaos Default to litmus . Available litmus and pumba. LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest TARGET_PODS Comma separated list of application pod name subjected to pod io stress chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mount Path \u00b6 The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod IO Stress"},{"location":"experiments/categories/pods/pod-io-stress/#introduction","text":"This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-io-stress/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-io-stress/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-io-stress/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-io-stress/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/pods/pod-io-stress/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/#mount-path","text":"The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Mount Path"},{"location":"experiments/categories/pods/pod-io-stress/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-io-stress/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-memory-hog-exec/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB (Up to 2000MB) TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Defaults to litmus TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod CHAOS_KILL_COMMAND The command to kill the chaos process Defaults to kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1) . Another useful one that generally works (in case the default doesn't) is kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Chaos Kill Commands \u00b6 It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog Exec"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#chaos-kill-commands","text":"It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Kill Commands"},{"location":"experiments/categories/pods/pod-memory-hog/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB (Up to 2000MB) NUMBER_OF_WORKERS The number of workers used to run the stress process Defaults to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Defaults to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos. If not provided, it will select the first container of the target pod CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog"},{"location":"experiments/categories/pods/pod-memory-hog/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-memory-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path.","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-memory-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-corruption/","text":"Introduction \u00b6 It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network corruption Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_CORRUPTION_PERCENTAGE Packet corruption in percentage Default (100) CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Corruption \u00b6 It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#introduction","text":"It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-corruption/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-corruption/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-corruption/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-corruption/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-corruption/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#network-packet-corruption","text":"It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-corruption/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-corruption/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-corruption/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-duplication/","text":"Introduction \u00b6 It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_DUPLICATION_PERCENTAGE The packet duplication in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Duplication \u00b6 It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#introduction","text":"It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-duplication/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-duplication/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-duplication/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-duplication/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-duplication/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#network-packet-duplication","text":"It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-duplication/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-duplication/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-duplication/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-latency/","text":"Introduction \u00b6 It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_LATENCY The latency/delay in milliseconds Optional Default 2000, provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Latency \u00b6 It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#introduction","text":"It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-latency/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-latency/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-latency/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-latency/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-latency/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-latency/#network-latency","text":"It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-latency/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-latency/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-latency/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-loss/","text":"Introduction \u00b6 It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network loss Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_LOSS_PERCENTAGE The packet loss in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Loss \u00b6 It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Loss"},{"location":"experiments/categories/pods/pod-network-loss/#introduction","text":"It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-loss/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-loss/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-loss/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-loss/#network-packet-loss","text":"It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Loss"},{"location":"experiments/categories/pods/pod-network-loss/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-loss/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-loss/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-loss/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/vmware/vm-poweroff/","text":"Introduction \u00b6 It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable. Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes APP_VM_MOID Moid of the vmware instance Once you open VM in vCenter WebClient, you can find MOID in address field (VirtualMachine:vm-5365). Eg: vm-5365 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop/Poweroff VM By MOID \u00b6 It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"VM Poweroff"},{"location":"experiments/categories/vmware/vm-poweroff/#introduction","text":"It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm","title":"Introduction"},{"location":"experiments/categories/vmware/vm-poweroff/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/vmware/vm-poweroff/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable.","title":"Prerequisites"},{"location":"experiments/categories/vmware/vm-poweroff/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/vmware/vm-poweroff/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/vmware/vm-poweroff/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#stoppoweroff-vm-by-moid","text":"It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop/Poweroff VM By MOID"},{"location":"experiments/chaos-resources/application-details/","text":"It defines the appns , applabel , and appkind to set the namespace, labels, and kind of the application under test. - appkind : It supports deployment , statefulset , daemonset , deploymentconfig , and rollout . It is mandatory for the pod-level experiments and optional for the rest of the experiments. Use the following example to tune this: # contains details of the AUT(application under test) # appns: name of the application # applabel: label of the applicaton # appkind: kind of the application. supports: deployment, statefulset, daemonset, rollout, deploymentconfig apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # AUT details appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Auxiliary Application Info \u00b6 The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Application Specifications"},{"location":"experiments/chaos-resources/application-details/#auxiliary-application-info","text":"The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Auxiliary Application Info"},{"location":"experiments/chaos-resources/chaos-resources/","text":"Chaos Engine Tunables \u00b6 The ChaosEngine is the main user-facing chaos custom resource with a namespace scope and is designed to hold information around how the chaos experiments are executed. It connects an application instance with one or more chaos experiments, while allowing the users to specify run level details (override experiment defaults, provide new environment variables and volumes, options to delete or retain experiment pods, etc.,). This CR is also updated/patched with status of the chaos experiments, making it the single source of truth with respect to the chaos. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same. ChaosEngine Spec Specification \u00b6 Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications Probes \u00b6 Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe","title":"Contents"},{"location":"experiments/chaos-resources/chaos-resources/#chaos-engine-tunables","text":"The ChaosEngine is the main user-facing chaos custom resource with a namespace scope and is designed to hold information around how the chaos experiments are executed. It connects an application instance with one or more chaos experiments, while allowing the users to specify run level details (override experiment defaults, provide new environment variables and volumes, options to delete or retain experiment pods, etc.,). This CR is also updated/patched with status of the chaos experiments, making it the single source of truth with respect to the chaos. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same.","title":"Chaos Engine Tunables"},{"location":"experiments/chaos-resources/chaos-resources/#chaosengine-spec-specification","text":"Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications","title":"ChaosEngine Spec Specification"},{"location":"experiments/chaos-resources/chaos-resources/#probes","text":"Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe","title":"Probes"},{"location":"experiments/chaos-resources/engine-state/","text":"It is a user-defined flag to trigger chaos. Setting it to active ensures the successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on the actual state of the ChaosEngine. It can be tuned via engineState field. It supports active and stop values. Use the following example to tune this: # contains the chaosengine state # supports: active and stop states apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : # contains the state of engine engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"State Specifications"},{"location":"experiments/chaos-resources/experiment-components/","text":"It contains all the experiment tunables provided at .spec.experiments[].spec.components inside chaosengine. Experiment Annotations \u00b6 It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment Experiment Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. - configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" Experiment Image \u00b6 It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\" Experiment ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred Experiment NodeSelectors \u00b6 The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos Experiment Resource Requirements \u00b6 It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" Experiment Tolerations \u00b6 It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" Experiment Status Check Timeout \u00b6 It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Specifications"},{"location":"experiments/chaos-resources/experiment-components/#experiment-annotations","text":"It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment","title":"Experiment Annotations"},{"location":"experiments/chaos-resources/experiment-components/#experiment-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. - configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\"","title":"Experiment Configmaps And Secrets"},{"location":"experiments/chaos-resources/experiment-components/#experiment-image","text":"It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\"","title":"Experiment Image"},{"location":"experiments/chaos-resources/experiment-components/#experiment-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred","title":"Experiment ImagePullSecrets"},{"location":"experiments/chaos-resources/experiment-components/#experiment-nodeselectors","text":"The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos","title":"Experiment NodeSelectors"},{"location":"experiments/chaos-resources/experiment-components/#experiment-resource-requirements","text":"It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\"","title":"Experiment Resource Requirements"},{"location":"experiments/chaos-resources/experiment-components/#experiment-tolerations","text":"It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\"","title":"Experiment Tolerations"},{"location":"experiments/chaos-resources/experiment-components/#experiment-status-check-timeout","text":"It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Status Check Timeout"},{"location":"experiments/chaos-resources/rbac-details/","text":"It specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment are provided in the .spec.definition.permissions field of the respective chaosexperiment CR. It can be tuned via chaosServiceAccount field. Use the following example to tune this: # contains name of the serviceAccount which contains all the RBAC permissions required for the experiment apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" # name of the service account w/ sufficient permissions chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"RBAC Specifications"},{"location":"experiments/chaos-resources/runner-components/","text":"It contains all the chaos-runner tunables provided at .spec.components.runner inside chaosengine. ChaosRunner Annotations \u00b6 It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Args And Command \u00b6 It defines the args and command to set the args and command of the chaos-runner respectively. - args : It allows developers to specify their own debug runner args. - command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. - configMaps : It provides for a means to insert config information into the runner pod. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Image and ImagePullPoicy \u00b6 It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. - image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . - imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner NodeSelectors \u00b6 The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Resource Requirements \u00b6 It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Tolerations \u00b6 It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runner Specifications"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-annotations","text":"It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Annotations"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-args-and-command","text":"It defines the args and command to set the args and command of the chaos-runner respectively. - args : It allows developers to specify their own debug runner args. - command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Args And Command"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. - configMaps : It provides for a means to insert config information into the runner pod. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Configmaps And Secrets"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-image-and-imagepullpoicy","text":"It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. - image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . - imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Image and ImagePullPoicy"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner ImagePullSecrets"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-nodeselectors","text":"The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner NodeSelectors"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-resource-requirements","text":"It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Resource Requirements"},{"location":"experiments/chaos-resources/runner-components/#chaosrunner-tolerations","text":"It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Tolerations"},{"location":"experiments/chaos-resources/runtime-details/","text":"Annotation Check \u00b6 It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Jobcleanup Policy \u00b6 It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanupPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Termination Grace Period Seconds \u00b6 It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runtime Specifications"},{"location":"experiments/chaos-resources/runtime-details/#annotation-check","text":"It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Annotation Check"},{"location":"experiments/chaos-resources/runtime-details/#jobcleanup-policy","text":"It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanupPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Jobcleanup Policy"},{"location":"experiments/chaos-resources/runtime-details/#termination-grace-period-seconds","text":"It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Termination Grace Period Seconds"},{"location":"experiments/chaos-resources/probes/cmdProbe/","text":"The command probe allows developers to run shell commands and match the resulting output as part of the entry/exit criteria. The intent behind this probe was to allow users to implement a non-standard & imperative way of expressing their hypothesis. For example, the cmdProbe enables you to check for specific data within a database, parse the value out of a JSON blob being dumped into a certain path, or check for the existence of a particular string in the service logs. It can be executed by setting type as cmdProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Inline Mode \u00b6 In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5 Source Mode \u00b6 In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Command Probe"},{"location":"experiments/chaos-resources/probes/cmdProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/cmdProbe/#inline-mode","text":"In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Inline Mode"},{"location":"experiments/chaos-resources/probes/cmdProbe/#source-mode","text":"In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Source Mode"},{"location":"experiments/chaos-resources/probes/httpProbe/","text":"The http probe allows developers to specify a URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. The received status code is mapped against an expected status. It supports http Get and Post methods. It can be executed by setting type as httpProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. HTTP Get Request \u00b6 In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a simple) \u00b6 It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a complex) \u00b6 In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Response Timout \u00b6 It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Skip Certification Check \u00b6 It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Probe"},{"location":"experiments/chaos-resources/probes/httpProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-get-request","text":"In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Get Request"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-simple","text":"It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a simple)"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-complex","text":"In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a complex)"},{"location":"experiments/chaos-resources/probes/httpProbe/#response-timout","text":"It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Response Timout"},{"location":"experiments/chaos-resources/probes/httpProbe/#skip-certification-check","text":"It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Skip Certification Check"},{"location":"experiments/chaos-resources/probes/k8sProbe/","text":"With the proliferation of custom resources & operators, especially in the case of stateful applications, the steady-state is manifested as status parameters/flags within Kubernetes resources. k8sProbe addresses verification of the desired resource state by allowing users to define the Kubernetes GVR (group-version-resource) with appropriate filters (field selectors/label selectors). The experiment makes use of the Kubernetes Dynamic Client to achieve this. It supports CRUD operations which can be defined at probe.k8sProbe/inputs.operation . It can be executed by setting type as k8sProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Create Operation \u00b6 It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi Delete Operation \u00b6 It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Present Operation \u00b6 It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Absent Operation \u00b6 It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"K8S Probe"},{"location":"experiments/chaos-resources/probes/k8sProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/k8sProbe/#create-operation","text":"It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi","title":"Create Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#delete-operation","text":"It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Delete Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#present-operation","text":"It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Present Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#absent-operation","text":"It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Absent Operation"},{"location":"experiments/chaos-resources/probes/litmus-probes/","text":"Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). It can be provided at .spec.experiments[].spec.probe inside chaosengine. It supports four types: cmdProbe , k8sProbe , httpProbe , and promProbe . Probe Modes \u00b6 The probes can be set up to run in five different modes. Which can be tuned via mode ENV. - SOT : Executed at the Start of the Test as a pre-chaos check - EOT : Executed at the End of the Test as a post-chaos check - Edge : Executed both, before and after the chaos - Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. - OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Run Properties \u00b6 All probes share some common attributes. Which can be tuned via runProperties ENV. - probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. - retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. - interval : The period between subsequent retries - probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2 Initial Delay Seconds \u00b6 It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds Stop/Continue Experiment On Probe Failure \u00b6 It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true Probe Chaining \u00b6 Probe chaining enables reuse of probe a result (represented by the template function {{ .&gt;probeName&lt;.probeArtifact.Register}}) in subsequent \"downstream\" probes defined in the ChaosEngine. Note : The order of execution of probes in the experiment depends purely on the order in which they are defined in the ChaosEngine. Use the following example to tune this: # chaining enables reuse of probe's result (represented by the template function {{ <probeName>.probeArtifact.Register}}) #-- in subsequent \"downstream\" probes defined in the ChaosEngine. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"probe1\" type : \"cmdProbe\" cmdProbe/inputs : command : \"<command>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1 - name : \"probe2\" type : \"cmdProbe\" cmdProbe/inputs : ## probe1's result being used as one of the args in probe2 command : \"<commmand> {{ .probe1.ProbeArtifacts.Register }} <arg2>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Litmus probes"},{"location":"experiments/chaos-resources/probes/litmus-probes/#probe-modes","text":"The probes can be set up to run in five different modes. Which can be tuned via mode ENV. - SOT : Executed at the Start of the Test as a pre-chaos check - EOT : Executed at the End of the Test as a post-chaos check - Edge : Executed both, before and after the chaos - Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. - OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Probe Modes"},{"location":"experiments/chaos-resources/probes/litmus-probes/#run-properties","text":"All probes share some common attributes. Which can be tuned via runProperties ENV. - probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. - retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. - interval : The period between subsequent retries - probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2","title":"Run Properties"},{"location":"experiments/chaos-resources/probes/litmus-probes/#initial-delay-seconds","text":"It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds","title":"Initial Delay Seconds"},{"location":"experiments/chaos-resources/probes/litmus-probes/#stopcontinue-experiment-on-probe-failure","text":"It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true","title":"Stop/Continue Experiment On Probe Failure"},{"location":"experiments/chaos-resources/probes/litmus-probes/#probe-chaining","text":"Probe chaining enables reuse of probe a result (represented by the template function {{ .&gt;probeName&lt;.probeArtifact.Register}}) in subsequent \"downstream\" probes defined in the ChaosEngine. Note : The order of execution of probes in the experiment depends purely on the order in which they are defined in the ChaosEngine. Use the following example to tune this: # chaining enables reuse of probe's result (represented by the template function {{ <probeName>.probeArtifact.Register}}) #-- in subsequent \"downstream\" probes defined in the ChaosEngine. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"probe1\" type : \"cmdProbe\" cmdProbe/inputs : command : \"<command>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1 - name : \"probe2\" type : \"cmdProbe\" cmdProbe/inputs : ## probe1's result being used as one of the args in probe2 command : \"<commmand> {{ .probe1.ProbeArtifacts.Register }} <arg2>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Probe Chaining"},{"location":"experiments/chaos-resources/probes/promProbe/","text":"The prometheus probe allows users to run Prometheus queries and match the resulting output against specific conditions. The intent behind this probe is to allow users to define metrics-based SLOs in a declarative way and determine the experiment verdict based on its success. The probe runs the query on a Prometheus server defined by the endpoint, and checks whether the output satisfies the specified criteria. It can be executed by setting type as promProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Prometheus Query(query is a simple) \u00b6 It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 Prometheus Query(query is a complex \u00b6 In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Probe"},{"location":"experiments/chaos-resources/probes/promProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-simple","text":"It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a simple)"},{"location":"experiments/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-complex","text":"In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a complex"}]}