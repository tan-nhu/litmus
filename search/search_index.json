{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ROADMAP/","text":"LITMUS ROADMAP \u00b6 This document captures only the high level roadmap items. For the detailed backlog, see issues list . Completed \u00b6 Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure In-Progress (Under Active Development) \u00b6 Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward Backlog \u00b6 Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Roadmap"},{"location":"ROADMAP/#litmus-roadmap","text":"This document captures only the high level roadmap items. For the detailed backlog, see issues list .","title":"LITMUS ROADMAP"},{"location":"ROADMAP/#completed","text":"Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure","title":"Completed"},{"location":"ROADMAP/#in-progress-under-active-development","text":"Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward","title":"In-Progress (Under Active Development)"},{"location":"ROADMAP/#backlog","text":"Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Backlog"},{"location":"experiments/api/contents/","text":"Litmus API Documentation \u00b6 Name Description References AUTH Server Contains AUTH Server API documentation AUTH Server GraphQL Server Contains GraphQL Server API documentation GraphQL Server","title":"API Docs"},{"location":"experiments/api/contents/#litmus-api-documentation","text":"Name Description References AUTH Server Contains AUTH Server API documentation AUTH Server GraphQL Server Contains GraphQL Server API documentation GraphQL Server","title":"Litmus API Documentation"},{"location":"experiments/categories/contents/","text":"Experiments \u00b6 The experiment execution is triggered upon creation of the ChaosEngine resource (various examples of which are provided under the respective experiments). Typically, these chaosengines are embedded within the 'steps' of a Litmus Chaos Workflow here . However, one may also create the chaos engines directly by hand, and the chaos-operator reconciles this resource and triggers the experiment execution. Provided below are tables with links to the individual experiment docs for easy navigation Kubernetes Experiments \u00b6 It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available: Generic \u00b6 Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart Pod Chaos \u00b6 Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container by invoking a utility within the app container base image pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container by invoking a utility within the app container base image pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss Node Chaos \u00b6 Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint Application Chaos \u00b6 While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete Cloud Infrastructure \u00b6 Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available: AWS \u00b6 Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag GCP \u00b6 Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss Azure \u00b6 Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop Azure Disk Loss Detach azure disk from instance azure-disk-loss VMWare \u00b6 Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"Contents"},{"location":"experiments/categories/contents/#experiments","text":"The experiment execution is triggered upon creation of the ChaosEngine resource (various examples of which are provided under the respective experiments). Typically, these chaosengines are embedded within the 'steps' of a Litmus Chaos Workflow here . However, one may also create the chaos engines directly by hand, and the chaos-operator reconciles this resource and triggers the experiment execution. Provided below are tables with links to the individual experiment docs for easy navigation","title":"Experiments"},{"location":"experiments/categories/contents/#kubernetes-experiments","text":"It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available:","title":"Kubernetes Experiments"},{"location":"experiments/categories/contents/#generic","text":"Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart","title":"Generic"},{"location":"experiments/categories/contents/#pod-chaos","text":"Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container by invoking a utility within the app container base image pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container by invoking a utility within the app container base image pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss","title":"Pod Chaos"},{"location":"experiments/categories/contents/#node-chaos","text":"Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint","title":"Node Chaos"},{"location":"experiments/categories/contents/#application-chaos","text":"While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete","title":"Application Chaos"},{"location":"experiments/categories/contents/#cloud-infrastructure","text":"Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available:","title":"Cloud Infrastructure"},{"location":"experiments/categories/contents/#aws","text":"Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag","title":"AWS"},{"location":"experiments/categories/contents/#gcp","text":"Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss","title":"GCP"},{"location":"experiments/categories/contents/#azure","text":"Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop Azure Disk Loss Detach azure disk from instance azure-disk-loss","title":"Azure"},{"location":"experiments/categories/contents/#vmware","text":"Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"VMWare"},{"location":"experiments/categories/aws/AWS-experiments-tunables/","text":"It contains the AWS specific experiment tunables. Managed Nodegroup \u00b6 It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"AWS experiments tunables"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#managed-nodegroup","text":"It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Managed Nodegroup"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws/ebs-loss-by-id/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_ID Comma separated list of volume IDs subjected to ebs detach chaos Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Detach Volumes By ID \u00b6 It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By ID"},{"location":"experiments/categories/aws/ebs-loss-by-id/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-id/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-id/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#detach-volumes-by-id","text":"It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By ID"},{"location":"experiments/categories/aws/ebs-loss-by-tag/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_TAG Provide the common tag for target volumes. It'll be in form of key:value (Ex: 'team:devops') Optional Fields Variables Description Notes VOLUME_AFFECTED_PERC The Percentage of total ebs volumes to target Defaults to 0 (corresponds to 1 volume), provide numeric value only TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single volume \u00b6 It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of volumes \u00b6 It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By Tag"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-single-volume","text":"It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single volume"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-percent-of-volumes","text":"It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of volumes"},{"location":"experiments/categories/aws/ec2-terminate-by-id/","text":"Introduction \u00b6 It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Stop Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#introduction","text":"It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#stop-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/","text":"Introduction \u00b6 It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes INSTANCE_TAG Instance Tag to filter the target ec2 instance. The INSTANCE_TAG should be provided as key:value ex: team:devops Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single instance \u00b6 It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By Tag"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#introduction","text":"It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-single-instance","text":"It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-percent-of-instances","text":"It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/","text":"It contains the aws-ssm specific experiment tunables. CPU Cores \u00b6 It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Percentage \u00b6 It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSM Docs \u00b6 It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers Count \u00b6 It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"AWS SSM experiments tunables"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#cpu-cores","text":"It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"CPU Cores"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#memory-percentage","text":"It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Percentage"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#ssm-docs","text":"It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSM Docs"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#workers-count","text":"It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers Count"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/","text":"Introduction \u00b6 AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Stress Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#introduction","text":"AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#stress-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stress Instances By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/","text":"Introduction \u00b6 AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_TAG Instance Tag to filter the target ec2 instance The EC2_INSTANCE_TAG should be provided as key:value ex: chaos:ssm Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Target single instance \u00b6 It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By Tag"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#introduction","text":"AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-single-instance","text":"It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-percent-of-instances","text":"It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/azure/azure-disk-loss/","text":"Introduction \u00b6 It causes detachment of virtual disk from an Azure instance before re-attaching it back to the instance after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Detach the virtual disk from instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to detach and attach a disk. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations Azure Disk should be connected to an instance. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-disk-loss-sa namespace : default labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-disk-loss-sa labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" ] resources : [ \"pods\" , \"jobs\" , \"secrets\" , \"events\" , \"pods/log\" , \"pods/exec\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-disk-loss-sa labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-disk-loss-sa subjects : - kind : ServiceAccount name : azure-disk-loss-sa namespace : default Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes VIRTUAL_DISK_NAMES Name of virtual disks to target. Provide comma separated names for multiple disks RESOURCE_GROUP The resource group of the target disk(s) Optional Fields Variables Description Notes SCALE_SET Whether disk is connected to Scale set instance Accepts \"enable\"/\"disable\". Default is \"disable\" TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance poweroff. Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Detach Virtual Disks By Name \u00b6 It contains comma separated list of disk names subjected to disk loss chaos. It can be tuned via VIRTUAL_DISK_NAMES ENV. Use the following example to tune this: # detach multiple azure disks by their names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # comma separated names of the azure disks attached to VMs - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Detach Virtual Disks Attached to Scale Set Instances By Name \u00b6 It contains comma separated list of disk names attached to scale set instances subjected to disk loss chaos. It can be tuned via VIRTUAL_DISK_NAMES and SCALE_SET ENV. Use the following example to tune this: # detach multiple azure disks attached to scale set VMs by their names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # comma separated names of the azure disks attached to scale set VMs - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>' # VM belongs to scale set or not - name : SCALE_SET value : 'enable' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>'","title":"Azure Disk Loss"},{"location":"experiments/categories/azure/azure-disk-loss/#introduction","text":"It causes detachment of virtual disk from an Azure instance before re-attaching it back to the instance after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Detach the virtual disk from instance","title":"Introduction"},{"location":"experiments/categories/azure/azure-disk-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/azure/azure-disk-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to detach and attach a disk. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/azure/azure-disk-loss/#default-validations","text":"View the default validations Azure Disk should be connected to an instance.","title":"Default Validations"},{"location":"experiments/categories/azure/azure-disk-loss/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-disk-loss-sa namespace : default labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-disk-loss-sa labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" ] resources : [ \"pods\" , \"jobs\" , \"secrets\" , \"events\" , \"pods/log\" , \"pods/exec\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-disk-loss-sa labels : name : azure-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-disk-loss-sa subjects : - kind : ServiceAccount name : azure-disk-loss-sa namespace : default","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/azure/azure-disk-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/azure/azure-disk-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/azure/azure-disk-loss/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/azure/azure-disk-loss/#detach-virtual-disks-by-name","text":"It contains comma separated list of disk names subjected to disk loss chaos. It can be tuned via VIRTUAL_DISK_NAMES ENV. Use the following example to tune this: # detach multiple azure disks by their names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # comma separated names of the azure disks attached to VMs - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Virtual Disks By Name"},{"location":"experiments/categories/azure/azure-disk-loss/#detach-virtual-disks-attached-to-scale-set-instances-by-name","text":"It contains comma separated list of disk names attached to scale set instances subjected to disk loss chaos. It can be tuned via VIRTUAL_DISK_NAMES and SCALE_SET ENV. Use the following example to tune this: # detach multiple azure disks attached to scale set VMs by their names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # comma separated names of the azure disks attached to scale set VMs - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>' # VM belongs to scale set or not - name : SCALE_SET value : 'enable' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Virtual Disks Attached to Scale Set Instances By Name"},{"location":"experiments/categories/azure/azure-disk-loss/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-disk-loss-sa experiments : - name : azure-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VIRTUAL_DISK_NAMES value : 'disk-01,disk-02' - name : RESOURCE_GROUP value : '<resource group of VIRTUAL_DISK_NAMES>'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/azure/azure-instance-stop/","text":"Introduction \u00b6 It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations Azure instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes AZURE_INSTANCE_NAME Instance name of the target azure instance For AKS nodes, the instance name is from the scale set section in Azure and not the node name from AKS node pool RESOURCE_GROUP The resource group of the target instance Optional Fields Variables Description Notes SCALE_SET Whether instance is part of Scale set Accepts \"enable\"/\"disable\". Default is \"disable\" TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance power off. Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop Instances By Name \u00b6 It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azure instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Stop Scale Set Instances \u00b6 It contains comma separated list of instance names subjected to instance stop chaos belonging to Scale Set or AKS. It can be tuned via SCALE_SET ENV. Use the following example to tune this: ## contains the azure instance details for scale set instances or AKS nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azure instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of Scale set>' # accepts enable/disable value. default is disable - name : SCALE_SET value : 'enable' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Azure Instance Stop"},{"location":"experiments/categories/azure/azure-instance-stop/#introduction","text":"It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance","title":"Introduction"},{"location":"experiments/categories/azure/azure-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/azure/azure-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/azure/azure-instance-stop/#default-validations","text":"View the default validations Azure instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/azure/azure-instance-stop/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/azure/azure-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#stop-instances-by-name","text":"It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azure instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By Name"},{"location":"experiments/categories/azure/azure-instance-stop/#stop-scale-set-instances","text":"It contains comma separated list of instance names subjected to instance stop chaos belonging to Scale Set or AKS. It can be tuned via SCALE_SET ENV. Use the following example to tune this: ## contains the azure instance details for scale set instances or AKS nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azure instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of Scale set>' # accepts enable/disable value. default is disable - name : SCALE_SET value : 'enable' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Scale Set Instances"},{"location":"experiments/categories/azure/azure-instance-stop/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CASSANDRA_SVC_NAME Cassandra Service Name Defaults value: cassandra KEYSPACE_REPLICATION_FACTOR Value of the Replication factor for the cassandra liveness deploy It needs to create keyspace while checking the livenss of cassandra CASSANDRA_PORT Port of the cassandra statefulset Defaults value: 9042 CASSANDRA_LIVENESS_CHECK It allows to check the liveness of the cassandra statefulset It can be enabled or disabled CASSANDRA_LIVENESS_IMAGE Image of the cassandra liveness deployment Default value: litmuschaos/cassandra-client:latest SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0% (corresponds to 1 replica) CHAOS_INTERVAL Time interval b/w two successive pod failures (sec) Defaults to 5s LIB The chaos lib used to inject the chaos Defaults to litmus . Supported litmus only FORCE Application Pod deletion mode. False indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Cassandra App Details \u00b6 It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Force Delete \u00b6 The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of cassandra \u00b6 The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra Pod Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#introduction","text":"It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod","title":"Introduction"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#default-validations","text":"View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas.","title":"Default Validations"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#cassandra-app-details","text":"It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra App Details"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#force-delete","text":"The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Force Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#liveness-check-of-cassandra","text":"The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of cassandra"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/","text":"It contains tunables, which are common for all the experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Duration of the chaos \u00b6 It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Ramp Time \u00b6 It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60' Sequence of chaos execution \u00b6 It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: parallel : The chaos is injected in all the targets at once. serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60' Name of chaos library \u00b6 It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60' Instance ID \u00b6 It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60' Image used by the helper pod \u00b6 It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for all experiments"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#duration-of-the-chaos","text":"It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Duration of the chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#ramp-time","text":"It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Ramp Time"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#sequence-of-chaos-execution","text":"It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: parallel : The chaos is injected in all the targets at once. serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Sequence of chaos execution"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#name-of-chaos-library","text":"It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Name of chaos library"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#instance-id","text":"It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Instance ID"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#image-used-by-the-helper-pod","text":"It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Image used by the helper pod"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/","text":"Introduction \u00b6 It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations Disk volumes are attached to their respective instances Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID The ID of the GCP Project of which the disk volumes are a part of All the target disk volumes should belong to a single GCP Project DISK_VOLUME_NAMES Target non-boot persistent disk volume names Multiple disk volume names can be provided as disk1,disk2,... DISK_ZONES The zones of respective target disk volumes Provide the zone for every target disk name as zone1,zone2... in the respective order of DISK_VOLUME_NAMES DEVICE_NAMES The device names of respective target disk volumes Provide the device name for every target disk name as deviceName1,deviceName2... in the respective order of DISK_VOLUME_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between the successive chaos iterations (sec) Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Detach Volumes By Names \u00b6 It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Disk Loss"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#introduction","text":"It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#default-validations","text":"View the default validations Disk volumes are attached to their respective instances","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#detach-volumes-by-names","text":"It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By Names"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/","text":"Introduction \u00b6 It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID GCP project ID to which the VM instances belong All the VM instances must belong to a single GCP project VM_INSTANCE_NAMES Name of target VM instances Multiple instance names can be provided as instance1,instance2,... INSTANCE_ZONES The zones of the target VM instances Zone for every instance name has to be provided as zone1,zone2,... in the same order of VM_INSTANCE_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination Defaults to 30s AUTO_SCALING_GROUP Set to enable if the target instance is the part of a auto-scaling group Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Target GCP Instances \u00b6 It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Autoscaling NodeGroup \u00b6 If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Instance Stop"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#introduction","text":"It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#target-gcp-instances","text":"It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target GCP Instances"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#autoscaling-nodegroup","text":"If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Autoscaling NodeGroup"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored. Default Validations \u00b6 View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes KAFKA_NAMESPACE Namespace of Kafka Brokers May be same as value for spec.appinfo.appns KAFKA_LABEL Unique label of Kafka Brokers May be same as value for spec.appinfo.applabel KAFKA_SERVICE Headless service of the Kafka Statefulset KAFKA_PORT Port of the Kafka ClusterIP service ZOOKEEPER_NAMESPACE Namespace of Zookeeper Cluster May be same as value for KAFKA_NAMESPACE or other ZOOKEEPER_LABEL Unique label of Zokeeper statefulset ZOOKEEPER_SERVICE Headless service of the Zookeeper Statefulset ZOOKEEPER_PORT Port of the Zookeeper ClusterIP service Optional Fields Variables Description Notes KAFKA_BROKER Kafka broker pod (name) to be deleted A target selection mode (random/liveness-based/specific) KAFKA_KIND Kafka deployment type Same as spec.appinfo.appkind . Supported: statefulset KAFKA_LIVENESS_STREAM Kafka liveness message stream Supported: enabled , disabled KAFKA_LIVENESS_IMAGE Image used for liveness message stream Set the liveness image as <registry_url>/<repository>:<image-tag> KAFKA_REPLICATION_FACTOR Number of partition replicas for liveness topic partition Necessary if KAFKA_LIVENESS_STREAM is enabled . The replication factor should be less than or equal to number of Kafka brokers KAFKA_INSTANCE_NAME Name of the Kafka chroot path on zookeeper Necessary if installation involves use of such path KAFKA_CONSUMER_TIMEOUT Kafka consumer message timeout, post which it terminates Defaults to 30000ms, Recommended timeout for EKS platform: 60000 ms TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s CHAOS_INTERVAL Time interval b/w two successive broker failures (sec) Defaults to 5s Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Kafka And Zookeeper App Details \u00b6 It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of kafka \u00b6 The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Kafka Broker Pod Failure"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#introduction","text":"It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod","title":"Introduction"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored.","title":"Prerequisites"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#default-validations","text":"View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken","title":"Default Validations"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#kafka-and-zookeeper-app-details","text":"It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kafka And Zookeeper App Details"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#liveness-check-of-kafka","text":"The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of kafka"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/","text":"It contains tunables, which are common for all the node experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Single Node \u00b6 It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Multiple Nodes \u00b6 It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Nodes With Labels \u00b6 It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Node Affected Percentage \u00b6 It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for node experiments"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-single-node","text":"It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Single Node"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-multiple-nodes","text":"It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Multiple Nodes"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-nodes-with-labels","text":"It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Nodes With Labels"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#node-affected-percentage","text":"It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Affected Percentage"},{"location":"experiments/categories/nodes/docker-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Docker Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Docker Service Kill"},{"location":"experiments/categories/nodes/docker-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/docker-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/docker-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/docker-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/docker-service-kill/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/docker-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#kill-docker-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Docker Service"},{"location":"experiments/categories/nodes/kubelet-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE The lib image used to inject kubelet kill chaos the image should have systemd installed in it. Defaults to ubuntu:16.04 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Kubelet Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kubelet Service Kill"},{"location":"experiments/categories/nodes/kubelet-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/kubelet-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/kubelet-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/kubelet-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/kubelet-service-kill/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/kubelet-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#kill-kubelet-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Kubelet Service"},{"location":"experiments/categories/nodes/node-cpu-hog/","text":"Introduction \u00b6 This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node cpu hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE Image used to run the stress command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before & after injection of chaos in sec Optional NODE_CPU_CORE Number of cores of node CPU to be consumed Defaults to 2 NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Node CPU Cores \u00b6 It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Hog"},{"location":"experiments/categories/nodes/node-cpu-hog/#introduction","text":"This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-cpu-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-cpu-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-cpu-hog/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-cpu-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#node-cpu-cores","text":"It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Cores"},{"location":"experiments/categories/nodes/node-drain/","text":"Introduction \u00b6 It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Drain Node \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Drain"},{"location":"experiments/categories/nodes/node-drain/#introduction","text":"It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-drain/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-drain/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-drain/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-drain/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-drain/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-drain/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-drain/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-drain/#drain-node","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Drain Node"},{"location":"experiments/categories/nodes/node-io-stress/","text":"Introduction \u00b6 This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node io stress chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120 FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. CPU Number of core of CPU to be used Default to 1 NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 VM_WORKERS It is the number vm workers involved in IO disk stress Default to 1 LIB The chaos lib used to inject the chaos Default to litmus LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Limit CPU Utilization \u00b6 The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this: Workers For Stress \u00b6 The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node IO Stress"},{"location":"experiments/categories/nodes/node-io-stress/#introduction","text":"This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node","title":"Introduction"},{"location":"experiments/categories/nodes/node-io-stress/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-io-stress/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-io-stress/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-io-stress/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/nodes/node-io-stress/#limit-cpu-utilization","text":"The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this:","title":"Limit CPU Utilization"},{"location":"experiments/categories/nodes/node-io-stress/#workers-for-stress","text":"The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-memory-hog/","text":"Introduction \u00b6 This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node memory hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in seconds) Optional Defaults to 120 LIB The chaos lib used to inject the chaos Optional Defaults to litmus LIB_IMAGE Image used to run the stress command Optional Defaults to litmuschaos/go-runner:latest MEMORY_CONSUMPTION_PERCENTAGE Percent of the total node memory capacity Optional Defaults to 30 MEMORY_CONSUMPTION_MEBIBYTES The size in Mebibytes of total available memory. When using this we need to keep MEMORY_CONSUMPTION_PERCENTAGE empty as the percentage have more precedence Optional NUMBER_OF_WORKERS It is the number of VM workers involved in IO disk stress Optional Default to 1 RAMP_TIME Period to wait before and after injection of chaos in sec Optional NODES_AFFECTED_PERC The Percentage of total nodes to target Optional Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Memory Consumption Percentage \u00b6 It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Consumption Mebibytes \u00b6 It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Memory Hog"},{"location":"experiments/categories/nodes/node-memory-hog/#introduction","text":"This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-memory-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-memory-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-memory-hog/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-memory-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-percentage","text":"It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Percentage"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-mebibytes","text":"It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Mebibytes"},{"location":"experiments/categories/nodes/node-memory-hog/#workers-for-stress","text":"The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-restart/","text":"Introduction \u00b6 It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment. Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of target node, subjected to chaos. If not provided it will select the random node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes LIB_IMAGE The image used to restart the node Defaults to litmuschaos/go-runner:latest SSH_USER name of ssh user Defaults to root TARGET_NODE_IP Internal IP of the target node, subjected to chaos. If not provided, the experiment will lookup the node IP of the TARGET_NODE node Defaults to empty REBOOT_COMMAND Command used for reboot Defaults to sudo systemctl reboot TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Reboot Command \u00b6 It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSH User \u00b6 It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Node Internal IP \u00b6 It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Restart"},{"location":"experiments/categories/nodes/node-restart/#introduction","text":"It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-restart/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-restart/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment.","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-restart/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-restart/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-restart/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-restart/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-restart/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-restart/#reboot-command","text":"It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Reboot Command"},{"location":"experiments/categories/nodes/node-restart/#ssh-user","text":"It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSH User"},{"location":"experiments/categories/nodes/node-restart/#target-node-internal-ip","text":"It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Node Internal IP"},{"location":"experiments/categories/nodes/node-taint/","text":"Introduction \u00b6 It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE TAINT_LABEL Label and effect to be tainted on application node Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Taint Label \u00b6 It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Taint"},{"location":"experiments/categories/nodes/node-taint/#introduction","text":"It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-taint/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-taint/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-taint/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-taint/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-taint/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-taint/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-taint/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-taint/#taint-label","text":"It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Taint Label"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/","text":"It contains tunables, which are common for all pod-level experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Specific Pods \u00b6 It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pod Affected Percentage \u00b6 It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for pod experiments"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-pods","text":"It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Pods"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#pod-affected-percentage","text":"It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Affected Percentage"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Container"},{"location":"experiments/categories/pods/container-kill/","text":"Introduction \u00b6 It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER The name of container to be killed inside the pod If the TARGET_CONTAINER is not provided it will delete the first container CHAOS_INTERVAL Time interval b/w two successive container kill (in sec) If the CHAOS_INTERVAL is not provided it will take the default value of 10s TOTAL_CHAOS_DURATION The time duration for chaos injection (seconds) Defaults to 20s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only TARGET_PODS Comma separated list of application pod name subjected to container kill chaos If not provided, it will select target pods randomly based on provided appLabels LIB_IMAGE LIB Image used to kill the container Defaults to litmuschaos/go-runner:latest LIB The category of lib use to inject chaos Default value: litmus, supported values: pumba and litmus RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel SIGNAL It contains termination signal used for container kill Default value: SIGKILL SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Kill Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path: CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Signal For Kill \u00b6 It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Kill"},{"location":"experiments/categories/pods/container-kill/#introduction","text":"It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container","title":"Introduction"},{"location":"experiments/categories/pods/container-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/container-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/container-kill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/container-kill/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/container-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/container-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/container-kill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/container-kill/#kill-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Specific Container"},{"location":"experiments/categories/pods/container-kill/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/container-kill/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path: CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/container-kill/#signal-for-kill","text":"It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Signal For Kill"},{"location":"experiments/categories/pods/container-kill/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/disk-fill/","text":"Introduction \u00b6 It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes FILL_PERCENTAGE Percentage to fill the Ephemeral storage limit Can be set to more than 100 also, to force evict the pod. The ephemeral-storage limits must be set in targeted pod to use this ENV. EPHEMERAL_STORAGE_MEBIBYTES Ephemeral storage which need to fill (unit: MiBi) It is mutually exclusive with the FILL_PERCENTAGE ENV. If both are provided then it will use the FILL_PERCENTAGE Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to disk-fill If not provided, the first container in the targeted pod will be subject to chaos CONTAINER_PATH Storage Location of containers Defaults to '/var/lib/docker/containers' TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 60s TARGET_PODS Comma separated list of application pod name subjected to disk fill chaos If not provided, it will select target pods randomly based on provided appLabels DATA_BLOCK_SIZE It contains data block size used to fill the disk(in KB) Defaults to 256, it supports unit as KB only PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only LIB_IMAGE The image used to fill the disk Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Disk Fill Percentage \u00b6 It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Disk Fill Mebibytes \u00b6 It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Data Block Size \u00b6 It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Path \u00b6 It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill"},{"location":"experiments/categories/pods/disk-fill/#introduction","text":"It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage","title":"Introduction"},{"location":"experiments/categories/pods/disk-fill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/disk-fill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\"","title":"Prerequisites"},{"location":"experiments/categories/pods/disk-fill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/disk-fill/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/disk-fill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/disk-fill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/disk-fill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-percentage","text":"It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Percentage"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-mebibytes","text":"It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Mebibytes"},{"location":"experiments/categories/pods/disk-fill/#data-block-size","text":"It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Data Block Size"},{"location":"experiments/categories/pods/disk-fill/#container-path","text":"It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Path"},{"location":"experiments/categories/pods/pod-autoscaler/","text":"Introduction \u00b6 The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes REPLICA_COUNT Number of replicas upto which we want to scale nil Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The timeout for the chaos experiment (in seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Replica counts \u00b6 It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Autoscaler"},{"location":"experiments/categories/pods/pod-autoscaler/#introduction","text":"The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas","title":"Introduction"},{"location":"experiments/categories/pods/pod-autoscaler/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-autoscaler/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-autoscaler/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-autoscaler/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-autoscaler/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#replica-counts","text":"It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Replica counts"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Default to litmus TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CHAOS_INJECT_COMMAND The command to inject the cpu chaos Default to md5sum /dev/zero CHAOS_KILL_COMMAND The command to kill the chaos process Default to kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}') . Another useful one that generally works (in case the default doesn't) is kill -9 \\((ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print\\) 1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Chaos Inject and Kill Commands \u00b6 It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog Exec"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#chaos-inject-and-kill-commands","text":"It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Inject and Kill Commands"},{"location":"experiments/categories/pods/pod-cpu-hog/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Default to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog"},{"location":"experiments/categories/pods/pod-cpu-hog/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-cpu-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-delete/","text":"Introduction \u00b6 It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod Uses \u00b6 View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application. Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in sec) Defaults to 15s, NOTE: Overall run duration of the experiment may exceed the TOTAL_CHAOS_DURATION by a few min CHAOS_INTERVAL Time interval b/w two successive pod failures (in sec) Defaults to 5s RANDOMNESS Introduces randomness to pod deletions with a minimum period defined by CHAOS_INTERVAL It supports true or false. Default value: false FORCE Application Pod deletion mode. false indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 TARGET_PODS Comma separated list of application pod name subjected to pod delete chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Force Delete \u00b6 The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Random Interval \u00b6 The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Pod Delete"},{"location":"experiments/categories/pods/pod-delete/#introduction","text":"It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-delete/#uses","text":"View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application.","title":"Uses"},{"location":"experiments/categories/pods/pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-delete/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-delete/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-delete/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-delete/#force-delete","text":"The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Force Delete"},{"location":"experiments/categories/pods/pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/pod-delete/#random-interval","text":"The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Random Interval"},{"location":"experiments/categories/pods/pod-dns-error/","text":"Introduction \u00b6 Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns-error None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_HOSTNAMES List of the target hostnames or keywords eg. '[\"litmuschaos\",\"chaosnative.com\"]' If not provided, all hostnames/domains will be targeted MATCH_SCHEME Determines whether the dns query has to match exactly with one of the targets or can have any of the targets as substring. Can be either exact or substring if not provided, it will be set as exact PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Target Host Names \u00b6 It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60' Match Scheme \u00b6 It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Error"},{"location":"experiments/categories/pods/pod-dns-error/#introduction","text":"Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-error/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-error/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-error/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-error/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-error/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-error/#target-host-names","text":"It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Target Host Names"},{"location":"experiments/categories/pods/pod-dns-error/#match-scheme","text":"It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Match Scheme"},{"location":"experiments/categories/pods/pod-dns-error/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-dns-spoof/","text":"Introduction \u00b6 Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns spoof None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) SPOOF_MAP Map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. If not provided, no hostnames/domains will be spoofed PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Spoof Map \u00b6 It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Spoof"},{"location":"experiments/categories/pods/pod-dns-spoof/#introduction","text":"Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-spoof/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-spoof/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-spoof/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-spoof/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-spoof/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#spoof-map","text":"It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Spoof Map"},{"location":"experiments/categories/pods/pod-dns-spoof/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/","text":"Introduction \u00b6 This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems Stressing the disk with continuous and heavy IO for example can cause degradation in reads written by other microservices that use this shared disk for example modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out. Another issue is the amount of scratch space eaten up on a node which leads to the lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup.. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120s VOLUME_MOUNT_PATH Fill the given volume mount path LIB The chaos lib used to inject the chaos Default to litmus . Available litmus and pumba. LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest TARGET_PODS Comma separated list of application pod name subjected to pod io stress chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mount Path \u00b6 The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod IO Stress"},{"location":"experiments/categories/pods/pod-io-stress/#introduction","text":"This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-io-stress/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems Stressing the disk with continuous and heavy IO for example can cause degradation in reads written by other microservices that use this shared disk for example modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out. Another issue is the amount of scratch space eaten up on a node which leads to the lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes","title":"Uses"},{"location":"experiments/categories/pods/pod-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-io-stress/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-io-stress/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup.. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-io-stress/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/pods/pod-io-stress/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/#mount-path","text":"The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Mount Path"},{"location":"experiments/categories/pods/pod-io-stress/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-io-stress/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-memory-hog-exec/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB (Up to 2000MB) TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Defaults to litmus TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod CHAOS_KILL_COMMAND The command to kill the chaos process Defaults to kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1) . Another useful one that generally works (in case the default doesn't) is kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Chaos Kill Commands \u00b6 It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog Exec"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#uses","text":"View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#chaos-kill-commands","text":"It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Kill Commands"},{"location":"experiments/categories/pods/pod-memory-hog/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB NUMBER_OF_WORKERS The number of workers used to run the stress process Defaults to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Defaults to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos. If not provided, it will select the first container of the target pod CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog"},{"location":"experiments/categories/pods/pod-memory-hog/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog/#uses","text":"View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-memory-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path.","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-memory-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-corruption/","text":"Introduction \u00b6 It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network corruption Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_CORRUPTION_PERCENTAGE Packet corruption in percentage Default (100) CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Corruption \u00b6 It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#introduction","text":"It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-corruption/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-corruption/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-corruption/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-corruption/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-corruption/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#network-packet-corruption","text":"It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-corruption/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-corruption/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-corruption/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-duplication/","text":"Introduction \u00b6 It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_DUPLICATION_PERCENTAGE The packet duplication in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Duplication \u00b6 It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#introduction","text":"It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-duplication/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-duplication/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-duplication/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-duplication/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-duplication/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#network-packet-duplication","text":"It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-duplication/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-duplication/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-duplication/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-latency/","text":"Introduction \u00b6 It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod Uses \u00b6 View the uses of the experiment The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container). The idea of this experiment is to simulate issues within your pod network OR microservice communication across services in different availability zones/regions etc. Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The experiment provides DESTINATION_IPS or DESTINATION_HOSTS so that you can control the chaos against specific services within or outside the cluster. The applications may stall or get corrupted while they wait endlessly for a packet. The experiment limits the impact (blast radius) to only the traffic you want to test by specifying IP addresses or application information.This experiment will help to improve the resilience of your services over time Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_LATENCY The latency/delay in milliseconds Optional Default 2000, provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Latency \u00b6 It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#introduction","text":"It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-latency/#uses","text":"View the uses of the experiment The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container). The idea of this experiment is to simulate issues within your pod network OR microservice communication across services in different availability zones/regions etc. Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The experiment provides DESTINATION_IPS or DESTINATION_HOSTS so that you can control the chaos against specific services within or outside the cluster. The applications may stall or get corrupted while they wait endlessly for a packet. The experiment limits the impact (blast radius) to only the traffic you want to test by specifying IP addresses or application information.This experiment will help to improve the resilience of your services over time","title":"Uses"},{"location":"experiments/categories/pods/pod-network-latency/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-latency/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-latency/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-latency/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-latency/#network-latency","text":"It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-latency/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-latency/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-latency/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-loss/","text":"Introduction \u00b6 It injects packet loss on the specified container by starting a traffic control (tc) process with netem rules to add egress/ingress loss It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network loss Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_LOSS_PERCENTAGE The packet loss in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Loss \u00b6 It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Loss"},{"location":"experiments/categories/pods/pod-network-loss/#introduction","text":"It injects packet loss on the specified container by starting a traffic control (tc) process with netem rules to add egress/ingress loss It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-loss/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-loss/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-loss/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-loss/#network-packet-loss","text":"It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Loss"},{"location":"experiments/categories/pods/pod-network-loss/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-loss/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-loss/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-loss/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : 'gaiadocker/iproute2' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-partition/","text":"Introduction \u00b6 It blocks the 100% Ingress and Egress traffic of the target application by creating network policy. It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-partition experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"networking.k8s.io\" ] resources : [ \"networkpolicies\" ] verbs : [ \"create\" , \"delete\" , \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-partition-sa subjects : - kind : ServiceAccount name : pod-network-partition-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) POLICY_TYPES Contains type of network policy It supports egress , ingress and all values POD_SELECTOR Contains labels of the destination pods NAMESPACE_SELECTOR Contains labels of the destination namespaces PORTS Comma separated list of the targeted ports DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined LIB The chaos lib used to inject the chaos supported value: litmus RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Destination IPs And Destination Hosts \u00b6 The network partition experiment interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Target Specific Namespace(s) \u00b6 The network partition experiment interrupt traffic for all the namespaces by default. The interruption of specific namespace can be tuned via providing namespace labels inside NAMESPACE_SELECTOR ENV. Use the following example to tune this: # it inject the chaos for specified namespaces, matched by labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # labels of the destination namespace - name : NAMESPACE_SELECTOR value : 'key=value' - name : TOTAL_CHAOS_DURATION value : '60' Target Specific Pod(s) \u00b6 The network partition experiment interrupt traffic for all the extranal pods by default. The interruption of specific pod(s) can be tuned via providing pod labels inside POD_SELECTOR ENV. Use the following example to tune this: # it inject the chaos for specified pods, matched by labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # labels of the destination pods - name : POD_SELECTOR value : 'key=value' - name : TOTAL_CHAOS_DURATION value : '60' Policy Type \u00b6 The network partition experiment interrupt both ingress and egress traffic by default. The interruption of either ingress or egress traffic can be tuned via POLICY_TYPES ENV. Use the following example to tune this: # inject network loss for only ingress or only engress or all traffics apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # provide the network policy type # it supports `ingress`, `egress`, and `all` values # default value is `all` - name : POLICY_TYPES value : 'all' - name : TOTAL_CHAOS_DURATION value : '60' Destination Ports \u00b6 The network partition experiment interrupt traffic for all the extranal ports by default. The interruption of specific port(s) can be tuned via providing comma separated list of ports inside PORTS ENV. Use the following example to tune this: # it inject the chaos for specified ports apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # comma separated list of ports - name : PORTS value : '8080,8088' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Partition"},{"location":"experiments/categories/pods/pod-network-partition/#introduction","text":"It blocks the 100% Ingress and Egress traffic of the target application by creating network policy. It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-partition/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-partition/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-partition experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-partition/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-partition/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"networking.k8s.io\" ] resources : [ \"networkpolicies\" ] verbs : [ \"create\" , \"delete\" , \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-partition-sa namespace : default labels : name : pod-network-partition-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-partition-sa subjects : - kind : ServiceAccount name : pod-network-partition-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-partition/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-partition/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-partition/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-partition/#destination-ips-and-destination-hosts","text":"The network partition experiment interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-partition/#target-specific-namespaces","text":"The network partition experiment interrupt traffic for all the namespaces by default. The interruption of specific namespace can be tuned via providing namespace labels inside NAMESPACE_SELECTOR ENV. Use the following example to tune this: # it inject the chaos for specified namespaces, matched by labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # labels of the destination namespace - name : NAMESPACE_SELECTOR value : 'key=value' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Target Specific Namespace(s)"},{"location":"experiments/categories/pods/pod-network-partition/#target-specific-pods","text":"The network partition experiment interrupt traffic for all the extranal pods by default. The interruption of specific pod(s) can be tuned via providing pod labels inside POD_SELECTOR ENV. Use the following example to tune this: # it inject the chaos for specified pods, matched by labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # labels of the destination pods - name : POD_SELECTOR value : 'key=value' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Target Specific Pod(s)"},{"location":"experiments/categories/pods/pod-network-partition/#policy-type","text":"The network partition experiment interrupt both ingress and egress traffic by default. The interruption of either ingress or egress traffic can be tuned via POLICY_TYPES ENV. Use the following example to tune this: # inject network loss for only ingress or only engress or all traffics apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # provide the network policy type # it supports `ingress`, `egress`, and `all` values # default value is `all` - name : POLICY_TYPES value : 'all' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Policy Type"},{"location":"experiments/categories/pods/pod-network-partition/#destination-ports","text":"The network partition experiment interrupt traffic for all the extranal ports by default. The interruption of specific port(s) can be tuned via providing comma separated list of ports inside PORTS ENV. Use the following example to tune this: # it inject the chaos for specified ports apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-partition-sa experiments : - name : pod-network-partition spec : components : env : # comma separated list of ports - name : PORTS value : '8080,8088' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination Ports"},{"location":"experiments/categories/vmware/vm-poweroff/","text":"Introduction \u00b6 It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable. Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes APP_VM_MOID Moid of the vmware instance Once you open VM in vCenter WebClient, you can find MOID in address field (VirtualMachine:vm-5365). Eg: vm-5365 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop/Poweroff VM By MOID \u00b6 It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"VM Poweroff"},{"location":"experiments/categories/vmware/vm-poweroff/#introduction","text":"It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm","title":"Introduction"},{"location":"experiments/categories/vmware/vm-poweroff/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/vmware/vm-poweroff/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable.","title":"Prerequisites"},{"location":"experiments/categories/vmware/vm-poweroff/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/vmware/vm-poweroff/#minimal-rbac-configuration-example-optional","text":"NOTE If you are using this experiment as part of a litmus workflow scheduled constructed & executed from chaos-center, then you may be making use of the litmus-admin RBAC, which is pre installed in the cluster as part of the agent setup. View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/vmware/vm-poweroff/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#stoppoweroff-vm-by-moid","text":"It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop/Poweroff VM By MOID"},{"location":"experiments/concepts/IAM/awsIamIntegration/","text":"IAM integration for Litmus service accounts \u00b6 You can execute Litmus AWS experiments to target different AWS services from the EKS cluster itself, for this we need to authenticate Litmus with the AWS platform, we can do this in two different ways: Using secrets: It is one of the common ways to authenticate litmus with AWS irrespective of the Kubernetes cluster used for the deployment. In other words, it is Kubernetes\u2019 native way for the authentication of litmus with the AWS platform. IAM Integration: It can be used when we\u2019ve deployed Litmus on EKS cluster , we can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the experiment pod that uses that service account. We\u2019ll discuss more this method in the below sections. Why should we use IAM integration for AWS authentication? \u00b6 The IAM roles for service accounts feature provides the following benefits: Least privilege: By using the IAM roles for service accounts feature, you no longer need to provide extended permissions to the node IAM role so that pods on that node can call AWS APIs. You can scope IAM permissions to a service account, and only pods that use that service account have access to those permissions. Credential isolation: The experiment can only retrieve credentials for the IAM role that is associated with the service account to which it belongs. The experiment never has access to credentials that are intended for another experiment that belongs to another pod. Enable service accounts to access AWS resources: \u00b6 Step 1: Create an IAM OIDC provider for your cluster \u00b6 We need to perform this once for a cluster. We\u2019re going to follow the AWS documentation to setup an OIDC provider with eksctl. Check whether you have an existing IAM OIDC provider for your cluster: To check this you can follow the given instruction. Note: For demonstration we\u2019ll be using cluster name as litmus-demo and region us-west-1 you can replace these values according to your ENV. root@demo> aws eks describe-cluster --name <litmus-demo> --query \"cluster.identity.oidc.issuer\" --output text Output: https://oidc.eks.us-west-1.amazonaws.com/id/D054E55B6947B1A7B3F200297789662C Now List the IAM OIDC providers in your account Command: aws iam list-open-id-connect-providers | grep <EXAMPLED539D4633E53DE1B716D3041E> Replace <D054E55B6947B1A7B3F200297789662C> (including <> ) with the value returned from the previous command. So now here we don\u2019t have an IAM OIDC identity provider, So we need to create it for your cluster with the following command. Replace <litmus-demo> ( including <> ) with your own value. root@demo$ eksctl utils associate-iam-oidc-provider --cluster litmus-demo --approve 2021 -09-07 14 :54:01 [ \u2139 ] eksctl version 0 .52.0 2021 -09-07 14 :54:01 [ \u2139 ] using region us-west-1 2021 -09-07 14 :54:04 [ \u2139 ] will create IAM Open ID Connect provider for cluster \"udit-cluster-11\" in \"us-west-1\" 2021 -09-07 14 :54:05 [ \u2714 ] created IAM Open ID Connect provider for cluster \"litmus-demo\" in \"us-west-1\" Step 2: Creating an IAM role and policy for your service account \u00b6 You must create an IAM policy that specifies the permissions that you would like the experiment should to have. You have several ways to create a new IAM permission policy. Check out the AWS docs for creating the IAM policy . We will make use of eksctl command to setup the same. root@demo> eksctl create iamserviceaccount \\ --name <service_account_name> \\ --namespace <service_account_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn <IAM_policy_ARN> \\ --approve \\ --override-existing-serviceaccont Step 3: Associate an IAM role with a service account \u00b6 Complete this task for each Kubernetes service account that needs access to AWS resources. We can do this by defining the IAM role to associate with a service account in your cluster by adding the following annotation to the service account. apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> You can also annotate the experiment service account running the following command. kubectl annotate serviceaccount -n <SERVICE_ACCOUNT_NAMESPACE> <SERVICE_ACCOUNT_NAME> \\ eks.amazonaws.com/role-arn = arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> Verify that the experiment service account is now associated with the IAM Describe one of the pods and verify that the AWS_WEB_IDENTITY_TOKEN_FILE and AWS_ROLE_ARN environment variables exist. kubectl exec -n kube-system <aws-node-9rgzw> env | grep AWS Output: AWS_VPC_K8S_CNI_LOGLEVEL=DEBUG AWS_ROLE_ARN=arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token Now we have successfully enabled the experiment service accounts to access AWS resources. Configure the Experiment CR. \u00b6 Since we have already configured the IAM for the experiment service account we don\u2019t need to create secret and mount it with experiment CR which is enabled by default. To remove the secret mount we have to remove the following lines from experiment YAML. secrets : - name : cloud-secret mountPath : /tmp/ We can now run the experiment with the direct IAM integration.","title":"AWS IAM Integration"},{"location":"experiments/concepts/IAM/awsIamIntegration/#iam-integration-for-litmus-service-accounts","text":"You can execute Litmus AWS experiments to target different AWS services from the EKS cluster itself, for this we need to authenticate Litmus with the AWS platform, we can do this in two different ways: Using secrets: It is one of the common ways to authenticate litmus with AWS irrespective of the Kubernetes cluster used for the deployment. In other words, it is Kubernetes\u2019 native way for the authentication of litmus with the AWS platform. IAM Integration: It can be used when we\u2019ve deployed Litmus on EKS cluster , we can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the experiment pod that uses that service account. We\u2019ll discuss more this method in the below sections.","title":"IAM integration for Litmus service accounts"},{"location":"experiments/concepts/IAM/awsIamIntegration/#why-should-we-use-iam-integration-for-aws-authentication","text":"The IAM roles for service accounts feature provides the following benefits: Least privilege: By using the IAM roles for service accounts feature, you no longer need to provide extended permissions to the node IAM role so that pods on that node can call AWS APIs. You can scope IAM permissions to a service account, and only pods that use that service account have access to those permissions. Credential isolation: The experiment can only retrieve credentials for the IAM role that is associated with the service account to which it belongs. The experiment never has access to credentials that are intended for another experiment that belongs to another pod.","title":"Why should we use IAM integration for AWS authentication?"},{"location":"experiments/concepts/IAM/awsIamIntegration/#enable-service-accounts-to-access-aws-resources","text":"","title":"Enable service accounts to access AWS resources:"},{"location":"experiments/concepts/IAM/awsIamIntegration/#step-1-create-an-iam-oidc-provider-for-your-cluster","text":"We need to perform this once for a cluster. We\u2019re going to follow the AWS documentation to setup an OIDC provider with eksctl. Check whether you have an existing IAM OIDC provider for your cluster: To check this you can follow the given instruction. Note: For demonstration we\u2019ll be using cluster name as litmus-demo and region us-west-1 you can replace these values according to your ENV. root@demo> aws eks describe-cluster --name <litmus-demo> --query \"cluster.identity.oidc.issuer\" --output text Output: https://oidc.eks.us-west-1.amazonaws.com/id/D054E55B6947B1A7B3F200297789662C Now List the IAM OIDC providers in your account Command: aws iam list-open-id-connect-providers | grep <EXAMPLED539D4633E53DE1B716D3041E> Replace <D054E55B6947B1A7B3F200297789662C> (including <> ) with the value returned from the previous command. So now here we don\u2019t have an IAM OIDC identity provider, So we need to create it for your cluster with the following command. Replace <litmus-demo> ( including <> ) with your own value. root@demo$ eksctl utils associate-iam-oidc-provider --cluster litmus-demo --approve 2021 -09-07 14 :54:01 [ \u2139 ] eksctl version 0 .52.0 2021 -09-07 14 :54:01 [ \u2139 ] using region us-west-1 2021 -09-07 14 :54:04 [ \u2139 ] will create IAM Open ID Connect provider for cluster \"udit-cluster-11\" in \"us-west-1\" 2021 -09-07 14 :54:05 [ \u2714 ] created IAM Open ID Connect provider for cluster \"litmus-demo\" in \"us-west-1\"","title":"Step 1: Create an IAM OIDC provider for your cluster"},{"location":"experiments/concepts/IAM/awsIamIntegration/#step-2-creating-an-iam-role-and-policy-for-your-service-account","text":"You must create an IAM policy that specifies the permissions that you would like the experiment should to have. You have several ways to create a new IAM permission policy. Check out the AWS docs for creating the IAM policy . We will make use of eksctl command to setup the same. root@demo> eksctl create iamserviceaccount \\ --name <service_account_name> \\ --namespace <service_account_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn <IAM_policy_ARN> \\ --approve \\ --override-existing-serviceaccont","title":"Step 2: Creating an IAM role and policy for your service account"},{"location":"experiments/concepts/IAM/awsIamIntegration/#step-3-associate-an-iam-role-with-a-service-account","text":"Complete this task for each Kubernetes service account that needs access to AWS resources. We can do this by defining the IAM role to associate with a service account in your cluster by adding the following annotation to the service account. apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> You can also annotate the experiment service account running the following command. kubectl annotate serviceaccount -n <SERVICE_ACCOUNT_NAMESPACE> <SERVICE_ACCOUNT_NAME> \\ eks.amazonaws.com/role-arn = arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> Verify that the experiment service account is now associated with the IAM Describe one of the pods and verify that the AWS_WEB_IDENTITY_TOKEN_FILE and AWS_ROLE_ARN environment variables exist. kubectl exec -n kube-system <aws-node-9rgzw> env | grep AWS Output: AWS_VPC_K8S_CNI_LOGLEVEL=DEBUG AWS_ROLE_ARN=arn:aws:iam::<ACCOUNT_ID>:role/<IAM_ROLE_NAME> AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token Now we have successfully enabled the experiment service accounts to access AWS resources.","title":"Step 3: Associate an IAM role with a service account"},{"location":"experiments/concepts/IAM/awsIamIntegration/#configure-the-experiment-cr","text":"Since we have already configured the IAM for the experiment service account we don\u2019t need to create secret and mount it with experiment CR which is enabled by default. To remove the secret mount we have to remove the following lines from experiment YAML. secrets : - name : cloud-secret mountPath : /tmp/ We can now run the experiment with the direct IAM integration.","title":"Configure the Experiment CR."},{"location":"experiments/concepts/chaos-resources/contents/","text":"Chaos Resources \u00b6 At the heart of the Litmus Platform are the chaos custom resources. This section consists of the specification (details of each field within the .spec & .status of the resources) as well as standard examples for tuning the supported parameters. Chaos Resource Name Description User Guide ChaosEngine Contains the ChaosEngine specifications user-guide ChaosEngine ChaosExperiment Contains the ChaosExperiment specifications user-guide ChaosExperiment ChaosResult Contains the ChaosResult specifications user-guide ChaosResult ChaosScheduler Contains the ChaosScheduler specifications user-guide ChaosScheduler Probes Contains the Probes specifications user-guide Probes","title":"Contents"},{"location":"experiments/concepts/chaos-resources/contents/#chaos-resources","text":"At the heart of the Litmus Platform are the chaos custom resources. This section consists of the specification (details of each field within the .spec & .status of the resources) as well as standard examples for tuning the supported parameters. Chaos Resource Name Description User Guide ChaosEngine Contains the ChaosEngine specifications user-guide ChaosEngine ChaosExperiment Contains the ChaosExperiment specifications user-guide ChaosExperiment ChaosResult Contains the ChaosResult specifications user-guide ChaosResult ChaosScheduler Contains the ChaosScheduler specifications user-guide ChaosScheduler Probes Contains the Probes specifications user-guide Probes","title":"Chaos Resources"},{"location":"experiments/concepts/chaos-resources/chaos-engine/application-details/","text":"It contains AUT and auxiliary applications details provided at spec.appinfo and spec.auxiliaryAppInfo respectively inside chaosengine. View the application specification schema Field .spec.appinfo.appns Description Flag to specify namespace of application under test Type Optional Range user-defined (type: string) Default n/a Notes The appns in the spec specifies the namespace of the AUT. Usually provided as a quoted string. It is optional for the infra chaos. Field .spec.appinfo.applabel Description Flag to specify unique label of application under test Type Optional Range user-defined (type: string)(pattern: \"label_key=label_value\") Default n/a Notes The applabel in the spec specifies a unique label of the AUT. Usually provided as a quoted string of pattern key=value. Note that if multiple applications share the same label within a given namespace, the AUT is filtered based on the presence of the chaos annotation litmuschaos.io/chaos: \"true\" . If, however, the annotationCheck is disabled, then a random application (pod) sharing the specified label is selected for chaos. It is optional for the infra chaos. Field .spec.appinfo.appkind Description Flag to specify resource kind of application under test Type Optional Range deployment , statefulset , daemonset , deploymentconfig , rollout Default n/a (depends on app type) Notes The appkind in the spec specifies the Kubernetes resource type of the app deployment. The Litmus ChaosOperator supports chaos on deployments, statefulsets and daemonsets. Application health check routines are dependent on the resource types, in case of some experiments. It is optional for the infra chaos Field .spec.auxiliaryAppInfo Description Flag to specify one or more app namespace-label pairs whose health is also monitored as part of the chaos experiment, in addition to a primary application specified in the .spec.appInfo . NOTE : If the auxiliary applications are deployed in namespaces other than the AUT, ensure that the chaosServiceAccount is bound to a cluster role and has adequate permissions to list pods on other namespaces. Type Optional Range user-defined (type: string)(pattern: \"namespace:label_key=label_value\"). Default n/a Notes The auxiliaryAppInfo in the spec specifies a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. Application Under Test \u00b6 It defines the appns , applabel , and appkind to set the namespace, labels, and kind of the application under test. appkind : It supports deployment , statefulset , daemonset , deploymentconfig , and rollout . It is mandatory for the pod-level experiments and optional for the rest of the experiments. Use the following example to tune this: # contains details of the AUT(application under test) # appns: name of the application # applabel: label of the applicaton # appkind: kind of the application. supports: deployment, statefulset, daemonset, rollout, deploymentconfig apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # AUT details appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Auxiliary Application Info \u00b6 The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Application Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/application-details/#application-under-test","text":"It defines the appns , applabel , and appkind to set the namespace, labels, and kind of the application under test. appkind : It supports deployment , statefulset , daemonset , deploymentconfig , and rollout . It is mandatory for the pod-level experiments and optional for the rest of the experiments. Use the following example to tune this: # contains details of the AUT(application under test) # appns: name of the application # applabel: label of the applicaton # appkind: kind of the application. supports: deployment, statefulset, daemonset, rollout, deploymentconfig apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # AUT details appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Application Under Test"},{"location":"experiments/concepts/chaos-resources/chaos-engine/application-details/#auxiliary-application-info","text":"The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Auxiliary Application Info"},{"location":"experiments/concepts/chaos-resources/chaos-engine/contents/","text":"Chaos Engine Specifications \u00b6 Bind an instance of a given app with one or more chaos experiments, define run characteristics, override chaos defaults, define steady-state hypothesis, reconciled by Litmus Chaos Operator. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same. Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications","title":"Contents"},{"location":"experiments/concepts/chaos-resources/chaos-engine/contents/#chaos-engine-specifications","text":"Bind an instance of a given app with one or more chaos experiments, define run characteristics, override chaos defaults, define steady-state hypothesis, reconciled by Litmus Chaos Operator. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same. Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications","title":"Chaos Engine Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/engine-state/","text":"It is a user-defined flag to trigger chaos. Setting it to active ensures the successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on the actual state of the ChaosEngine. It can be tuned via engineState field. It supports active and stop values. View the state specification schema Field .spec.engineState Description Flag to control the state of the chaosengine Type Mandatory Range active , stop Default active Notes The engineState in the spec is a user defined flag to trigger chaos. Setting it to active ensures successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on actual state of the ChaosEngine. Use the following example to tune this: # contains the chaosengine state # supports: active and stop states apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : # contains the state of engine engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"State Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/","text":"It contains all the experiment tunables provided at .spec.experiments[].spec.components inside chaosengine. View the experiment specification schema Field .spec.experiments[].spec.components.configMaps Description Configmaps passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.configMaps provides for a means to insert config information into the experiment. The configmaps definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.secrets Description Kubernetes secrets passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the experiment pods. These are especially useful in case of platform-level/infra-level chaos experiments. The secrets definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.experimentImage Description Override the image of the chaos experiment Type Optional Range string Default n/a Notes The experiment[].spec.components.experimentImage overrides the experiment image for the chaoexperiment. Field .spec.experiments[].spec.components.experimentImagePullSecrets Description Flag to specify imagePullSecrets for the ChaosExperiment Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.experimentImagePullSecrets allows developers to specify the imagePullSecret name for ChaosExperiment. Field .spec.experiments[].spec.components.nodeSelector Description Provide the node selector for the experiment pod Type Optional Range Labels in the from of label key=value Default n/a Notes The experiment[].spec.components.nodeSelector The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.experiments[].spec.components.statusCheckTimeouts Description Provides the timeout and retry values for the status checks. Defaults to 180s & 90 retries (2s per retry) Type Optional Range It contains values in the form {delay: int, timeout: int} Default delay: 2s and timeout: 180s Notes The experiment[].spec.components.statusCheckTimeouts The statusCheckTimeouts override the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. Field .spec.experiments[].spec.components.resources Description Specify the resource requirements for the ChaosExperiment pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The experiment[].spec.components.resources contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. Field .spec.experiments[].spec.components.experimentAnnotations Description Annotations that needs to be provided in the pod which will be created (experiment-pod) Type Optional Range user-defined (type: label key=value) Default n/a Notes The .spec.components.experimentAnnotation allows developers to specify the custom annotations for the experiment pod. Field .spec.experiments[].spec.components.tolerations Description Toleration for the experiment pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.tolerations Tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. Experiment Annotations \u00b6 It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment Experiment Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" Experiment Image \u00b6 It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\" Experiment ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred Experiment NodeSelectors \u00b6 The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos Experiment Resource Requirements \u00b6 It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" Experiment Tolerations \u00b6 It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" Experiment Status Check Timeout \u00b6 It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-annotations","text":"It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment","title":"Experiment Annotations"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\"","title":"Experiment Configmaps And Secrets"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-image","text":"It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\"","title":"Experiment Image"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred","title":"Experiment ImagePullSecrets"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-nodeselectors","text":"The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos","title":"Experiment NodeSelectors"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-resource-requirements","text":"It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\"","title":"Experiment Resource Requirements"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-tolerations","text":"It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\"","title":"Experiment Tolerations"},{"location":"experiments/concepts/chaos-resources/chaos-engine/experiment-components/#experiment-status-check-timeout","text":"It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Status Check Timeout"},{"location":"experiments/concepts/chaos-resources/chaos-engine/rbac-details/","text":"It specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment are provided in the .spec.definition.permissions field of the respective chaosexperiment CR. It can be tuned via chaosServiceAccount field. View the RBAC specification schema Field .spec.chaosServiceAccount Description Flag to specify serviceaccount used for chaos experiment Type Mandatory Range user-defined (type: string) Default n/a Notes The chaosServiceAccount in the spec specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment is provided in the .spec.definition.permissions field of the respective chaosexperiment CR. Use the following example to tune this: # contains name of the serviceAccount which contains all the RBAC permissions required for the experiment apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" # name of the service account w/ sufficient permissions chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"RBAC Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/","text":"It contains all the chaos-runner tunables provided at .spec.components.runner inside chaosengine. View the runner specification schema Field .spec.components.runner.image Description Flag to specify image of ChaosRunner pod Type Optional Range user-defined (type: string) Default n/a (refer Notes ) Notes The .components.runner.image allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE Field .spec.components.runner.imagePullPolicy Description Flag to specify imagePullPolicy for the ChaosRunner Type Optional Range Always , IfNotPresent Default IfNotPresent Notes The .components.runner.imagePullPolicy allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Field .spec.components.runner.imagePullSecrets Description Flag to specify imagePullSecrets for the ChaosRunner Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.imagePullSecrets allows developers to specify the imagePullSecret name for ChaosRunner. Field .spec.components.runner.runnerAnnotations Description Annotations that needs to be provided in the pod which will be created (runner-pod) Type Optional Range user-defined (type: map[string]string) Default n/a Notes The .components.runner.runnerAnnotation allows developers to specify the custom annotations for the runner pod. Field .spec.components.runner.args Description Specify the args for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.args allows developers to specify their own debug runner args. Field .spec.components.runner.command Description Specify the commands for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.command allows developers to specify their own debug runner commands. Field .spec.components.runner.configMaps Description Configmaps passed to the chaos runner pod Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.configMaps provides for a means to insert config information into the runner pod. Field .spec.components.runner.secrets Description Kubernetes secrets passed to the chaos runner pod. Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the chaos runner pod. These are especially useful in case of platform-level/infra-level chaos experiments. Field .spec.components.runner.nodeSelector Description Node selectors for the runner pod Type Optional Range Labels in the from of label key=value Default n/a Notes The .spec.components.runner.nodeSelector The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.components.runner.resources Description Specify the resource requirements for the ChaosRunner pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The .spec.components.runner.resources contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. Field .spec.components.runner.tolerations Description Toleration for the runner pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.runner.tolerations Provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. ChaosRunner Annotations \u00b6 It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Args And Command \u00b6 It defines the args and command to set the args and command of the chaos-runner respectively. args : It allows developers to specify their own debug runner args. command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. configMaps : It provides for a means to insert config information into the runner pod. secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Image and ImagePullPoicy \u00b6 It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner NodeSelectors \u00b6 The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Resource Requirements \u00b6 It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Tolerations \u00b6 It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runner Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-annotations","text":"It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Annotations"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-args-and-command","text":"It defines the args and command to set the args and command of the chaos-runner respectively. args : It allows developers to specify their own debug runner args. command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Args And Command"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. configMaps : It provides for a means to insert config information into the runner pod. secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Configmaps And Secrets"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-image-and-imagepullpoicy","text":"It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Image and ImagePullPoicy"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner ImagePullSecrets"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-nodeselectors","text":"The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner NodeSelectors"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-resource-requirements","text":"It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Resource Requirements"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runner-components/#chaosrunner-tolerations","text":"It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Tolerations"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runtime-details/","text":"It contains runtime details of the chaos experiments provided at .spec inside chaosengine. View the runtime specification schema Field .spec.annotationCheck Description Flag to control annotationChecks on applications as prerequisites for chaos Type Optional Range true , false Default true Notes The annotationCheck in the spec controls whether or not the operator checks for the annotation \"litmuschaos.io/chaos\" to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated, while setting it to false suppresses this check and proceeds with chaos injection. Field .spec.terminationGracePeriodSeconds Description Flag to control terminationGracePeriodSeconds for the chaos pods(abort case) Type Optional Range integer value Default 30 Notes The terminationGracePeriodSeconds in the spec controls the terminationGracePeriodSeconds for the chaos resources in abort case. Chaos pods contains chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before completely terminated. Field .spec.jobCleanUpPolicy Description Flag to control cleanup of chaos experiment job post execution of chaos Type Optional Range delete , retain Default delete Notes <The jobCleanUpPolicy controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). Annotation Check \u00b6 It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Jobcleanup Policy \u00b6 It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanUpPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Termination Grace Period Seconds \u00b6 It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runtime Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runtime-details/#annotation-check","text":"It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Annotation Check"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runtime-details/#jobcleanup-policy","text":"It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanUpPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Jobcleanup Policy"},{"location":"experiments/concepts/chaos-resources/chaos-engine/runtime-details/#termination-grace-period-seconds","text":"It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Termination Grace Period Seconds"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/component-specification/","text":"It contains component details provided at spec.definition inside chaosexperiment View the component specification schema Field .spec.definition.image Description Flag to specify the image to run the ChaosExperiment Type Mandatory Range user-defined (type: string) Default n/a (refer Notes) Notes The .spec.definition.image allows the developers to specify their experiment images. Typically set to the Litmus go-runner or the ansible-runner . This feature of the experiment enables BYOC (BringYourOwnChaos), where developers can implement their own variants of a standard chaos experiment Field .spec.definition.imagePullPolicy Description Flag that helps the developers to specify imagePullPolicy for the ChaosExperiment Type Mandatory Range IfNotPresent , Always (type: string) Default Always Notes The .spec.definition.imagePullPolicy allows developers to specify the pull policy for ChaosExperiment image. Set to Always during debug/test Field .spec.definition.args Description Flag to specify the entrypoint for the ChaosExperiment Type Mandatory Range user-defined (type:list of string) Default n/a Notes The .spec.definition.args specifies the entrypoint for the ChaosExperiment. It depends on the language used in the experiment. For litmus-go the .spec.definition.args contains a single binary of all experiments and managed via -name flag to indicate experiment to run( -name (exp-name) ). Field .spec.definition.command Description Flag to specify the shell on which the ChaosExperiment will execute Type Mandatory Range user-defined (type: list of string). Default /bin/bash Notes The .spec.definition.command specifies the shell used to run the experiment /bin/bash is the most common shell to be used. Image \u00b6 It allows the developers to specify their experiment images. Typically set to the Litmus go-runner or the ansible-runner. This feature of the experiment enables BYOC (BringYourOwnChaos), where developers can implement their own variants of a standard chaos experiment. It can be tuned via image field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" # image of the chaosexperiment image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest ImagePullPolicy \u00b6 It allows developers to specify the pull policy for ChaosExperiment image. Set to Always during debug/test. It can be tuned via imagePullPolicy field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" # imagePullPolicy of the chaosexperiment imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Args \u00b6 It specifies the entrypoint for the ChaosExperiment. It depends on the language used in the experiment. For litmus-go the .spec.definition.args contains a single binary of all experiments and managed via -name flag to indicate experiment to run(-name (exp-name)). It can be tuned via args field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always # it contains args of the experiment args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Command \u00b6 It specifies the shell used to run the experiment /bin/bash is the most common shell to be used. It can be tuned via command field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete # it contains command of the experiment command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Component Specification"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/component-specification/#image","text":"It allows the developers to specify their experiment images. Typically set to the Litmus go-runner or the ansible-runner. This feature of the experiment enables BYOC (BringYourOwnChaos), where developers can implement their own variants of a standard chaos experiment. It can be tuned via image field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" # image of the chaosexperiment image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Image"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/component-specification/#imagepullpolicy","text":"It allows developers to specify the pull policy for ChaosExperiment image. Set to Always during debug/test. It can be tuned via imagePullPolicy field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" # imagePullPolicy of the chaosexperiment imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"ImagePullPolicy"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/component-specification/#args","text":"It specifies the entrypoint for the ChaosExperiment. It depends on the language used in the experiment. For litmus-go the .spec.definition.args contains a single binary of all experiments and managed via -name flag to indicate experiment to run(-name (exp-name)). It can be tuned via args field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always # it contains args of the experiment args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Args"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/component-specification/#command","text":"It specifies the shell used to run the experiment /bin/bash is the most common shell to be used. It can be tuned via command field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete # it contains command of the experiment command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Command"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/","text":"It contains configuration details provided at spec.definition inside chaosexperiment View the configuration specification schema Field .spec.definition.labels Description Flag to specify the label for the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.labels allow developers to specify the ChaosPod label for an experiment. Field .spec.definition.securityContext.podSecurityContext Description Flag to specify security context for ChaosPod Type Optional Range user-defined (type:corev1.PodSecurityContext) Default n/a Notes The .spec.definition.securityContext.podSecurityContext allows the developers to specify the security context for the ChaosPod which applies to all containers inside the Pod. Field .spec.definition.securityContext.containerSecurityContext.privileged Description Flag to specify the security context for the ChaosExperiment pod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.securityContext.containerSecurityContext.privileged specify the securityContext params to the experiment container. Field .spec.definition.configMaps Description Flag to specify the configmap for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.configMaps allows the developers to mount the ConfigMap volume into the experiment pod. Field .spec.definition.secrets Description Flag to specify the secrets for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.secrets specify the secret data to be passed for the ChaosPod. The secrets typically contains confidential information like credentials. Field .spec.definition.experimentAnnotations Description Flag to specify the custom annotation to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.experimentAnnotations allows the developer to specify the Custom annotation for the chaos pod. Field .spec.definition.hostFileVolumes Description Flag to specify the host file volumes to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.hostFileVolumes allows the developer to specify the host file volumes to the ChaosPod. Field .spec.definition.hostPID Description Flag to specify the host PID for the ChaosPod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.hostPID allows the developer to specify the host PID for the ChaosPod. Labels \u00b6 It allows developers to specify the ChaosPod label for an experiment. It can be tuned via labels field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains experiment labels labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest PodSecurityContext \u00b6 It allows the developers to specify the security context for the ChaosPod which applies to all containers inside the Pod. It can be tuned via podSecurityContext field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains pod security context securityContext : podSecurityContext : allowPrivilegeEscalation : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Container Security Context \u00b6 It allows the developers to specify the security context for the container inside ChaosPod. It can be tuned via containerSecurityContext field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains container security context securityContext : containerSecurityContext : privileged : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest ConfigMaps \u00b6 It allows the developers to mount the ConfigMap volume into the experiment pod. It can tuned via configMaps field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains configmaps details configMaps : - name : experiment-data mountPath : \"/mnt\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Secrets \u00b6 It specify the secret data to be passed for the ChaosPod. The secrets typically contains confidential information like credentials. It can be tuned via secret field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains secret details secret : - name : auth-credentials mountPath : \"/tmp\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Experiment Annotations \u00b6 It allows the developer to specify the Custom annotation for the chaos pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains experiment annotations experimentAnnotations : context : chaos labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Host File Volumes \u00b6 It allows the developer to specify the host file volumes to the ChaosPod. It can be tuned via hostFileVolumes field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains host file volumes hostFileVolumes : - name : socket file mountPath : \"/var/run/docker.sock\" nodePath : \"/var/run/docker.sock\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Host PID \u00b6 It allows the developer to specify the host PID for the ChaosPod. It can be tuned via hostPID field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it allows hostPID hostPID : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Configuration Specification"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#labels","text":"It allows developers to specify the ChaosPod label for an experiment. It can be tuned via labels field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains experiment labels labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Labels"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#podsecuritycontext","text":"It allows the developers to specify the security context for the ChaosPod which applies to all containers inside the Pod. It can be tuned via podSecurityContext field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains pod security context securityContext : podSecurityContext : allowPrivilegeEscalation : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"PodSecurityContext"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#container-security-context","text":"It allows the developers to specify the security context for the container inside ChaosPod. It can be tuned via containerSecurityContext field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains container security context securityContext : containerSecurityContext : privileged : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Container Security Context"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#configmaps","text":"It allows the developers to mount the ConfigMap volume into the experiment pod. It can tuned via configMaps field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains configmaps details configMaps : - name : experiment-data mountPath : \"/mnt\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"ConfigMaps"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#secrets","text":"It specify the secret data to be passed for the ChaosPod. The secrets typically contains confidential information like credentials. It can be tuned via secret field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains secret details secret : - name : auth-credentials mountPath : \"/tmp\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Secrets"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#experiment-annotations","text":"It allows the developer to specify the Custom annotation for the chaos pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains experiment annotations experimentAnnotations : context : chaos labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Experiment Annotations"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#host-file-volumes","text":"It allows the developer to specify the host file volumes to the ChaosPod. It can be tuned via hostFileVolumes field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it contains host file volumes hostFileVolumes : - name : socket file mountPath : \"/var/run/docker.sock\" nodePath : \"/var/run/docker.sock\" labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Host File Volumes"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/configuration-specification/#host-pid","text":"It allows the developer to specify the host PID for the ChaosPod. It can be tuned via hostPID field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' # it allows hostPID hostPID : true labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Host PID"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/contents/","text":"Chaos Experiment Specifications \u00b6 Granular definition of chaos intent specified via image, librar, necessary permissions, low-level chaos parameters (default values). This section describes the fields in the ChaosExperiment and the possible values that can be set against the same. Field Name Description User Guide Scope Specification It defines scope of the chaosexperiment Scope Specifications Component Specification It defines component details of the chaosexperiment Component Specifications Experiment Tunables Specification It defines tunables of the chaosexperiment Experiment Tunables Specification Configuration Specification It defines configuration details of the chaosexperiment Configuration Specification","title":"Contents"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/contents/#chaos-experiment-specifications","text":"Granular definition of chaos intent specified via image, librar, necessary permissions, low-level chaos parameters (default values). This section describes the fields in the ChaosExperiment and the possible values that can be set against the same. Field Name Description User Guide Scope Specification It defines scope of the chaosexperiment Scope Specifications Component Specification It defines component details of the chaosexperiment Component Specifications Experiment Tunables Specification It defines tunables of the chaosexperiment Experiment Tunables Specification Configuration Specification It defines configuration details of the chaosexperiment Configuration Specification","title":"Chaos Experiment Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/experiment-tunable-specification/","text":"It contains the array of tunables passed to the experiment pods as environment variables. It is used to manage the experiment execution. We can set the default values for all the variables (tunable) here which can be overridden by ChaosEngine from .spec.experiments[].spec.components.env if required. To know about the variables that need to be overridden check the list of \"mandatory\" & \"optional\" env for an experiment as provided within the respective experiment documentation. It can be provided at spec.definition.env inside chaosexperiment. View the experiment tunables specification Field .spec.definition.env Description Flag to specify env used for ChaosExperiment Type Mandatory Range user-defined (type: {name: string, value: string}) Default n/a Notes The .spec.definition.env specifies the array of tunables passed to the experiment pods as environment variables. It is used to manage the experiment execution. We can set the default values for all the variables (tunable) here which can be overridden by ChaosEngine from .spec.experiments[].spec.components.env if required. To know about the variables that need to be overridden check the list of \"mandatory\" & \"optional\" env for an experiment as provided within the respective experiment documentation. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced # permissions for the chaosexperiment permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash # it contains experiment tunables env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Experiment Tunables Specification"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/scope-specification/","text":"It contains scope and permissions details provided at spec.definition.scope and spec.definition.permissions respectively inside chaosexperiment. View the scope specification schema Field .spec.definition.scope Description Flag to specify the scope of the ChaosExperiment Type Optional Range Namespaced , Cluster Default n/a (depends on experiment type) Notes The .spec.definition.scope specifies the scope of the experiment. It can be Namespaced scope for pod level experiments and Cluster for the experiments having a cluster wide impact. Field .spec.definition.permissions Description Flag to specify the minimum permission to run the ChaosExperiment Type Optional Range user-defined (type: list) Default n/a Notes The .spec.definition.permissions specify the minimum permission that is required to run the ChaosExperiment. It also helps to estimate the blast radius for the ChaosExperiment. Experiment Scope \u00b6 It specifies the scope of the experiment. It can be Namespaced scope for pod level experiments and Cluster for the experiments having a cluster wide impact. It can be tuned via scope field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : # scope of the chaosexperiment scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest Experiment Permissions \u00b6 It specify the minimum permission that is required to run the ChaosExperiment. It also helps to estimate the blast radius for the ChaosExperiment. It can be tuned via permissions field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced # permissions for the chaosexperiment permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' ## it defines the sequence of chaos execution for multiple target pods ## supported values: serial, parallel - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Scope Specification"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/scope-specification/#experiment-scope","text":"It specifies the scope of the experiment. It can be Namespaced scope for pod level experiments and Cluster for the experiments having a cluster wide impact. It can be tuned via scope field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : # scope of the chaosexperiment scope : Namespaced permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Experiment Scope"},{"location":"experiments/concepts/chaos-resources/chaos-experiment/scope-specification/#experiment-permissions","text":"It specify the minimum permission that is required to run the ChaosExperiment. It also helps to estimate the blast radius for the ChaosExperiment. It can be tuned via permissions field. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 description : message : | Deletes a pod belonging to a deployment/statefulset/daemonset kind : ChaosExperiment metadata : name : pod-delete labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : chaosexperiment app.kubernetes.io/version : latest spec : definition : scope : Namespaced # permissions for the chaosexperiment permissions : - apiGroups : - \"\" - \"apps\" - \"apps.openshift.io\" - \"argoproj.io\" - \"batch\" - \"litmuschaos.io\" resources : - \"deployments\" - \"jobs\" - \"pods\" - \"pods/log\" - \"replicationcontrollers\" - \"deployments\" - \"statefulsets\" - \"daemonsets\" - \"replicasets\" - \"deploymentconfigs\" - \"rollouts\" - \"pods/exec\" - \"events\" - \"chaosengines\" - \"chaosexperiments\" - \"chaosresults\" verbs : - \"create\" - \"list\" - \"get\" - \"patch\" - \"update\" - \"delete\" - \"deletecollection\" image : \"litmuschaos/go-runner:latest\" imagePullPolicy : Always args : - -c - ./experiments -name pod-delete command : - /bin/bash env : - name : TOTAL_CHAOS_DURATION value : '15' - name : RAMP_TIME value : '' - name : FORCE value : 'true' - name : CHAOS_INTERVAL value : '5' - name : PODS_AFFECTED_PERC value : '' - name : LIB value : 'litmus' - name : TARGET_PODS value : '' ## it defines the sequence of chaos execution for multiple target pods ## supported values: serial, parallel - name : SEQUENCE value : 'parallel' labels : name : pod-delete app.kubernetes.io/part-of : litmus app.kubernetes.io/component : experiment-job app.kubernetes.io/version : latest","title":"Experiment Permissions"},{"location":"experiments/concepts/chaos-resources/chaos-result/contents/","text":"Chaos Result Specifications \u00b6 Hold engine reference, experiment state, verdict(on complete), salient application/result attributes, sources for metrics collection This section describes the fields in the ChaosResult and the possible values that can be set against the same. Field Name Description User Guide Spec Specification It defines spec details of the chaosresult Spec Specification Status Specification It defines status details of the chaosresult Status Specification Probe Specification It defines component details of the chaosresult Probe Specification","title":"Contents"},{"location":"experiments/concepts/chaos-resources/chaos-result/contents/#chaos-result-specifications","text":"Hold engine reference, experiment state, verdict(on complete), salient application/result attributes, sources for metrics collection This section describes the fields in the ChaosResult and the possible values that can be set against the same. Field Name Description User Guide Spec Specification It defines spec details of the chaosresult Spec Specification Status Specification It defines status details of the chaosresult Status Specification Probe Specification It defines component details of the chaosresult Probe Specification","title":"Chaos Result Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-result/probe-specification/","text":"It contains probe details provided at status.probeStatus inside chaosresult. It contains following fields: name : Flag to show the name of probe used in the experiment type : Flag to show the type of probe used status.continuous : Flag to show the result of probe in continuous mode status.prechaos : Flag to show the result of probe in pre chaos status.postchaos : Flag to show the result of probe in post chaos View the probe schema Field .status.probestatus.name Description Flag to show the name of probe used in the experiment Range n/a n/a (type: string) Notes The .status.probestatus.name shows the name of the probe used in the experiment. Field .status.probestatus.type Description Flag to show the type of probe used Range HTTPProbe,K8sProbe,CmdProbe (type:string) Notes The .status.probestatus.type shows the type of probe used. Field .status.probestatus.status.continuous Description Flag to show the result of probe in continuous mode Range Awaited,Passed,Better Luck Next Time (type: string) Notes The .status.probestatus.status.continuous helps to get the result of the probe in the continuous mode. The httpProbe is better used in the Continuous mode. Field .status.probestatus.status.postchaos Description Flag to show the probe result post chaos Range Awaited,Passed,Better Luck Next Time (type:map[string]string) Notes The .status.probestatus.status.postchaos shows the result of probe setup in EOT mode executed at the End of Test as a post-chaos check. Field .status.probestatus.status.prechaos Description Flag to show the probe result pre chaos Range Awaited,Passed,Better Luck Next Time (type:string) Notes The .status.probestatus.status.prechaos shows the result of probe setup in SOT mode executed at the Start of Test as a pre-chaos check. view the sample example: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : Engine : engine-nginx Experiment : pod-delete Status : Experiment Status : Fail Step : N/A Phase : Completed Probe Success Percentage : 100 Verdict : Pass History : Failed Runs : 1 Passed Runs : 1 Stopped Runs : 0 Targets : Chaos Status : targeted Kind : deployment Name : hello Probe Status : # name of probe Name : check-frontend-access-url # status of probe Status : Continuous : Passed \ud83d\udc4d #Continuous # type of probe Type : HTTPProbe # name of probe Name : check-app-cluster-cr-status # status of probe Status : Post Chaos : Passed \ud83d\udc4d #EoT # type of probe Type : K8sProbe # name of probe Name : check-database-integrity # status of probe Status : Post Chaos : Passed \ud83d\udc4d #Edge Pre Chaos : Passed \ud83d\udc4d # type of probe Type : CmdProbe Events : <none>","title":"Probe Status"},{"location":"experiments/concepts/chaos-resources/chaos-result/spec-specification/","text":"It contains spec details provided at spec inside chaosresult. The name of chaosengine and chaosexperiment are present at spec.engine and spec.experiment respectively. View the spec details schema Field .spec.engine Description Flag to hold the ChaosEngine name for the experiment Range n/a (type: string) Notes The .spec.engine holds the engine name for the current course of the experiment. Field .spec.experiment Description Flag to hold the ChaosExperiment name which induces chaos. Range n/a (type: string) Notes The .spec.experiment holds the ChaosExperiment name for the current course of the experiment. view the sample chaosresult: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : # name of the chaosengine Engine : engine-nginx # name of the chaosexperiment Experiment : pod-delete Status : Experiment Status : Fail Step : N/A Phase : Completed Probe Success Percentage : 100 Verdict : Pass History : Failed Runs : 1 Passed Runs : 1 Stopped Runs : 0 Targets : Chaos Status : targeted Kind : deployment Name : hello Events : <none>","title":"Spec Specification"},{"location":"experiments/concepts/chaos-resources/chaos-result/status-specification/","text":"It contains status details provided at status inside chaosresult. Experiment Status \u00b6 It contains experiment status provided at status.experimentStatus inside chaosresult. It contains following fields: failStep : Flag to show the failure step of the ChaosExperiment phase : Flag to show the current phase of the experiment probesuccesspercentage : Flag to show the probe success percentage verdict : Flag to show the verdict of the experiment View the experiment status Field .status.experimentStatus.failstep Description Flag to show the failure step of the ChaosExperiment Range n/a (type: string) Notes The .status.experimentStatus.failstep Show the step at which the experiment failed. It helps in faster debugging of failures in the experiment execution. Field .status.experimentStatus.phase Description Flag to show the current phase of the experiment Range Awaited,Running,Completed,Aborted (type: string) Notes The .status.experimentStatus.phase shows the current phase in which the experiment is. It gets updated as the experiment proceeds.If the experiment is aborted then the status will be Aborted. Field .status.experimentStatus.probesuccesspercentage Description Flag to show the probe success percentage Range 1 to 100 (type: int) Notes The .status.experimentStatus.probesuccesspercentage shows the probe success percentage which is a ratio of successful checks v/s total probes. Field .status.experimentStatus.verdict Description Flag to show the verdict of the experiment. Range Awaited,Pass,Fail,Stopped (type: string) Notes The .status.experimentStatus.verdict shows the verdict of the experiment. It is Awaited when the experiment is in progress and ends up with Pass or Fail according to the experiment result. view the sample example: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : Engine : engine-nginx Experiment : pod-delete Status : Experiment Status : # step on which experiment fails Fail Step : N/A # phase of the chaos result Phase : Completed # Success Percentage of the litmus probes Probe Success Percentage : 100 # Verdict of the chaos result Verdict : Pass History : Failed Runs : 1 Passed Runs : 1 Stopped Runs : 0 Targets : Chaos Status : targeted Kind : deployment Name : hello Events : <none> Result History \u00b6 It contains history of experiment runs present at status.history . It contains following fields: passedRuns : It contains cumulative passed run count failedRuns : It contains cumulative failed run count stoppedRuns : It contains cumulative stopped run count targets.name : It contains name of target application target.kind : It contains kinds of target application target.chaosStatus : It contains chaos status View the history details Field .status.history.passedRuns Description It contains cumulative passed run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.passedRuns contains cumulative passed run counts for a specific ChaosResult. Field .status.history.failedRuns Description It contains cumulative failed run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.failedRuns contains cumulative failed run counts for a specific ChaosResult. Field .status.history.stoppedRuns Description It contains cumulative stopped run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.stoppedRuns contains cumulative stopped run counts for a specific ChaosResult. Field .status.history.targets.name Description It contains name of the target application Range string Notes The .status.history.targets.name contains name of the target application Field .status.history.targets.kind Description It contains kind of the target application Range string Notes The .status.history.targets.kind contains kind of the target application Field .status.history.targets.chaosStatus Description It contains status of the chaos Range targeted, injected, reverted Notes The .status.history.targets.chaosStatus contains status of the chaos view the sample example: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : Engine : engine-nginx Experiment : pod-delete Status : Experiment Status : Fail Step : N/A Phase : Completed Probe Success Percentage : 100 Verdict : Pass History : # fail experiment run count Failed Runs : 1 # passed experiment run count Passed Runs : 1 # stopped experiment run count Stopped Runs : 0 Targets : # status of the chaos Chaos Status : targeted # kind of the application Kind : deployment # name of the application Name : hello Events : <none>","title":"Status Specification"},{"location":"experiments/concepts/chaos-resources/chaos-result/status-specification/#experiment-status","text":"It contains experiment status provided at status.experimentStatus inside chaosresult. It contains following fields: failStep : Flag to show the failure step of the ChaosExperiment phase : Flag to show the current phase of the experiment probesuccesspercentage : Flag to show the probe success percentage verdict : Flag to show the verdict of the experiment View the experiment status Field .status.experimentStatus.failstep Description Flag to show the failure step of the ChaosExperiment Range n/a (type: string) Notes The .status.experimentStatus.failstep Show the step at which the experiment failed. It helps in faster debugging of failures in the experiment execution. Field .status.experimentStatus.phase Description Flag to show the current phase of the experiment Range Awaited,Running,Completed,Aborted (type: string) Notes The .status.experimentStatus.phase shows the current phase in which the experiment is. It gets updated as the experiment proceeds.If the experiment is aborted then the status will be Aborted. Field .status.experimentStatus.probesuccesspercentage Description Flag to show the probe success percentage Range 1 to 100 (type: int) Notes The .status.experimentStatus.probesuccesspercentage shows the probe success percentage which is a ratio of successful checks v/s total probes. Field .status.experimentStatus.verdict Description Flag to show the verdict of the experiment. Range Awaited,Pass,Fail,Stopped (type: string) Notes The .status.experimentStatus.verdict shows the verdict of the experiment. It is Awaited when the experiment is in progress and ends up with Pass or Fail according to the experiment result. view the sample example: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : Engine : engine-nginx Experiment : pod-delete Status : Experiment Status : # step on which experiment fails Fail Step : N/A # phase of the chaos result Phase : Completed # Success Percentage of the litmus probes Probe Success Percentage : 100 # Verdict of the chaos result Verdict : Pass History : Failed Runs : 1 Passed Runs : 1 Stopped Runs : 0 Targets : Chaos Status : targeted Kind : deployment Name : hello Events : <none>","title":"Experiment Status"},{"location":"experiments/concepts/chaos-resources/chaos-result/status-specification/#result-history","text":"It contains history of experiment runs present at status.history . It contains following fields: passedRuns : It contains cumulative passed run count failedRuns : It contains cumulative failed run count stoppedRuns : It contains cumulative stopped run count targets.name : It contains name of target application target.kind : It contains kinds of target application target.chaosStatus : It contains chaos status View the history details Field .status.history.passedRuns Description It contains cumulative passed run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.passedRuns contains cumulative passed run counts for a specific ChaosResult. Field .status.history.failedRuns Description It contains cumulative failed run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.failedRuns contains cumulative failed run counts for a specific ChaosResult. Field .status.history.stoppedRuns Description It contains cumulative stopped run count Range ANY NON NEGATIVE INTEGER Notes The .status.history.stoppedRuns contains cumulative stopped run counts for a specific ChaosResult. Field .status.history.targets.name Description It contains name of the target application Range string Notes The .status.history.targets.name contains name of the target application Field .status.history.targets.kind Description It contains kind of the target application Range string Notes The .status.history.targets.kind contains kind of the target application Field .status.history.targets.chaosStatus Description It contains status of the chaos Range targeted, injected, reverted Notes The .status.history.targets.chaosStatus contains status of the chaos view the sample example: Name : engine-nginx-pod-delete Namespace : default Labels : app.kubernetes.io/component=experiment-job app.kubernetes.io/part-of=litmus app.kubernetes.io/version=1.13.8 chaosUID=aa0a0084-f20f-4294-a879-d6df9aba6f9b controller-uid=6943c955-0154-4542-8745-de991eb47c61 job-name=pod-delete-w4p5op name=engine-nginx-pod-delete Annotations : <none> API Version : litmuschaos.io/v1alpha1 Kind : ChaosResult Metadata : Creation Timestamp : 2021-09-29T13:28:59Z Generation : 6 Resource Version : 66788 Self Link : /apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/engine-nginx-pod-delete UID : fe7f01c8-8118-4761-8ff9-0a87824d863f Spec : Engine : engine-nginx Experiment : pod-delete Status : Experiment Status : Fail Step : N/A Phase : Completed Probe Success Percentage : 100 Verdict : Pass History : # fail experiment run count Failed Runs : 1 # passed experiment run count Passed Runs : 1 # stopped experiment run count Stopped Runs : 0 Targets : # status of the chaos Chaos Status : targeted # kind of the application Kind : deployment # name of the application Name : hello Events : <none>","title":"Result History"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/contents/","text":"Chaos Scheduler Specifications \u00b6 Hold attributes for repeated execution (run now, once@timestamp, b/w start-end timestamp@ interval). Embeds the ChaosEngine as template This section describes the fields in the ChaosScheduler and the possible values that can be set against the same. Parameter Description User Guide Schedule Once Schedule chaos once on specified time or now Schedule Once Repeat Schedule Schedule chaos in repeat mode Repeat Schedule Schedule State Defines the state of the schedule Schedule State Engine Specifications Defines the chaosengine specifications Engine Specifications","title":"Contents"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/contents/#chaos-scheduler-specifications","text":"Hold attributes for repeated execution (run now, once@timestamp, b/w start-end timestamp@ interval). Embeds the ChaosEngine as template This section describes the fields in the ChaosScheduler and the possible values that can be set against the same. Parameter Description User Guide Schedule Once Schedule chaos once on specified time or now Schedule Once Repeat Schedule Schedule chaos in repeat mode Repeat Schedule Schedule State Defines the state of the schedule Schedule State Engine Specifications Defines the chaosengine specifications Engine Specifications","title":"Chaos Scheduler Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/engine-specification/","text":"Engine Specification \u00b6 It embeds the ChaosEngine as a template inside schedule CR. Which contains the chaosexperiment and target application details. View the engine details Field .spec.engineTemplateSpec Description Flag to control chaosengine to be formed Type Mandatory Range n/a Default n/a Notes The engineTemplateSpec is the ChaosEngineSpec of ChaosEngine that is to be formed. Engine Specification \u00b6 Specify the chaosengine details at spec.engineTemplateSpec inside schedule CR apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : #format should be like \"10m\" or \"2h\" accordingly for minutes or hours minChaosInterval : \"2m\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Engine Specifications"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/engine-specification/#engine-specification","text":"It embeds the ChaosEngine as a template inside schedule CR. Which contains the chaosexperiment and target application details. View the engine details Field .spec.engineTemplateSpec Description Flag to control chaosengine to be formed Type Mandatory Range n/a Default n/a Notes The engineTemplateSpec is the ChaosEngineSpec of ChaosEngine that is to be formed.","title":"Engine Specification"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/engine-specification/#engine-specification_1","text":"Specify the chaosengine details at spec.engineTemplateSpec inside schedule CR apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : #format should be like \"10m\" or \"2h\" accordingly for minutes or hours minChaosInterval : \"2m\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Engine Specification"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/","text":"Schedule Once \u00b6 It schedule the chaos once either on the specified time or immediately after creation of schedule CR. View the schedule once schema Schedule NOW \u00b6 Field .spec.schedule.now Description Flag to control the type of scheduling Type Mandatory Range true , false Default n/a Notes The now in the spec.schedule ensures immediate creation of chaosengine, i.e., injection of chaos. Schedule Once \u00b6 Field .spec.schedule.once.executionTime Description Flag to specify execution timestamp at which chaos is injected, when the policy is once . The chaosengine is created exactly at this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes .spec.schedule.once refers to a single-instance execution of chaos at a particular timestamp specified by .spec.schedule.once.executionTime Immediate Chaos \u00b6 It schedule the chaos immediately after creation of the chaos-schedule CR. It can be tuned via setting spec.schedule.now to true . apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : now : true engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Chaos at a Specified TimeStamp \u00b6 It schedule the chaos once at the specified time. It can be tuned via setting spec.schedule.once.executionTime . The execution time should be in UTC Timezone . apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : once : #should be modified according to current UTC Time executionTime : \"2020-05-12T05:47:00Z\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Schedule Once"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/#schedule-once","text":"It schedule the chaos once either on the specified time or immediately after creation of schedule CR. View the schedule once schema","title":"Schedule Once"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/#schedule-now","text":"Field .spec.schedule.now Description Flag to control the type of scheduling Type Mandatory Range true , false Default n/a Notes The now in the spec.schedule ensures immediate creation of chaosengine, i.e., injection of chaos.","title":"Schedule NOW"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/#schedule-once_1","text":"Field .spec.schedule.once.executionTime Description Flag to specify execution timestamp at which chaos is injected, when the policy is once . The chaosengine is created exactly at this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes .spec.schedule.once refers to a single-instance execution of chaos at a particular timestamp specified by .spec.schedule.once.executionTime","title":"Schedule Once"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/#immediate-chaos","text":"It schedule the chaos immediately after creation of the chaos-schedule CR. It can be tuned via setting spec.schedule.now to true . apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : now : true engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Immediate Chaos"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-once/#chaos-at-a-specified-timestamp","text":"It schedule the chaos once at the specified time. It can be tuned via setting spec.schedule.once.executionTime . The execution time should be in UTC Timezone . apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : once : #should be modified according to current UTC Time executionTime : \"2020-05-12T05:47:00Z\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Chaos at a Specified TimeStamp"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/","text":"Repeat Schedule \u00b6 It schedule the chaos in the repeat mode. There are various ways we can set up this type of schedule by varying the the fields inside spec.repeat . Note - We have just one field i.e. minChaosInterval to be specified as mandatory one. All other fields are optional and totally dependent on the desired behaviour. View the schedule repeat schema Field .spec.schedule.repeat.timeRange.startTime Description Flag to specify start timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created before this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When startTime is specified against the policy repeat , ChaosEngine will not be formed before this time, no matter when it was created. Field .spec.schedule.repeat.timeRange.endTime Description Flag to specify end timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created after this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When endTime is specified against the policy repeat , ChaosEngine will not be formed after this time. Field .spec.schedule.repeat.properties.minChaosInterval.hour.everyNthHour Description Flag to specify the hours between each successive schedule Type Mandatory Range integer Default n/a Notes The minChaosInterval.hour.everyNthHour in the spec specifies the time interval in hours between each schedule Field .spec.schedule.repeat.properties.minChaosInterval.hour.minuteOfTheHour Description Flag to specify minute of hour for each successive schedule Type Mandatory Range integer Default 0 Notes The minChaosInterval.hour.minuteOfTheHour in the spec specifies the minute of the hour between each schedule Field .spec.schedule.repeat.properties.minChaosInterval.minute.everyNthMinute Description Flag to specify the minutes for each successive schedule Type Mandatory Range integer Default n/a Notes The minChaosInterval.hour.everyNthMinute in the spec specifies the time interval in minutes between each schedule Field .spec.schedule.repeat.workDays.includedDays Description Flag to specify the days at which chaos is allowed to take place Type Mandatory Range user-defined (type: string)(pattern: [{day_name},{day_name}...]). Default n/a Notes The includedDays in the spec specifies a (comma-separated) list of days of the week at which chaos is allowed to take place. {day_name} is to be specified with the first 3 letters of the name of day such as Mon , Tue etc. Field .spec.schedule.repeat.workHours.includedHours Description Flag to specify the hours at which chaos is allowed to take place Type Mandatory Range {hour_number} will range from 0 to 23 (type: string)(pattern: {hour_number}-{hour_number}). Default n/a Notes The includedHours in the spec specifies a range of hours of the day at which chaos is allowed to take place. 24 hour format is followed Basic Schema to Execute Repeat Strategy \u00b6 This will keep executing the schedule and creating engines for an indefinite amount of time. Schedule ChaosEngine at every nth minute \u00b6 apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos at every 5 minutes minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Schedule ChaosEngine at every nth hour \u00b6 apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos every hour at 0th minute hour : everyNthHour : 1 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Schedule ChaosEngine at nth minute of every nth hour \u00b6 apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos every hour at 30th minute hour : everyNthHour : 1 minuteOfTheHour : 30 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Specifying Time Range for the Chaos Schedule \u00b6 This will manipulate the schedule to be started and ended according to our definition. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time startTime : \"2020-05-12T05:47:00Z\" endTime : \"2020-09-13T02:58:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Specifying Just the End Time \u00b6 Assumes the custom resource creation timestamp as the StartTime apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time endTime : \"2020-09-13T02:58:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Specifying Just the StartTime \u00b6 Executes chaos indefinitely (until the ChaosSchedule CR is removed) starting from the specified timestamp apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time startTime : \"2020-05-12T05:47:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Specifying Work Hours \u00b6 This ensures chaos execution within the specified hours of the day, everyday. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : minute : everyNthMinute : 5 workHours : # format should be <starting-hour-number>-<ending-hour-number>(inclusive) includedHours : 0-12 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' # It can be true/false annotationCheck : 'true' #ex. values: ns1:name=percona,ns2:run=nginx auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa # It can be delete/retain jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false' Specifying work days \u00b6 This executes chaos on specified days of the week, with the specified minimum interval. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : minute : everyNthMinute : 5 workDays : includedDays : \"Mon,Tue,Wed,Sat,Sun\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Schedule Repeat"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#repeat-schedule","text":"It schedule the chaos in the repeat mode. There are various ways we can set up this type of schedule by varying the the fields inside spec.repeat . Note - We have just one field i.e. minChaosInterval to be specified as mandatory one. All other fields are optional and totally dependent on the desired behaviour. View the schedule repeat schema Field .spec.schedule.repeat.timeRange.startTime Description Flag to specify start timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created before this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When startTime is specified against the policy repeat , ChaosEngine will not be formed before this time, no matter when it was created. Field .spec.schedule.repeat.timeRange.endTime Description Flag to specify end timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created after this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When endTime is specified against the policy repeat , ChaosEngine will not be formed after this time. Field .spec.schedule.repeat.properties.minChaosInterval.hour.everyNthHour Description Flag to specify the hours between each successive schedule Type Mandatory Range integer Default n/a Notes The minChaosInterval.hour.everyNthHour in the spec specifies the time interval in hours between each schedule Field .spec.schedule.repeat.properties.minChaosInterval.hour.minuteOfTheHour Description Flag to specify minute of hour for each successive schedule Type Mandatory Range integer Default 0 Notes The minChaosInterval.hour.minuteOfTheHour in the spec specifies the minute of the hour between each schedule Field .spec.schedule.repeat.properties.minChaosInterval.minute.everyNthMinute Description Flag to specify the minutes for each successive schedule Type Mandatory Range integer Default n/a Notes The minChaosInterval.hour.everyNthMinute in the spec specifies the time interval in minutes between each schedule Field .spec.schedule.repeat.workDays.includedDays Description Flag to specify the days at which chaos is allowed to take place Type Mandatory Range user-defined (type: string)(pattern: [{day_name},{day_name}...]). Default n/a Notes The includedDays in the spec specifies a (comma-separated) list of days of the week at which chaos is allowed to take place. {day_name} is to be specified with the first 3 letters of the name of day such as Mon , Tue etc. Field .spec.schedule.repeat.workHours.includedHours Description Flag to specify the hours at which chaos is allowed to take place Type Mandatory Range {hour_number} will range from 0 to 23 (type: string)(pattern: {hour_number}-{hour_number}). Default n/a Notes The includedHours in the spec specifies a range of hours of the day at which chaos is allowed to take place. 24 hour format is followed","title":"Repeat Schedule"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#basic-schema-to-execute-repeat-strategy","text":"This will keep executing the schedule and creating engines for an indefinite amount of time.","title":"Basic Schema to Execute Repeat Strategy"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#schedule-chaosengine-at-every-nth-minute","text":"apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos at every 5 minutes minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Schedule ChaosEngine at every nth minute"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#schedule-chaosengine-at-every-nth-hour","text":"apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos every hour at 0th minute hour : everyNthHour : 1 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Schedule ChaosEngine at every nth hour"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#schedule-chaosengine-at-nth-minute-of-every-nth-hour","text":"apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : # schedule the chaos every hour at 30th minute hour : everyNthHour : 1 minuteOfTheHour : 30 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Schedule ChaosEngine at nth minute of every nth hour"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#specifying-time-range-for-the-chaos-schedule","text":"This will manipulate the schedule to be started and ended according to our definition. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time startTime : \"2020-05-12T05:47:00Z\" endTime : \"2020-09-13T02:58:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Specifying Time Range for the Chaos Schedule"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#specifying-just-the-end-time","text":"Assumes the custom resource creation timestamp as the StartTime apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time endTime : \"2020-09-13T02:58:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Specifying Just the End Time"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#specifying-just-the-starttime","text":"Executes chaos indefinitely (until the ChaosSchedule CR is removed) starting from the specified timestamp apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : timeRange : #should be modified according to current UTC Time startTime : \"2020-05-12T05:47:00Z\" properties : minChaosInterval : minute : everyNthMinute : 5 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Specifying Just the StartTime"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#specifying-work-hours","text":"This ensures chaos execution within the specified hours of the day, everyday. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : minute : everyNthMinute : 5 workHours : # format should be <starting-hour-number>-<ending-hour-number>(inclusive) includedHours : 0-12 engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' # It can be true/false annotationCheck : 'true' #ex. values: ns1:name=percona,ns2:run=nginx auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa # It can be delete/retain jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Specifying Work Hours"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/schedule-repeat/#specifying-work-days","text":"This executes chaos on specified days of the week, with the specified minimum interval. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosSchedule metadata : name : schedule-nginx spec : schedule : repeat : properties : minChaosInterval : minute : everyNthMinute : 5 workDays : includedDays : \"Mon,Tue,Wed,Sat,Sun\" engineTemplateSpec : engineState : 'active' appinfo : appns : 'default' applabel : 'app=nginx' appkind : 'deployment' annotationCheck : 'true' auxiliaryAppInfo : '' chaosServiceAccount : pod-delete-sa jobCleanUpPolicy : 'delete' experiments : - name : pod-delete spec : components : env : # set chaos duration (in sec) as desired - name : TOTAL_CHAOS_DURATION value : '30' # set chaos interval (in sec) as desired - name : CHAOS_INTERVAL value : '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name : FORCE value : 'false'","title":"Specifying work days"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/state/","text":"Halt/Resume ChaosSchedule \u00b6 Chaos Schedules can be halted or resumed as per need. It can tuned via setting spec.scheduleState to halt and active respectively. View the state schema Field .spec.scheduleState Description Flag to control chaosshedule state Type Optional Range active , halt , complete Default active Notes The scheduleState is the current state of ChaosSchedule. If the schedule is running its state will be active , if the schedule is halted its state will be halt and if the schedule is completed it state will be complete . Halt The Schedule \u00b6 Follow the below steps to halt the active schedule: Edit the ChaosSchedule CR in your favourite editor kubectl edit chaosschedule schedule-nginx Change the spec.scheduleState to halt spec: scheduleState: halt Resume The Schedule \u00b6 Follow the below steps to resume the halted schedule: Edit the chaosschedule kubectl edit chaosschedule schedule-nginx Change the spec.scheduleState to active spec: scheduleState: active","title":"Schedule State"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/state/#haltresume-chaosschedule","text":"Chaos Schedules can be halted or resumed as per need. It can tuned via setting spec.scheduleState to halt and active respectively. View the state schema Field .spec.scheduleState Description Flag to control chaosshedule state Type Optional Range active , halt , complete Default active Notes The scheduleState is the current state of ChaosSchedule. If the schedule is running its state will be active , if the schedule is halted its state will be halt and if the schedule is completed it state will be complete .","title":"Halt/Resume ChaosSchedule"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/state/#halt-the-schedule","text":"Follow the below steps to halt the active schedule: Edit the ChaosSchedule CR in your favourite editor kubectl edit chaosschedule schedule-nginx Change the spec.scheduleState to halt spec: scheduleState: halt","title":"Halt The Schedule"},{"location":"experiments/concepts/chaos-resources/chaos-scheduler/state/#resume-the-schedule","text":"Follow the below steps to resume the halted schedule: Edit the chaosschedule kubectl edit chaosschedule schedule-nginx Change the spec.scheduleState to active spec: scheduleState: active","title":"Resume The Schedule"},{"location":"experiments/concepts/chaos-resources/probes/cmdProbe/","text":"The command probe allows developers to run shell commands and match the resulting output as part of the entry/exit criteria. The intent behind this probe was to allow users to implement a non-standard & imperative way of expressing their hypothesis. For example, the cmdProbe enables you to check for specific data within a database, parse the value out of a JSON blob being dumped into a certain path, or check for the existence of a particular string in the service logs. It can be executed by setting type as cmdProbe inside .spec.experiments[].spec.probe . View the command probe schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .cmdProbe/inputs.command Description Flag to hold the command for the cmdProbe Type Mandatory Range n/a {type: string} Notes The .cmdProbe/inputs.command contains the shell command, which should be run as part of cmdProbe Field .cmdProbe/inputs.source Description Flag to hold the source for the cmdProbe Type Mandatory Range inline , any source docker image Notes The .cmdProbe/inputs.source It supports inline value when command can be run from within the experiment image. Otherwise provide the source image which can be used to launch a external pod where the command execution is carried out. Field .cmdProbe/inputs.comparator.type Description Flag to hold type of the data used for comparision Type Mandatory Range string , int , float Notes The .cmdProbe/inputs.comparator.type contains type of data, which should be compare as part of comparision operation Field .cmdProbe/inputs.comparator.criteria Description Flag to hold criteria for the comparision Type Mandatory Range it supports {>=, <=, ==, >, <, !=, oneOf, between} for int & float type. And {equal, notEqual, contains, matches, notMatches, oneOf} for string type. Notes The .cmdProbe/inputs.comparator.criteria contains criteria of the comparision, which should be fulfill as part of comparision operation. Field .cmdProbe/inputs.comparator.value Description Flag to hold value for the comparision Type Mandatory Range n/a {type: string} Notes The .cmdProbe/inputs.comparator.value contains value of the comparision, which should follow the given criteria as part of comparision operation. Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Inline Mode \u00b6 In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5 Source Mode \u00b6 In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Command Probe"},{"location":"experiments/concepts/chaos-resources/probes/cmdProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/concepts/chaos-resources/probes/cmdProbe/#inline-mode","text":"In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Inline Mode"},{"location":"experiments/concepts/chaos-resources/probes/cmdProbe/#source-mode","text":"In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Source Mode"},{"location":"experiments/concepts/chaos-resources/probes/contents/","text":"Probes Specifications \u00b6 Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe Probe Chaining It chain the litmus probes Probe Chaining","title":"Contents"},{"location":"experiments/concepts/chaos-resources/probes/contents/#probes-specifications","text":"Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe Probe Chaining It chain the litmus probes Probe Chaining","title":"Probes Specifications"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/","text":"The http probe allows developers to specify a URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. The received status code is mapped against an expected status. It supports http Get and Post methods. It can be executed by setting type as httpProbe inside .spec.experiments[].spec.probe . View the http probe schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .httpProbe/inputs.url Description Flag to hold the URL for the httpProbe Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.url contains the URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. Field .httpProbe/inputs.insecureSkipVerify Description Flag to hold the flag to skip certificate checks for the httpProbe Type Optional Range true , false Notes The .httpProbe/inputs.insecureSkipVerify contains flag to skip certificate checks. Field .httpProbe/inputs.responseTimeout Description Flag to hold the flag to response timeout for the httpProbe Type Optional Range n/a {type: integer} Notes The .httpProbe/inputs.responseTimeout contains flag to provide the response timeout for the http Get/Post request. Field .httpProbe/inputs.method.get.criteria Description Flag to hold the criteria for the http get request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.get.criteria contains criteria to match the http get request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.get.responseCode Description Flag to hold the expected response code for the get request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.get.responseCode contains the expected response code for the http get request as part of httpProbe run Field .httpProbe/inputs.method.post.contentType Description Flag to hold the content type of the post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.contentType contains the content type of the http body data, which need to be passed for the http post request Field .httpProbe/inputs.method.post.body Description Flag to hold the body of the http post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.body contains the http body, which is required for the http post request. It is used for the simple http body. If the http body is complex then use .httpProbe/inputs.method.post.bodyPath field. Field .httpProbe/inputs.method.post.bodyPath Description Flag to hold the path of the http body, required for the http post request Type Optional Range n/a {type: string} Notes The .httpProbe/inputs.method.post.bodyPath This field is used in case of complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. Field .httpProbe/inputs.method.post.criteria Description Flag to hold the criteria for the http post request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.post.criteria contains criteria to match the http post request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.post.responseCode Description Flag to hold the expected response code for the post request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.post.responseCode contains the expected response code for the http post request as part of httpProbe run Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. HTTP Get Request \u00b6 In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a simple) \u00b6 It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a complex) \u00b6 In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Response Timout \u00b6 It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Skip Certification Check \u00b6 It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Probe"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#http-get-request","text":"In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Get Request"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-simple","text":"It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a simple)"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-complex","text":"In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a complex)"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#response-timout","text":"It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Response Timout"},{"location":"experiments/concepts/chaos-resources/probes/httpProbe/#skip-certification-check","text":"It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Skip Certification Check"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/","text":"With the proliferation of custom resources & operators, especially in the case of stateful applications, the steady-state is manifested as status parameters/flags within Kubernetes resources. k8sProbe addresses verification of the desired resource state by allowing users to define the Kubernetes GVR (group-version-resource) with appropriate filters (field selectors/label selectors). The experiment makes use of the Kubernetes Dynamic Client to achieve this. It supports CRUD operations which can be defined at probe.k8sProbe/inputs.operation . It can be executed by setting type as k8sProbe inside .spec.experiments[].spec.probe . View the k8s probe schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .k8sProbe/inputs.group Description Flag to hold the group of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.group contains group of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.version Description Flag to hold the apiVersion of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.version contains apiVersion of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.resource Description Flag to hold the kubernetes resource name for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.resource contains the kubernetes resource name on which k8sProbe performs the specified operation Field .k8sProbe/inputs.namespace Description Flag to hold the namespace of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.namespace contains namespace of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.fieldSelector Description Flag to hold the fieldSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.fieldSelector contains fieldSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.labelSelector Description Flag to hold the labelSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.labelSelector contains labelSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.operation Description Flag to hold the operation type for the k8sProbe Type Mandatory Range create , delete , present , absent Notes The .k8sProbe/inputs.operation contains operation which should be applied on the kubernetes resource as part of k8sProbe. It supports four type of operation. It can be one of create , delete , present , absent . Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Create Operation \u00b6 It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi Delete Operation \u00b6 It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Present Operation \u00b6 It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Absent Operation \u00b6 It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"K8S Probe"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/#create-operation","text":"It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi","title":"Create Operation"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/#delete-operation","text":"It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Delete Operation"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/#present-operation","text":"It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Present Operation"},{"location":"experiments/concepts/chaos-resources/probes/k8sProbe/#absent-operation","text":"It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Absent Operation"},{"location":"experiments/concepts/chaos-resources/probes/litmus-probes/","text":"Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). It can be provided at .spec.experiments[].spec.probe inside chaosengine. It supports four types: cmdProbe , k8sProbe , httpProbe , and promProbe . Probe Modes \u00b6 The probes can be set up to run in five different modes. Which can be tuned via mode ENV. SOT : Executed at the Start of the Test as a pre-chaos check EOT : Executed at the End of the Test as a post-chaos check Edge : Executed both, before and after the chaos Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Run Properties \u00b6 All probes share some common attributes. Which can be tuned via runProperties ENV. probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. interval : The period between subsequent retries probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2 Initial Delay Seconds \u00b6 It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds Stop/Continue Experiment On Probe Failure \u00b6 It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true","title":"Introduction"},{"location":"experiments/concepts/chaos-resources/probes/litmus-probes/#probe-modes","text":"The probes can be set up to run in five different modes. Which can be tuned via mode ENV. SOT : Executed at the Start of the Test as a pre-chaos check EOT : Executed at the End of the Test as a post-chaos check Edge : Executed both, before and after the chaos Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Probe Modes"},{"location":"experiments/concepts/chaos-resources/probes/litmus-probes/#run-properties","text":"All probes share some common attributes. Which can be tuned via runProperties ENV. probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. interval : The period between subsequent retries probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2","title":"Run Properties"},{"location":"experiments/concepts/chaos-resources/probes/litmus-probes/#initial-delay-seconds","text":"It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds","title":"Initial Delay Seconds"},{"location":"experiments/concepts/chaos-resources/probes/litmus-probes/#stopcontinue-experiment-on-probe-failure","text":"It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true","title":"Stop/Continue Experiment On Probe Failure"},{"location":"experiments/concepts/chaos-resources/probes/probe-chaining/","text":"Probe chaining enables reuse of probe a result (represented by the template function {{ .<probeName>.probeArtifact.Register}}) in subsequent \"downstream\" probes defined in the ChaosEngine. Note : The order of execution of probes in the experiment depends purely on the order in which they are defined in the ChaosEngine. Use the following example to tune this: # chaining enables reuse of probe's result (represented by the template function {{ <probeName>.probeArtifact.Register}}) #-- in subsequent \"downstream\" probes defined in the ChaosEngine. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"probe1\" type : \"cmdProbe\" cmdProbe/inputs : command : \"<command>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1 - name : \"probe2\" type : \"cmdProbe\" cmdProbe/inputs : ## probe1's result being used as one of the args in probe2 command : \"<commmand> {{ .probe1.ProbeArtifacts.Register }} <arg2>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Probe Chaining"},{"location":"experiments/concepts/chaos-resources/probes/promProbe/","text":"The prometheus probe allows users to run Prometheus queries and match the resulting output against specific conditions. The intent behind this probe is to allow users to define metrics-based SLOs in a declarative way and determine the experiment verdict based on its success. The probe runs the query on a Prometheus server defined by the endpoint, and checks whether the output satisfies the specified criteria. It can be executed by setting type as promProbe inside .spec.experiments[].spec.probe . View the prometheus probe schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .promProbe/inputs.endpoint Description Flag to hold the prometheus endpoints for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.endpoint contains the prometheus endpoints Field .promProbe/inputs.query Description Flag to hold the promql query for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.query contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint Field .promProbe/inputs.queryPath Description Flag to hold the path of the promql query for the promProbe Type Optional Range n/a {type: string} Notes The .promProbe/inputs.queryPath This field is used in case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. Field .promProbe/inputs.comparator.criteria Description Flag to hold criteria for the comparision Type Mandatory Range it supports {>=, <=, ==, >, <, !=, oneOf, between} criteria Notes The .promProbe/inputs.comparator.criteria contains criteria of the comparision, which should be fulfill as part of comparision operation. Field .promProbe/inputs.comparator.value Description Flag to hold value for the comparision Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.comparator.value contains value of the comparision, which should follow the given criteria as part of comparision operation. Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Prometheus Query(query is a simple) \u00b6 It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 Prometheus Query(query is a complex \u00b6 In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Probe"},{"location":"experiments/concepts/chaos-resources/probes/promProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/concepts/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-simple","text":"It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a simple)"},{"location":"experiments/concepts/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-complex","text":"In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a complex"},{"location":"experiments/concepts/security/kyverno-policies/","text":"Kyverno policies blocks configurations that don't match a policy (enforce mode) or can generate policy violations (audit mode). It scans existing configurations and reports violations in the cluster. Litmus recommends using the provided policy configuration to enable the execution of all supported (out-of-the-box) experiments listed in the chaoshub. Having said that, this is recommendatory in nature and left to user discretion/choice depending upon experiments desired. The details listed here are expected to aid users of Kyverno. If you are using alternate means to enforce runtime security, such as native Kubernetes PSPs (pod security policies), refer this section: <> Policies in Litmus \u00b6 Litmus recommends using the following policies: Add Capabilities : It restricts add capabilities except the NET_ADMIN and SYS_ADMIN for the pods that use runtime API Host Namespaces : It validates following host namespaces for the pods that use runtime API. HostPID: It allows hostPID. It should be set to true . HostIPC: It restricts the host IPC. It should be set to false . HostNetwork: It restricts the hostNetwork. It should be set to false . Host Paths : It restricts hostPath except the socket-path & container-path host paths for the pods that uses runtime API. It allows hostPaths for service-kill experiments. Privilege Escalation : It restricts privilege escalation except for the pods that use runtime API Privilege Container : It restricts privileged containers except for the pods that use runtime API User Groups : It allows users groups for all the experiment pods Install Policies \u00b6 These Kyverno policies are based on the Kubernetes Pod Security Standards definitons. To apply all pod security policies (recommended) install Kyverno and kustomize , then run: kustomize build https://github.com/litmuschaos/chaos-charts/kyverno-policies/security/kyverno | kubectl apply -f - Pod Security Policies in restricted setup \u00b6 If setup contains restricted policies which don't allow execution of litmus experiments by default. For Example deny-privilege-escalation policy doesn't allow privileged escalation. It deny all the pods to use privileged escalation. To allow litmus pods to use the privileged escalation. Add the litmus serviceAcccount or ClusterRole/Role inside the exclude block as : apiVersion : kyverno.io/v1 kind : ClusterPolicy metadata : name : deny-privilege-escalation annotations : policies.kyverno.io/category : Pod Security Standards (Restricted) policies.kyverno.io/severity : medium policies.kyverno.io/subject : Pod policies.kyverno.io/description : >- Privilege escalation, such as via set-user-ID or set-group-ID file mode, should not be allowed. This policy ensures the `allowPrivilegeEscalation` fields are either undefined or set to `false`. spec : background : true validationFailureAction : enforce rules : - name : deny-privilege-escalation match : resources : kinds : - Pod exclude : clusterRoles : # add litmus cluster roles here - litmus-admin roles : # add litmus roles here - litmus-roles subjects : # add serviceAccount name here - kind : ServiceAccount name : pod-network-loss-sa validate : message : >- Privilege escalation is disallowed. The fields spec.containers[*].securityContext.allowPrivilegeEscalation, and spec.initContainers[*].securityContext.allowPrivilegeEscalation must be undefined or set to `false`. pattern : spec : =(initContainers) : - =(securityContext) : =(allowPrivilegeEscalation) : \"false\" containers : - =(securityContext) : =(allowPrivilegeEscalation) : \"false\"","title":"Kyverno Policies"},{"location":"experiments/concepts/security/kyverno-policies/#policies-in-litmus","text":"Litmus recommends using the following policies: Add Capabilities : It restricts add capabilities except the NET_ADMIN and SYS_ADMIN for the pods that use runtime API Host Namespaces : It validates following host namespaces for the pods that use runtime API. HostPID: It allows hostPID. It should be set to true . HostIPC: It restricts the host IPC. It should be set to false . HostNetwork: It restricts the hostNetwork. It should be set to false . Host Paths : It restricts hostPath except the socket-path & container-path host paths for the pods that uses runtime API. It allows hostPaths for service-kill experiments. Privilege Escalation : It restricts privilege escalation except for the pods that use runtime API Privilege Container : It restricts privileged containers except for the pods that use runtime API User Groups : It allows users groups for all the experiment pods","title":"Policies in Litmus"},{"location":"experiments/concepts/security/kyverno-policies/#install-policies","text":"These Kyverno policies are based on the Kubernetes Pod Security Standards definitons. To apply all pod security policies (recommended) install Kyverno and kustomize , then run: kustomize build https://github.com/litmuschaos/chaos-charts/kyverno-policies/security/kyverno | kubectl apply -f -","title":"Install Policies"},{"location":"experiments/concepts/security/kyverno-policies/#pod-security-policies-in-restricted-setup","text":"If setup contains restricted policies which don't allow execution of litmus experiments by default. For Example deny-privilege-escalation policy doesn't allow privileged escalation. It deny all the pods to use privileged escalation. To allow litmus pods to use the privileged escalation. Add the litmus serviceAcccount or ClusterRole/Role inside the exclude block as : apiVersion : kyverno.io/v1 kind : ClusterPolicy metadata : name : deny-privilege-escalation annotations : policies.kyverno.io/category : Pod Security Standards (Restricted) policies.kyverno.io/severity : medium policies.kyverno.io/subject : Pod policies.kyverno.io/description : >- Privilege escalation, such as via set-user-ID or set-group-ID file mode, should not be allowed. This policy ensures the `allowPrivilegeEscalation` fields are either undefined or set to `false`. spec : background : true validationFailureAction : enforce rules : - name : deny-privilege-escalation match : resources : kinds : - Pod exclude : clusterRoles : # add litmus cluster roles here - litmus-admin roles : # add litmus roles here - litmus-roles subjects : # add serviceAccount name here - kind : ServiceAccount name : pod-network-loss-sa validate : message : >- Privilege escalation is disallowed. The fields spec.containers[*].securityContext.allowPrivilegeEscalation, and spec.initContainers[*].securityContext.allowPrivilegeEscalation must be undefined or set to `false`. pattern : spec : =(initContainers) : - =(securityContext) : =(allowPrivilegeEscalation) : \"false\" containers : - =(securityContext) : =(allowPrivilegeEscalation) : \"false\"","title":"Pod Security Policies in restricted setup"},{"location":"experiments/concepts/security/psp/","text":"Using Pod Security Policies with Litmus \u00b6 While working in environments (clusters) that have restrictive security policies, the default litmuschaos experiment execution procedure may be inhibited. This is mainly due to the fact that the experiment pods running the chaos injection tasks run with a root user. This, in turn, is necessitated due to the mounting of container runtime-specific socket files from the Kubernetes nodes in order to invoke runtime APIs. While this is not needed for all experiments (a considerable number of them use purely the K8s API), those involving injection of chaos processes into the network/process namespaces of other containers have this requirement (ex: netem, stress). The restrictive policies are often enforced via pod security policies (PSP) today, with organizations opting for the default \"restricted\" policy. Applying Pod Security Policies to Litmus Chaos Pods \u00b6 To run the litmus pods with operating characteristics described above, first create a custom PodSecurityPolicy that allows the same: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : litmus annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true # Required to prevent escalations to root. allowPrivilegeEscalation : true # Allow core volume types. volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' allowedHostPaths : # substitutes this path with an appropriate socket path # ex: '/var/run/docker.sock', '/run/containerd/containerd.sock', '/run/crio/crio.sock' - pathPrefix : \"/var/run/docker.sock\" # substitutes this path with an appropriate container path # ex: '/var/lib/docker/containers', '/var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io', '/var/lib/containers/storage/overlay/' - pathPrefix : \"/var/lib/docker/containers\" allowedCapabilities : - \"NET_ADMIN\" - \"SYS_ADMIN\" hostNetwork : false hostIPC : false hostPID : true seLinux : # This policy assumes the nodes are using AppArmor rather than SELinux. rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 readOnlyRootFilesystem : false Note : This PodSecurityPolicy is a sample configuration which works for a majority of the usecases. It is left to the user's discretion to modify it based on the environment. For example, if the experiment doesn't need the socket file to be mounted, allowedHostPaths can be excluded from the psp spec. On the other hand, in case of CRI-O runtime, network-chaos tests need the chaos pods executed in privileged mode. It is also possible that different PSP configs are used in different namespaces based on ChaosExperiments installed/executed in them. Subscribe to the created PSP in the experiment RBAC (or in the admin-mode rbac, as applicable). For example, the pod-delete experiment rbac instrumented with the PSP is shown below: --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"policy\" ] resources : [ \"podsecuritypolicies\" ] verbs : [ \"use\" ] resourceNames : [ \"litmus\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Execute the ChaosEngine and verify that the litmus experiment pods are created successfully.","title":"Pod Security Policies"},{"location":"experiments/concepts/security/psp/#using-pod-security-policies-with-litmus","text":"While working in environments (clusters) that have restrictive security policies, the default litmuschaos experiment execution procedure may be inhibited. This is mainly due to the fact that the experiment pods running the chaos injection tasks run with a root user. This, in turn, is necessitated due to the mounting of container runtime-specific socket files from the Kubernetes nodes in order to invoke runtime APIs. While this is not needed for all experiments (a considerable number of them use purely the K8s API), those involving injection of chaos processes into the network/process namespaces of other containers have this requirement (ex: netem, stress). The restrictive policies are often enforced via pod security policies (PSP) today, with organizations opting for the default \"restricted\" policy.","title":"Using Pod Security Policies with Litmus"},{"location":"experiments/concepts/security/psp/#applying-pod-security-policies-to-litmus-chaos-pods","text":"To run the litmus pods with operating characteristics described above, first create a custom PodSecurityPolicy that allows the same: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : litmus annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true # Required to prevent escalations to root. allowPrivilegeEscalation : true # Allow core volume types. volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' allowedHostPaths : # substitutes this path with an appropriate socket path # ex: '/var/run/docker.sock', '/run/containerd/containerd.sock', '/run/crio/crio.sock' - pathPrefix : \"/var/run/docker.sock\" # substitutes this path with an appropriate container path # ex: '/var/lib/docker/containers', '/var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io', '/var/lib/containers/storage/overlay/' - pathPrefix : \"/var/lib/docker/containers\" allowedCapabilities : - \"NET_ADMIN\" - \"SYS_ADMIN\" hostNetwork : false hostIPC : false hostPID : true seLinux : # This policy assumes the nodes are using AppArmor rather than SELinux. rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 readOnlyRootFilesystem : false Note : This PodSecurityPolicy is a sample configuration which works for a majority of the usecases. It is left to the user's discretion to modify it based on the environment. For example, if the experiment doesn't need the socket file to be mounted, allowedHostPaths can be excluded from the psp spec. On the other hand, in case of CRI-O runtime, network-chaos tests need the chaos pods executed in privileged mode. It is also possible that different PSP configs are used in different namespaces based on ChaosExperiments installed/executed in them. Subscribe to the created PSP in the experiment RBAC (or in the admin-mode rbac, as applicable). For example, the pod-delete experiment rbac instrumented with the PSP is shown below: --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"policy\" ] resources : [ \"podsecuritypolicies\" ] verbs : [ \"use\" ] resourceNames : [ \"litmus\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Execute the ChaosEngine and verify that the litmus experiment pods are created successfully.","title":"Applying Pod Security Policies to Litmus Chaos Pods"},{"location":"experiments/faq/ci-cd/","text":"CI/CD \u00b6 Table of Contents \u00b6 Is there any use case to integrate litmus into CI? Which experiment have you integrated as part of the CI? And what would you do if a microservice fails an experiment in the CI? Is there any way to use Litmus within GitHub? When someone submits a k8s deployment for a PR , We want to run a chaos Experiment on that to see whether it passes or not How can users integrate Litmuschaos in their environment with Gitops? How can we use litmus in our DevOps pipeline/cycle? Is there any use case to integrate litmus into CI? Which experiment have you integrated as part of the CI? And what would you do if a microservice fails an experiment in the CI? \u00b6 We have integrated litmus with a couple of CI tools, the major ones are : GitHub Actions using litmuschaos actions GitLab using remote templates. Keptn Spinnaker templates By this, we induce chaos as part of the CI stage as Continuous Chaos allows us to automatically identify application failures over the development phase. Failure of an exp in ci should invariably fail the pipeline. The pass would be more subjective. Depends on what is the nature of the ci pipeline - what it is the tests being carried is like etc., If you are doing a simple pod-delete or cpu-hog on a microservice pod w/o traffic OR w/o running it in an env that doesn't need it to interact w/ other services then the insights are limited. Is there any way to use Litmus within GitHub? When someone submits a k8s deployment for a PR , We want to run a chaos Experiment on that to see whether it passes or not. \u00b6 Yes, with the help of GitHub-chaos-action we can automate the chaos execution on an application in the same place where the code is stored. We can write individual tasks along with chaos actions and combine them to create a custom GitHub workflow. GitHub Workflows are custom automated processes that we can set up in our repository to build, test, package, or deploy any code project on GitHub. Including the GitHub chaos actions in our workflow YAML, We can test the performance/resiliency of our application in a much simpler and better way. To know more visit our Github chaos action repository . How can users integrate Litmuschaos in their environment with Gitops? \u00b6 Gitops feature in Litmus enables users to sync workflows from a configured git repo, any workflow inserts/updates made to the repo will be monitored and picked up by the litmus portal and will be executed on the target cluster. Litmus portal gitops also includes an event-driven chaos injection feature where users can annotate an application to be watched for changes and if and when the change happens chaos workflows can be triggered automatically. This integrates with other gitops tools like flux/argo cd and enables users to automatically run chaos workflows whenever a new release happens or a particular change occurs in the application. To configure a git repo the user must provide the Git URL of the repository and the branch name and the authentication credentials which are of two types: Access Token SSH Key Once GitOps is enabled, any new workflows created will be stored in the configured repo in the path litmus/<project-id>/<workflow-name>.yaml How can we use litmus in our DevOps pipeline/cycle? \u00b6 You can add litmus to the CI/CD pipelines as part of an end-to-end testing approach due to its minimal pre-requisites and simple result mechanisms. It also provides utilities for quick setup of Kubernetes clusters on different platforms as well as installation of storage provider control plane components (operators). Openebs.ci is a reference implementation of how litmus can be used in the DevOps pipeline.","title":"CI/CD"},{"location":"experiments/faq/ci-cd/#cicd","text":"","title":"CI/CD"},{"location":"experiments/faq/ci-cd/#table-of-contents","text":"Is there any use case to integrate litmus into CI? Which experiment have you integrated as part of the CI? And what would you do if a microservice fails an experiment in the CI? Is there any way to use Litmus within GitHub? When someone submits a k8s deployment for a PR , We want to run a chaos Experiment on that to see whether it passes or not How can users integrate Litmuschaos in their environment with Gitops? How can we use litmus in our DevOps pipeline/cycle?","title":"Table of Contents"},{"location":"experiments/faq/ci-cd/#is-there-any-use-case-to-integrate-litmus-into-ci-which-experiment-have-you-integrated-as-part-of-the-ci-and-what-would-you-do-if-a-microservice-fails-an-experiment-in-the-ci","text":"We have integrated litmus with a couple of CI tools, the major ones are : GitHub Actions using litmuschaos actions GitLab using remote templates. Keptn Spinnaker templates By this, we induce chaos as part of the CI stage as Continuous Chaos allows us to automatically identify application failures over the development phase. Failure of an exp in ci should invariably fail the pipeline. The pass would be more subjective. Depends on what is the nature of the ci pipeline - what it is the tests being carried is like etc., If you are doing a simple pod-delete or cpu-hog on a microservice pod w/o traffic OR w/o running it in an env that doesn't need it to interact w/ other services then the insights are limited.","title":"Is there any use case to integrate litmus into CI? Which experiment have you integrated as part of the CI? And what would you do if a microservice fails an experiment in the CI?"},{"location":"experiments/faq/ci-cd/#is-there-any-way-to-use-litmus-within-github-when-someone-submits-a-k8s-deployment-for-a-pr-we-want-to-run-a-chaos-experiment-on-that-to-see-whether-it-passes-or-not","text":"Yes, with the help of GitHub-chaos-action we can automate the chaos execution on an application in the same place where the code is stored. We can write individual tasks along with chaos actions and combine them to create a custom GitHub workflow. GitHub Workflows are custom automated processes that we can set up in our repository to build, test, package, or deploy any code project on GitHub. Including the GitHub chaos actions in our workflow YAML, We can test the performance/resiliency of our application in a much simpler and better way. To know more visit our Github chaos action repository .","title":"Is there any way to use Litmus within GitHub? When someone submits a k8s deployment for a PR , We want to run a chaos Experiment on that to see whether it passes or not."},{"location":"experiments/faq/ci-cd/#how-can-users-integrate-litmuschaos-in-their-environment-with-gitops","text":"Gitops feature in Litmus enables users to sync workflows from a configured git repo, any workflow inserts/updates made to the repo will be monitored and picked up by the litmus portal and will be executed on the target cluster. Litmus portal gitops also includes an event-driven chaos injection feature where users can annotate an application to be watched for changes and if and when the change happens chaos workflows can be triggered automatically. This integrates with other gitops tools like flux/argo cd and enables users to automatically run chaos workflows whenever a new release happens or a particular change occurs in the application. To configure a git repo the user must provide the Git URL of the repository and the branch name and the authentication credentials which are of two types: Access Token SSH Key Once GitOps is enabled, any new workflows created will be stored in the configured repo in the path litmus/<project-id>/<workflow-name>.yaml","title":"How can users integrate Litmuschaos in their environment with Gitops?"},{"location":"experiments/faq/ci-cd/#how-can-we-use-litmus-in-our-devops-pipelinecycle","text":"You can add litmus to the CI/CD pipelines as part of an end-to-end testing approach due to its minimal pre-requisites and simple result mechanisms. It also provides utilities for quick setup of Kubernetes clusters on different platforms as well as installation of storage provider control plane components (operators). Openebs.ci is a reference implementation of how litmus can be used in the DevOps pipeline.","title":"How can we use litmus in our DevOps pipeline/cycle?"},{"location":"experiments/faq/content/","text":"Litmus FAQ \u00b6 FAQ \u00b6 Category Description References Install Questions related to litmus installation Install Experiments Questions related to litmus experiments Experiments Portal Questions related to litmus portal Portal Scheduler Questions related to litmus scheduler Scheduler Security Questions related to litmus security Security CI/CD Questions related to litmus CI/CD integration CI/CD Troubleshooting \u00b6 Category Description References Install Troubleshooting related to litmus installation Install Experiments Troubleshooting related to litmus experiments Experiments Portal Troubleshooting related to litmus portal Portal Scheduler Troubleshooting related to litmus scheduler Scheduler","title":"Contents"},{"location":"experiments/faq/content/#litmus-faq","text":"","title":"Litmus FAQ"},{"location":"experiments/faq/content/#faq","text":"Category Description References Install Questions related to litmus installation Install Experiments Questions related to litmus experiments Experiments Portal Questions related to litmus portal Portal Scheduler Questions related to litmus scheduler Scheduler Security Questions related to litmus security Security CI/CD Questions related to litmus CI/CD integration CI/CD","title":"FAQ"},{"location":"experiments/faq/content/#troubleshooting","text":"Category Description References Install Troubleshooting related to litmus installation Install Experiments Troubleshooting related to litmus experiments Experiments Portal Troubleshooting related to litmus portal Portal Scheduler Troubleshooting related to litmus scheduler Scheduler","title":"Troubleshooting"},{"location":"experiments/faq/experiments/","text":"Litmus Experiments \u00b6 Table of Contents \u00b6 Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? What are the typical probes used for pod-network related experiments? Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? How to run chaos experiment programatically using apis? Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? The experiment is not targeting all pods with the given label, it just selects only one pod by default Do we have a way to see what pods are targeted when users use percentages? What is the function of spec.definition.scope of a ChaosExperiment CR? Pod network latency -- I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? How to check the NETWORK_INTERFACE and SOCKET_PATH variable? What are the different ways to target the pods and nodes for chaos? Does the pod affected perc select the random set of pods from the total pods under chaos? How to extract the chaos start time and end time? How do we check the MTTR (Mean time to recovery) for an application post chaos? What is the difference between Ramp Time and Chaos Interval? Can the appkind be a pod? What type of chaos experiments are supported by Litmus? What are the permissions required to run Litmus Chaos Experiments? What is the scope of a Litmus Chaos Experiment? To get started with running chaos experiments using Litmus? How to view and interpret the results of a chaos experiment? Do chaos experiments run as a standard set of pods? Is it mandatory to annotate application deployments for chaos? How to add Custom Annotations as chaos filters? Is it mandatory for the chaosengine and chaos experiment resources to exist in the same namespace? How to get the chaos logs in Litmus? Does Litmus support generation of events during chaos? How to stop or abort a chaos experiment? Can a chaos experiment be resumed once stopped or aborted? How to restart chaosengine after graceful completion? Does Litmus support any chaos metrics for experiments? Does Litmus track any usage metrics on the test clusters? What to choose between minChaosInterval and instanceCount? Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? \u00b6 The experiment takes a percentage of the total memory capacity of the Node. The helper pod runs on the target node to stress the resources of that node. So The experiment will not consume/hog the memory resources greater than the total memory available on Node. In other words there will always be an upper limit for the amount of memory to be consumed, which equal to the total available memory. Please refer to this blog for more details. Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? \u00b6 Yes, ultimately these are different ways to simulate a degraded network. Both cases are expected to typically cause retransmissions (for tcp). The extent of degradation depends on the percentage of loss/corruption How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? \u00b6 The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in different availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The args passed to the tc netem command run against the target container changes depending on the type of n/w fault What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? \u00b6 The pod cpu and memory chaos experiment till now (version 1.13.7) was using an exec mode of execution which means - we were execing inside the specified target container and launching process like md5sum and dd to consume the cpu and memory respectively. This is done by providing CHAOS_INJECT_COMMAND and CHAOS-KILL-COMMAND in chaosengine CR. But we have some limitations of using this method. Those were: The chaos inject and kill command are highly dependent on the base image of the target container and may work for some and for others you may have to derive it manually and use it. For scratch images that don't expose shells we couldn't execute the chaos. To overcome this - The stress-chaos experiments (cpu, memory and io) are enhanced to use a non exec mode of chaos execution. It makes use of target container cgroup for the resource allocation and container pid namespace for showing the stress-ng process in target container. This stress-ng process will consume the resources on the target container without doing an exec. The new enhanced experiments are available from litmus 1.13.8 version. What are the typical probes used for pod-network related experiments? \u00b6 Precisely the role of the experiment. Cause n/w degradation w/o the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in diff availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? \u00b6 The optional libs (like Pumba) is more of an illustration of how you can use 3 rd party tools with litmus. Called the BYOC (Bring Your Own Chaos). The preferred LIB is litmus . How to run chaos experiment programatically using apis? \u00b6 To directly consume/manipulate the chaos resources (i.e., chaosexperiment, chaosengine or chaosresults) via API - you can directly use the kube API. The CRDs by default provide us with an API endpoint. You can use any generic client implementation (go/python are most used ones) to access them. In case you use go, there is a clientset available as well: go-client Here are some simple CRUD ops against chaosresources you could construct with curl (I have used kubectl proxy, one could use an auth token instead)- just for illustration purposes. Create ChaosEngine: \u00b6 For example, assume this is the engine spec curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines -XPOST -H 'Content-Type: application/json' -d@pod-delete-chaosengine-trigger.json Read ChaosEngine status: \u00b6 curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos | jq '.status.engineStatus, .status.experiments[].verdict' Update ChaosEngine Spec: \u00b6 (say, this is the patch: https://gist.github.com/ksatchit/be54955a1f4231314797f25361ac488d ) curl --header \"Content-Type: application/json-patch+json\" --request PATCH --data '[{\"op\": \"replace\", \"path\": \"/spec/engineState\", \"value\": \"stop\"}]' http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos Delete the ChaosEngine resource: \u00b6 curl -X DELETE localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\" Similarly, to check the results/verdict of the experiment from ChaosResult, you could use: \u00b6 curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/nginx-chaos-pod-delete | jq '.status.experimentStatus.verdict, .status.experimentStatus.probeSuccessPercentage' Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? \u00b6 There are some scenarios that can still occur despite whatever availability aids K8s provides. For example, take disk usage or CPU hogs -- problems you would generally refer to as \"Noisy Neighbour\" problems. Stressing the disk w/ continuous and heavy I/O for example can cause degradation in reads and writes performed by other microservices that use this shared disk - for example. (modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out). Another issue is the amount of scratch space eaten up on a node - leading to lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes. Similarly w/ CPU chaos -- by injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in app traffic OR in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods. The experiment is not targeting all pods with the given label, it just selects only one pod by default. \u00b6 Yes. You can use either the PODS_AFFECTED_PERCENTAGE or TARGET_PODS env to select multiple pods. Refer: experiment tunable envs . Do we have a way to see what pods are targeted when users use percentages? \u00b6 We can view the target pods from the experiment logs or inside chaos results. What is the function of spec.definition.scope of a ChaosExperiment CR? \u00b6 The spec.definition.scope & .spec.definition.permissions is mostly for indicative/illustration purposes (for external tools to identify and validate what are the permissions associated to run the exp). By itself, it doesn't influence how and where an exp can be used.One could remove these fields if needed (of course along w/ the crd validation) and store these manifests if desired. In Pod network latency - I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? \u00b6 It will target the [1:total_replicas] (based on PODS_AFFECTED_PERC) numbers of random pods with matching labels(appinfo.applabel) and namespace(appinfo.appns). But if you want to target a specific pod then you can provide their names as a comma separated list inside TARGET_PODS . Yes, you can provide service B details inside DESTINATION_IPS or DESTINATION_HOSTS . The NETWORK_INTERFACE should be eth0 . How to check the NETWORK_INTERFACE and SOCKET_PATH variable? \u00b6 The NETWORK_INTERFACE is the interface name inside the pod/container that needs to be targeted. You can find it by execing into the target pod and checking the available interfaces. You can try ip link , iwconfig , ifconfig depending on the tools installed in the pod either of those could work. The SOCKET_PATH by default takes the docker socket path. If you are using something else like containerd, crio or have a different socket path by any chance you can specify it. This is required to communicate with the container runtime of your cluster. In addition to this if container-runtime is different then provide the name of container runtime inside CONTAINER_RUNTIME ENV. It supports docker , containerd , and crio runtimes. What are the different ways to target the pods and nodes for chaos? \u00b6 The different ways are: Pod Chaos: Appinfo : Provide the target pod labels in the chaos engine appinfo section. TARGET_PODS : You can provide the target pod names as a Comma Separated Variable. Like pod1,pod2. Node Chaos: TARGET_NODE or TARGET_NODES : Provide the target node or nodes in these envs. NODE_LABEL : Provide the label of the target nodes. Does the pod affected percentage select the random set of pods from the total pods under chaos? \u00b6 Yes, it selects the random pods based on the POD_AFFACTED_PERC ENV. In pod-delete experiment it selects random pods for each iterations of chaos. But for rest of the experiments(if it supports iterations) then it will select random pods once and use the same set of pods for remaining iterations. How to extract the chaos start time and end time? \u00b6 We can use the Chaos exporter metrics for the same. One can also visualise these events along with time in chaos engine events. How do we check the MTTR (Mean time to recovery) for an application post chaos? \u00b6 The MTTR can be validated by using statusCheck Timeout in the chaos engine. By default its value will be 180 seconds. We can also overwrite this using ChaosEngine. For more details refer this What is the difference between Ramp Time and Chaos Interval? \u00b6 The ramp time is the time duration to wait before and after injection of chaos in seconds. While the chaos interval is the time interval (in second) between successive chaos iterations. Can the appkind be a pod? \u00b6 The appkind as pod is not supported explicitly. The supported appkind are deployment , statefulset , replicaset , daemonset , rollout , and deploymentconfig . But we can target the pods by following ways: provide labels and namespace at spec.appinfo.applabel and spec.appinfo.appns respectively and provide spec.appinfo.appkind as empty. provide pod names at TARGET_PODS ENV and provide spec.appinfo as nil NOTE : The annotationCheck should be provided as false What type of chaos experiments are supported by Litmus? \u00b6 Litmus broadly defines Kubernetes chaos experiments into two categories: application or pod-level chaos experiments and platform or infra-level chaos experiments. The former includes pod-delete, container-kill, pod-cpu-hog, pod-network-loss etc., while the latter includes node-drain, disk-loss, node-cpu-hog etc., The infra chaos experiments typically have a higher blast radius and impacts more than one application deployed on the Kubernetes cluster. Litmus also categorizes experiments on the basis of the applications, with the experiments consisting of app-specific health checks. For example, OpenEBS, Kafka, CoreDNS. For a full list of supported chaos experiments, visit: https://hub.litmuschaos.io What are the permissions required to run Litmus Chaos Experiments? \u00b6 By default, the Litmus operator uses the \u201clitmus\u201d serviceaccount that is bound to a ClusterRole, in order to watch for the ChaosEngine resource across namespaces. However, the experiments themselves are associated with \u201cchaosServiceAccounts\u201d which are created by the developers with bare-minimum permissions necessary to execute the experiment in question. Visit the chaos-charts repo to view the experiment-specific rbac permissions. For example, here are the permissions for container-kill chaos. What is the scope of a Litmus Chaos Experiment? \u00b6 The chaos CRs (chaosexperiment, chaosengine, chaosresults) themselves are namespace scoped and are installed in the same namespace as that of the target application. While most of the experiments can be executed with service accounts mapped to namespaced roles, some infra chaos experiments typically perform health checks of applications across namespaces & therefore need their serviceaccounts mapped to ClusterRoles. To get started with running chaos experiments using Litmus? \u00b6 Litmus has a low entry barrier and is easy to install/use. Typically, it involves installing the chaos-operator, chaos experiment CRs from the charthub , annotating an application for chaos and creating a chaosengine CR to map your application instance with a desired chaos experiment. Refer to the getting started documentation to learn more on how to run a simple chaos experiment. How to view and interpret the results of a chaos experiment? \u00b6 The results of a chaos experiment can be obtained from the verdict property of the chaosresult custom resource. If the verdict is Pass, it means that the application under test is resilient to the chaos injected. Alternatively, Fail reflects that the application is not resilient enough to the injected chaos, and indicates the need for a relook into the deployment sanity or possible application bugs/issues. kubectl describe chaosresult <chaosengine-name>-<chaos-experiment> -n <namespace> The status of the experiment can also be gauged by the \u201cstatus\u201d property of the ChaosEngine. Kubectl describe chaosengine <chaosengne-name> -n <namespace> Do chaos experiments run as a standard set of pods? \u00b6 The chaos experiment (triggered after creation of the ChaosEngine resource) workflow consists of launching the \u201cchaos-runner\u201d pod, which is an umbrella executor of different chaos experiments listed in the engine. The chaos-runner creates one pod (job) per each experiment to run the actual experiment business logic, and also manages the lifecycle of these experiment pods (performs functions such as experiment dependencies validation, job cleanup, patching of status back into ChaosEngine etc.,). Optionally, a monitor pod is created to export the chaos metrics. Together, these 3 pods are a standard set created upon execution of the experiment. The experiment job, in turn may spawn dependent (helper) resources if necessary to run the experiments, but this depends on the experiment selected, chaos libraries chosen etc., Is it mandatory to annotate application deployments for chaos? \u00b6 Typically applications are expected to be annotated with litmuschaos.io/chaos=\"true\" to lend themselves to chaos. This is in order to support selection of the right applications with similar labels in a namespaces, thereby isolating the application under test (AUT) & reduce the blast radius. It is also helpful for supporting automated execution (say, via cron) as a background service. However, in cases where the app deployment specifications are sacrosanct and not expected to be modified, or in cases where annotating a single application for chaos when the experiment itself is known to have a higher blast radius doesn\u2019t make sense (ex: infra chaos), the annotationCheck can be disabled via the ChaosEngine tunable annotationCheck ( .spec.annotationCheck: false ). How to add Custom Annotations as chaos filters? \u00b6 Currently Litmus allows you to set your own/custom keys for Annotation filters, the value being true/false. To use your custom annotation, add this key under an ENV named as CUSTOM_ANNOTATION in ChaosOperator deployment. A sample chaos-operator deployment spec is provided here for reference: view the manifest --- apiVersion: apps/v1 kind: Deployment metadata: name: chaos-operator-ce namespace: litmus spec: replicas: 1 selector: matchLabels: name: chaos-operator template: metadata: labels: name: chaos-operator spec: serviceAccountName: litmus containers: - name: chaos-operator # 'latest' tag corresponds to the latest released image image: litmuschaos/chaos-operator:latest command: - chaos-operator imagePullPolicy: Always env: - name: CUSTOM_ANNOTATION value: \"mayadata.io/chaos\" - name: CHAOS_RUNNER_IMAGE value: \"litmuschaos/chaos-runner:latest\" - name: WATCH_NAMESPACE value: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: OPERATOR_NAME value: \"chaos-operator\" Is it mandatory for the chaosengine and chaos experiment resources to exist in the same namespace? \u00b6 Yes. As of today, the chaos resources are expected to co-exist in the same namespace, which typically is also the application's (AUT) namespace. How to get the chaos logs in Litmus? \u00b6 The chaos logs can be viewed in the following manner. To view the successful launch/removal of chaos resources upon engine creation, for identification of application under test (AUT) etc., view the chaos-operator logs: kubectl logs -f <chaos-operator-(hash)-(hash)> -n <chaos_namespace> To view lifecycle management logs of a given (or set of) chaos experiments, view the chaos-runner logs: kubectl logs -f <chaosengine_name>-runner -n <chaos_namespace> To view the chaos logs itself (details of experiment chaos injection, application health checks et al), view the experiment pod logs: kubectl logs -f <experiment_name_(hash)_(hash)> -n <chaos_namespace> Does Litmus support generation of events during chaos? \u00b6 The chaos-operator generates Kubernetes events to signify the creation of removal of chaos resources over the course of a chaos experiment, which can be obtained by running the following command: kubectl describe chaosengine <chaosengine-name> -n <namespace> Note: Efforts are underway to add more events around chaos injection in subsequent releases. How to stop or abort a chaos experiment? \u00b6 A chaos experiment can be stopped/aborted inflight by patching the .spec.engineState property of the chaosengine to stop . This will delete all the chaos resources associated with the engine/experiment at once. kubectl patch chaosengine <chaosengine-name> -n <namespace> --type merge --patch '{\"spec\":{\"engineState\":\"stop\"}}' The same effect will be caused by deleting the respective chaosengine resource. Can a chaos experiment be resumed once stopped or aborted? \u00b6 Once stopped/aborted, patching the chaosengine .spec.engineState with active causes the experiment to be re-executed. Another way is to re-apply the ChaosEngine YAML, this will delete all stale chaos resources, and restart ChaosEngine lifecycle. However, support is yet to be added for saving state and resuming an in-flight experiment (i.e., execute pending iterations etc.,) kubectl patch chaosengine <chaosengine-name> -n <namespace> --type merge --patch '{\"spec\":{\"engineState\":\"active\"}}' How to restart chaosengine after graceful completion? \u00b6 To restart chaosengine, check the .spec.engineState, which should be equal to stop, which means your chaosengine has gracefully completed, or forcefully aborted. In this case, restart is quite easy, as you can re-apply the chaosengine YAML to restart it. This will remove all stale chaos resources linked to this chaosengine, and restart its own lifecycle. Does Litmus support any chaos metrics for experiments? \u00b6 Litmus provides a basic set of prometheus metrics indicating the total count of chaos experiments, passed/failed experiments and individual status of experiments specified in the ChaosEngine, which can be queried against the monitor pod. Work to enhance and improve this is underway. Does Litmus track any usage metrics on the test clusters? \u00b6 By default, the installation count of chaos-operator & run count of a given chaos experiment is collected as part of general analytics to gauge user adoption & chaos trends. However, if you wish to inhibit this, please use the following ENV setting on the chaos-operator deployment: env: name: ANALYTICS value: 'FALSE' What to choose between minChaosInterval and instanceCount? \u00b6 Only one should be chosen ideally between minChaosInterval and instanceCount. However if both are specified minChaosInterval will be given priority. minChaosInterval specifies the minimum interval that should be present between the launch of 2 chaosengines and instanceCount specifies the exact number of chaosengines to be launched between the range (start and end time). SO we can choose depending on our requirements.","title":"Experiments"},{"location":"experiments/faq/experiments/#litmus-experiments","text":"","title":"Litmus Experiments"},{"location":"experiments/faq/experiments/#table-of-contents","text":"Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? What are the typical probes used for pod-network related experiments? Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? How to run chaos experiment programatically using apis? Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? The experiment is not targeting all pods with the given label, it just selects only one pod by default Do we have a way to see what pods are targeted when users use percentages? What is the function of spec.definition.scope of a ChaosExperiment CR? Pod network latency -- I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? How to check the NETWORK_INTERFACE and SOCKET_PATH variable? What are the different ways to target the pods and nodes for chaos? Does the pod affected perc select the random set of pods from the total pods under chaos? How to extract the chaos start time and end time? How do we check the MTTR (Mean time to recovery) for an application post chaos? What is the difference between Ramp Time and Chaos Interval? Can the appkind be a pod? What type of chaos experiments are supported by Litmus? What are the permissions required to run Litmus Chaos Experiments? What is the scope of a Litmus Chaos Experiment? To get started with running chaos experiments using Litmus? How to view and interpret the results of a chaos experiment? Do chaos experiments run as a standard set of pods? Is it mandatory to annotate application deployments for chaos? How to add Custom Annotations as chaos filters? Is it mandatory for the chaosengine and chaos experiment resources to exist in the same namespace? How to get the chaos logs in Litmus? Does Litmus support generation of events during chaos? How to stop or abort a chaos experiment? Can a chaos experiment be resumed once stopped or aborted? How to restart chaosengine after graceful completion? Does Litmus support any chaos metrics for experiments? Does Litmus track any usage metrics on the test clusters? What to choose between minChaosInterval and instanceCount?","title":"Table of Contents"},{"location":"experiments/faq/experiments/#node-memory-hog-experiments-pod-oom-killed-even-before-the-kubelet-sees-the-memory-stress","text":"The experiment takes a percentage of the total memory capacity of the Node. The helper pod runs on the target node to stress the resources of that node. So The experiment will not consume/hog the memory resources greater than the total memory available on Node. In other words there will always be an upper limit for the amount of memory to be consumed, which equal to the total available memory. Please refer to this blog for more details.","title":"Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress?"},{"location":"experiments/faq/experiments/#pod-network-corruption-and-pod-network-loss-both-experiments-force-network-packet-loss-is-it-worthwhile-trying-out-both-experiments-in-a-scheduled-chaos-test","text":"Yes, ultimately these are different ways to simulate a degraded network. Both cases are expected to typically cause retransmissions (for tcp). The extent of degradation depends on the percentage of loss/corruption","title":"Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test?"},{"location":"experiments/faq/experiments/#how-is-the-packet-loss-achieved-in-pod-network-loss-and-corruption-experiments-what-are-the-internals-of-it","text":"The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in different availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The args passed to the tc netem command run against the target container changes depending on the type of n/w fault","title":"How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it?"},{"location":"experiments/faq/experiments/#whats-the-difference-between-pod-memorycpu-hog-vs-pod-memorycpu-hog-exec","text":"The pod cpu and memory chaos experiment till now (version 1.13.7) was using an exec mode of execution which means - we were execing inside the specified target container and launching process like md5sum and dd to consume the cpu and memory respectively. This is done by providing CHAOS_INJECT_COMMAND and CHAOS-KILL-COMMAND in chaosengine CR. But we have some limitations of using this method. Those were: The chaos inject and kill command are highly dependent on the base image of the target container and may work for some and for others you may have to derive it manually and use it. For scratch images that don't expose shells we couldn't execute the chaos. To overcome this - The stress-chaos experiments (cpu, memory and io) are enhanced to use a non exec mode of chaos execution. It makes use of target container cgroup for the resource allocation and container pid namespace for showing the stress-ng process in target container. This stress-ng process will consume the resources on the target container without doing an exec. The new enhanced experiments are available from litmus 1.13.8 version.","title":"What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec?"},{"location":"experiments/faq/experiments/#what-are-the-typical-probes-used-for-pod-network-related-experiments","text":"Precisely the role of the experiment. Cause n/w degradation w/o the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in diff availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree","title":"What are the typical probes used for pod-network related experiments?"},{"location":"experiments/faq/experiments/#litmus-provides-multiple-libs-to-run-some-chaos-experiments-like-stress-chaos-and-network-chaos-so-which-library-should-be-preferred-to-use","text":"The optional libs (like Pumba) is more of an illustration of how you can use 3 rd party tools with litmus. Called the BYOC (Bring Your Own Chaos). The preferred LIB is litmus .","title":"Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use?"},{"location":"experiments/faq/experiments/#how-to-run-chaos-experiment-programatically-using-apis","text":"To directly consume/manipulate the chaos resources (i.e., chaosexperiment, chaosengine or chaosresults) via API - you can directly use the kube API. The CRDs by default provide us with an API endpoint. You can use any generic client implementation (go/python are most used ones) to access them. In case you use go, there is a clientset available as well: go-client Here are some simple CRUD ops against chaosresources you could construct with curl (I have used kubectl proxy, one could use an auth token instead)- just for illustration purposes.","title":"How to run chaos experiment programatically using apis?"},{"location":"experiments/faq/experiments/#create-chaosengine","text":"For example, assume this is the engine spec curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines -XPOST -H 'Content-Type: application/json' -d@pod-delete-chaosengine-trigger.json","title":"Create ChaosEngine:"},{"location":"experiments/faq/experiments/#read-chaosengine-status","text":"curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos | jq '.status.engineStatus, .status.experiments[].verdict'","title":"Read ChaosEngine status:"},{"location":"experiments/faq/experiments/#update-chaosengine-spec","text":"(say, this is the patch: https://gist.github.com/ksatchit/be54955a1f4231314797f25361ac488d ) curl --header \"Content-Type: application/json-patch+json\" --request PATCH --data '[{\"op\": \"replace\", \"path\": \"/spec/engineState\", \"value\": \"stop\"}]' http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos","title":"Update ChaosEngine Spec:"},{"location":"experiments/faq/experiments/#delete-the-chaosengine-resource","text":"curl -X DELETE localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\"","title":"Delete the ChaosEngine resource:"},{"location":"experiments/faq/experiments/#similarly-to-check-the-resultsverdict-of-the-experiment-from-chaosresult-you-could-use","text":"curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/nginx-chaos-pod-delete | jq '.status.experimentStatus.verdict, .status.experimentStatus.probeSuccessPercentage'","title":"Similarly, to check the results/verdict of the experiment from ChaosResult, you could use:"},{"location":"experiments/faq/experiments/#kubernetes-by-default-has-built-in-features-like-replicasetdeployment-to-prevent-service-unavailability-continuous-curl-from-the-httpprobe-on-litmus-should-not-fail-in-case-of-container-kill-pod-delete-and-oom-due-to-pod-memory-hog-then-why-do-we-need-cpu-io-and-network-related-chaos-experiments","text":"There are some scenarios that can still occur despite whatever availability aids K8s provides. For example, take disk usage or CPU hogs -- problems you would generally refer to as \"Noisy Neighbour\" problems. Stressing the disk w/ continuous and heavy I/O for example can cause degradation in reads and writes performed by other microservices that use this shared disk - for example. (modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out). Another issue is the amount of scratch space eaten up on a node - leading to lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes. Similarly w/ CPU chaos -- by injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in app traffic OR in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.","title":"Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments?"},{"location":"experiments/faq/experiments/#the-experiment-is-not-targeting-all-pods-with-the-given-label-it-just-selects-only-one-pod-by-default","text":"Yes. You can use either the PODS_AFFECTED_PERCENTAGE or TARGET_PODS env to select multiple pods. Refer: experiment tunable envs .","title":"The experiment is not targeting all pods with the given label, it just selects only one pod by default."},{"location":"experiments/faq/experiments/#do-we-have-a-way-to-see-what-pods-are-targeted-when-users-use-percentages","text":"We can view the target pods from the experiment logs or inside chaos results.","title":"Do we have a way to see what pods are targeted when users use percentages?"},{"location":"experiments/faq/experiments/#what-is-the-function-of-specdefinitionscope-of-a-chaosexperiment-cr","text":"The spec.definition.scope & .spec.definition.permissions is mostly for indicative/illustration purposes (for external tools to identify and validate what are the permissions associated to run the exp). By itself, it doesn't influence how and where an exp can be used.One could remove these fields if needed (of course along w/ the crd validation) and store these manifests if desired.","title":"What is the function of spec.definition.scope of a ChaosExperiment CR?"},{"location":"experiments/faq/experiments/#in-pod-network-latency-i-have-pod-a-talking-to-pod-b-over-service-b-and-i-want-to-introduce-latency-between-pod-a-and-service-b-what-would-go-into-specappinfo-section-pod-a-namespace-label-selector-and-kind-what-will-go-into-destination_ip-and-destination_host-service-b-details-what-are-the-target_pods","text":"It will target the [1:total_replicas] (based on PODS_AFFECTED_PERC) numbers of random pods with matching labels(appinfo.applabel) and namespace(appinfo.appns). But if you want to target a specific pod then you can provide their names as a comma separated list inside TARGET_PODS . Yes, you can provide service B details inside DESTINATION_IPS or DESTINATION_HOSTS . The NETWORK_INTERFACE should be eth0 .","title":"In Pod network latency - I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS?"},{"location":"experiments/faq/experiments/#how-to-check-the-network_interface-and-socket_path-variable","text":"The NETWORK_INTERFACE is the interface name inside the pod/container that needs to be targeted. You can find it by execing into the target pod and checking the available interfaces. You can try ip link , iwconfig , ifconfig depending on the tools installed in the pod either of those could work. The SOCKET_PATH by default takes the docker socket path. If you are using something else like containerd, crio or have a different socket path by any chance you can specify it. This is required to communicate with the container runtime of your cluster. In addition to this if container-runtime is different then provide the name of container runtime inside CONTAINER_RUNTIME ENV. It supports docker , containerd , and crio runtimes.","title":"How to check the NETWORK_INTERFACE and SOCKET_PATH variable?"},{"location":"experiments/faq/experiments/#what-are-the-different-ways-to-target-the-pods-and-nodes-for-chaos","text":"The different ways are: Pod Chaos: Appinfo : Provide the target pod labels in the chaos engine appinfo section. TARGET_PODS : You can provide the target pod names as a Comma Separated Variable. Like pod1,pod2. Node Chaos: TARGET_NODE or TARGET_NODES : Provide the target node or nodes in these envs. NODE_LABEL : Provide the label of the target nodes.","title":"What are the different ways to target the pods and nodes for chaos?"},{"location":"experiments/faq/experiments/#does-the-pod-affected-percentage-select-the-random-set-of-pods-from-the-total-pods-under-chaos","text":"Yes, it selects the random pods based on the POD_AFFACTED_PERC ENV. In pod-delete experiment it selects random pods for each iterations of chaos. But for rest of the experiments(if it supports iterations) then it will select random pods once and use the same set of pods for remaining iterations.","title":"Does the pod affected percentage select the random set of pods from the total pods under chaos?"},{"location":"experiments/faq/experiments/#how-to-extract-the-chaos-start-time-and-end-time","text":"We can use the Chaos exporter metrics for the same. One can also visualise these events along with time in chaos engine events.","title":"How to extract the chaos start time and end time?"},{"location":"experiments/faq/experiments/#how-do-we-check-the-mttr-mean-time-to-recovery-for-an-application-post-chaos","text":"The MTTR can be validated by using statusCheck Timeout in the chaos engine. By default its value will be 180 seconds. We can also overwrite this using ChaosEngine. For more details refer this","title":"How do we check the MTTR (Mean time to recovery) for an application post chaos?"},{"location":"experiments/faq/experiments/#what-is-the-difference-between-ramp-time-and-chaos-interval","text":"The ramp time is the time duration to wait before and after injection of chaos in seconds. While the chaos interval is the time interval (in second) between successive chaos iterations.","title":"What is the difference between Ramp Time and Chaos Interval?"},{"location":"experiments/faq/experiments/#can-the-appkind-be-a-pod","text":"The appkind as pod is not supported explicitly. The supported appkind are deployment , statefulset , replicaset , daemonset , rollout , and deploymentconfig . But we can target the pods by following ways: provide labels and namespace at spec.appinfo.applabel and spec.appinfo.appns respectively and provide spec.appinfo.appkind as empty. provide pod names at TARGET_PODS ENV and provide spec.appinfo as nil NOTE : The annotationCheck should be provided as false","title":"Can the appkind be a pod?"},{"location":"experiments/faq/experiments/#what-type-of-chaos-experiments-are-supported-by-litmus","text":"Litmus broadly defines Kubernetes chaos experiments into two categories: application or pod-level chaos experiments and platform or infra-level chaos experiments. The former includes pod-delete, container-kill, pod-cpu-hog, pod-network-loss etc., while the latter includes node-drain, disk-loss, node-cpu-hog etc., The infra chaos experiments typically have a higher blast radius and impacts more than one application deployed on the Kubernetes cluster. Litmus also categorizes experiments on the basis of the applications, with the experiments consisting of app-specific health checks. For example, OpenEBS, Kafka, CoreDNS. For a full list of supported chaos experiments, visit: https://hub.litmuschaos.io","title":"What type of chaos experiments are supported by Litmus?"},{"location":"experiments/faq/experiments/#what-are-the-permissions-required-to-run-litmus-chaos-experiments","text":"By default, the Litmus operator uses the \u201clitmus\u201d serviceaccount that is bound to a ClusterRole, in order to watch for the ChaosEngine resource across namespaces. However, the experiments themselves are associated with \u201cchaosServiceAccounts\u201d which are created by the developers with bare-minimum permissions necessary to execute the experiment in question. Visit the chaos-charts repo to view the experiment-specific rbac permissions. For example, here are the permissions for container-kill chaos.","title":"What are the permissions required to run Litmus Chaos Experiments?"},{"location":"experiments/faq/experiments/#what-is-the-scope-of-a-litmus-chaos-experiment","text":"The chaos CRs (chaosexperiment, chaosengine, chaosresults) themselves are namespace scoped and are installed in the same namespace as that of the target application. While most of the experiments can be executed with service accounts mapped to namespaced roles, some infra chaos experiments typically perform health checks of applications across namespaces & therefore need their serviceaccounts mapped to ClusterRoles.","title":"What is the scope of a Litmus Chaos Experiment?"},{"location":"experiments/faq/experiments/#to-get-started-with-running-chaos-experiments-using-litmus","text":"Litmus has a low entry barrier and is easy to install/use. Typically, it involves installing the chaos-operator, chaos experiment CRs from the charthub , annotating an application for chaos and creating a chaosengine CR to map your application instance with a desired chaos experiment. Refer to the getting started documentation to learn more on how to run a simple chaos experiment.","title":"To get started with running chaos experiments using Litmus?"},{"location":"experiments/faq/experiments/#how-to-view-and-interpret-the-results-of-a-chaos-experiment","text":"The results of a chaos experiment can be obtained from the verdict property of the chaosresult custom resource. If the verdict is Pass, it means that the application under test is resilient to the chaos injected. Alternatively, Fail reflects that the application is not resilient enough to the injected chaos, and indicates the need for a relook into the deployment sanity or possible application bugs/issues. kubectl describe chaosresult <chaosengine-name>-<chaos-experiment> -n <namespace> The status of the experiment can also be gauged by the \u201cstatus\u201d property of the ChaosEngine. Kubectl describe chaosengine <chaosengne-name> -n <namespace>","title":"How to view and interpret the results of a chaos experiment?"},{"location":"experiments/faq/experiments/#do-chaos-experiments-run-as-a-standard-set-of-pods","text":"The chaos experiment (triggered after creation of the ChaosEngine resource) workflow consists of launching the \u201cchaos-runner\u201d pod, which is an umbrella executor of different chaos experiments listed in the engine. The chaos-runner creates one pod (job) per each experiment to run the actual experiment business logic, and also manages the lifecycle of these experiment pods (performs functions such as experiment dependencies validation, job cleanup, patching of status back into ChaosEngine etc.,). Optionally, a monitor pod is created to export the chaos metrics. Together, these 3 pods are a standard set created upon execution of the experiment. The experiment job, in turn may spawn dependent (helper) resources if necessary to run the experiments, but this depends on the experiment selected, chaos libraries chosen etc.,","title":"Do chaos experiments run as a standard set of pods?"},{"location":"experiments/faq/experiments/#is-it-mandatory-to-annotate-application-deployments-for-chaos","text":"Typically applications are expected to be annotated with litmuschaos.io/chaos=\"true\" to lend themselves to chaos. This is in order to support selection of the right applications with similar labels in a namespaces, thereby isolating the application under test (AUT) & reduce the blast radius. It is also helpful for supporting automated execution (say, via cron) as a background service. However, in cases where the app deployment specifications are sacrosanct and not expected to be modified, or in cases where annotating a single application for chaos when the experiment itself is known to have a higher blast radius doesn\u2019t make sense (ex: infra chaos), the annotationCheck can be disabled via the ChaosEngine tunable annotationCheck ( .spec.annotationCheck: false ).","title":"Is it mandatory to annotate application deployments for chaos?"},{"location":"experiments/faq/experiments/#how-to-add-custom-annotations-as-chaos-filters","text":"Currently Litmus allows you to set your own/custom keys for Annotation filters, the value being true/false. To use your custom annotation, add this key under an ENV named as CUSTOM_ANNOTATION in ChaosOperator deployment. A sample chaos-operator deployment spec is provided here for reference: view the manifest --- apiVersion: apps/v1 kind: Deployment metadata: name: chaos-operator-ce namespace: litmus spec: replicas: 1 selector: matchLabels: name: chaos-operator template: metadata: labels: name: chaos-operator spec: serviceAccountName: litmus containers: - name: chaos-operator # 'latest' tag corresponds to the latest released image image: litmuschaos/chaos-operator:latest command: - chaos-operator imagePullPolicy: Always env: - name: CUSTOM_ANNOTATION value: \"mayadata.io/chaos\" - name: CHAOS_RUNNER_IMAGE value: \"litmuschaos/chaos-runner:latest\" - name: WATCH_NAMESPACE value: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: OPERATOR_NAME value: \"chaos-operator\"","title":"How to add Custom Annotations as chaos filters?"},{"location":"experiments/faq/experiments/#is-it-mandatory-for-the-chaosengine-and-chaos-experiment-resources-to-exist-in-the-same-namespace","text":"Yes. As of today, the chaos resources are expected to co-exist in the same namespace, which typically is also the application's (AUT) namespace.","title":"Is it mandatory for the chaosengine and chaos experiment resources to exist in the same namespace?"},{"location":"experiments/faq/experiments/#how-to-get-the-chaos-logs-in-litmus","text":"The chaos logs can be viewed in the following manner. To view the successful launch/removal of chaos resources upon engine creation, for identification of application under test (AUT) etc., view the chaos-operator logs: kubectl logs -f <chaos-operator-(hash)-(hash)> -n <chaos_namespace> To view lifecycle management logs of a given (or set of) chaos experiments, view the chaos-runner logs: kubectl logs -f <chaosengine_name>-runner -n <chaos_namespace> To view the chaos logs itself (details of experiment chaos injection, application health checks et al), view the experiment pod logs: kubectl logs -f <experiment_name_(hash)_(hash)> -n <chaos_namespace>","title":"How to get the chaos logs in Litmus?"},{"location":"experiments/faq/experiments/#does-litmus-support-generation-of-events-during-chaos","text":"The chaos-operator generates Kubernetes events to signify the creation of removal of chaos resources over the course of a chaos experiment, which can be obtained by running the following command: kubectl describe chaosengine <chaosengine-name> -n <namespace> Note: Efforts are underway to add more events around chaos injection in subsequent releases.","title":"Does Litmus support generation of events during chaos?"},{"location":"experiments/faq/experiments/#how-to-stop-or-abort-a-chaos-experiment","text":"A chaos experiment can be stopped/aborted inflight by patching the .spec.engineState property of the chaosengine to stop . This will delete all the chaos resources associated with the engine/experiment at once. kubectl patch chaosengine <chaosengine-name> -n <namespace> --type merge --patch '{\"spec\":{\"engineState\":\"stop\"}}' The same effect will be caused by deleting the respective chaosengine resource.","title":"How to stop or abort a chaos experiment?"},{"location":"experiments/faq/experiments/#can-a-chaos-experiment-be-resumed-once-stopped-or-aborted","text":"Once stopped/aborted, patching the chaosengine .spec.engineState with active causes the experiment to be re-executed. Another way is to re-apply the ChaosEngine YAML, this will delete all stale chaos resources, and restart ChaosEngine lifecycle. However, support is yet to be added for saving state and resuming an in-flight experiment (i.e., execute pending iterations etc.,) kubectl patch chaosengine <chaosengine-name> -n <namespace> --type merge --patch '{\"spec\":{\"engineState\":\"active\"}}'","title":"Can a chaos experiment be resumed once stopped or aborted?"},{"location":"experiments/faq/experiments/#how-to-restart-chaosengine-after-graceful-completion","text":"To restart chaosengine, check the .spec.engineState, which should be equal to stop, which means your chaosengine has gracefully completed, or forcefully aborted. In this case, restart is quite easy, as you can re-apply the chaosengine YAML to restart it. This will remove all stale chaos resources linked to this chaosengine, and restart its own lifecycle.","title":"How to restart chaosengine after graceful completion?"},{"location":"experiments/faq/experiments/#does-litmus-support-any-chaos-metrics-for-experiments","text":"Litmus provides a basic set of prometheus metrics indicating the total count of chaos experiments, passed/failed experiments and individual status of experiments specified in the ChaosEngine, which can be queried against the monitor pod. Work to enhance and improve this is underway.","title":"Does Litmus support any chaos metrics for experiments?"},{"location":"experiments/faq/experiments/#does-litmus-track-any-usage-metrics-on-the-test-clusters","text":"By default, the installation count of chaos-operator & run count of a given chaos experiment is collected as part of general analytics to gauge user adoption & chaos trends. However, if you wish to inhibit this, please use the following ENV setting on the chaos-operator deployment: env: name: ANALYTICS value: 'FALSE'","title":"Does Litmus track any usage metrics on the test clusters?"},{"location":"experiments/faq/experiments/#what-to-choose-between-minchaosinterval-and-instancecount","text":"Only one should be chosen ideally between minChaosInterval and instanceCount. However if both are specified minChaosInterval will be given priority. minChaosInterval specifies the minimum interval that should be present between the launch of 2 chaosengines and instanceCount specifies the exact number of chaosengines to be launched between the range (start and end time). SO we can choose depending on our requirements.","title":"What to choose between minChaosInterval and instanceCount?"},{"location":"experiments/faq/install/","text":"Install \u00b6 Table of Contents \u00b6 I encountered the concept of namespace and cluster scope during the installation. What is meant by the scopes, and how does it affect experiments to be performed outside or inside the litmus Namespace? Does Litmus 2.0 maintain backward compatibility with Kubernetes? Can I run LitmusChaos Outside of my Kubernetes clusters? What is the minimum system requirement to run Portal and agent together? Can I use LitmusChaos in Production? Why should I use Litmus? What is its distinctive feature? What licensing model does Litmus use? What are the prerequisites to get started with Litmus? How to Install Litmus on the Kubernetes Cluster? I encountered the concept of namespace and cluster scope during the installation. What is meant by the scopes, and how does it affect experiments to be performed outside or inside the litmus Namespace? \u00b6 The scope of portal control plane (portal) installation tuned by the env PORTAL_SCOPE of litmusportal-server deployment can be kept as a namespace if you want to provide very restricted access to litmus; It's useful in dev environments like Okteto cloud etc. That restricts portal installation along with its agent to a single namespace and the chaos operator, exporter all get installed in a single namespace and can only perform and monitor chaos in that namespace. Other than that there is another key in the control plane\u2019s configmap litmus-portal-admin-config called AgentScope, this is given to allow users to restrict access to the litmus self-agent components self-agent is the agent for your control plane cluster (exporter, operator, etc), you can use both of them in a way to give access as per the requirement. The above holds for the control plane and self agent, for the external agents which can be connected using the litmusctl CLI you can provide the scope of the agent while using the utility to connect your other cluster to the control plane with access to just a single namespace or cluster-wide access. Using a combination of AgentScope: cluster and PORTAL_SCOPE env set to cluster would give you cluster-admin privileges to inject chaos on all namespaces where the control plane/portal is installed. For external agents just selecting the scope of installation as cluster would be sufficient via litmusctl. Does Litmus 2.0 maintain backward compatibility with Kubernetes? \u00b6 Yes Litmus maintains a separate CRD manifest to support backward compatibility. Can I run LitmusChaos Outside of my Kubernetes clusters? \u00b6 You can run the chaos experiments outside of the k8s cluster(as a container) which is dockerized. But other components such as chaos-operator,chaos-exporter, and runner are Kubernetes native. They require k8s cluster to run on it. What is the minimum system requirement to run Portal and agent together? \u00b6 To run LitmusPortal you need to have a minimum of 1 GiB memory and 1 core of CPU free. Can I use LitmusChaos in Production? \u00b6 Yes, you can use Litmuschaos in production. Litmus has a wide variety of experiments and is designed as per the principles of chaos. But, if you are new to Chaos Engineering, we would recommend you to first try Litmus on your dev environment, and then after getting the confidence, you should use it in Production. Why should I use Litmus? What is its distinctive feature? \u00b6 Litmus is a toolset to do cloud-native Chaos Engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help developers and SREs find weaknesses in their application deployments. Litmus can be used to run chaos experiments initially in the staging environment and eventually in production to find bugs, vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. Litmus adopts a \u201cKubernetes-native\u201d approach to define chaos intent in a declarative manner via custom resources. What licensing model does Litmus use? \u00b6 Litmus is developed under Apache License 2.0 license at the project level. Some components of the projects are derived from the other Open Source projects and are distributed under their respective licenses. What are the prerequisites to get started with Litmus? \u00b6 For getting started with Litmus the only prerequisites is to have Kubernetes 1.11+ cluster. While most pod/container level experiments are supported on any Kubernetes platform, some of the infrastructure chaos experiments are supported on specific platforms. To find the list of supported platforms for an experiment, view the \"Platforms\" section on the sidebar in the experiment page. How to Install Litmus on the Kubernetes Cluster? \u00b6 You can install/deploy stable litmus using this command: kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-latest.yaml","title":"Install"},{"location":"experiments/faq/install/#install","text":"","title":"Install"},{"location":"experiments/faq/install/#table-of-contents","text":"I encountered the concept of namespace and cluster scope during the installation. What is meant by the scopes, and how does it affect experiments to be performed outside or inside the litmus Namespace? Does Litmus 2.0 maintain backward compatibility with Kubernetes? Can I run LitmusChaos Outside of my Kubernetes clusters? What is the minimum system requirement to run Portal and agent together? Can I use LitmusChaos in Production? Why should I use Litmus? What is its distinctive feature? What licensing model does Litmus use? What are the prerequisites to get started with Litmus? How to Install Litmus on the Kubernetes Cluster?","title":"Table of Contents"},{"location":"experiments/faq/install/#i-encountered-the-concept-of-namespace-and-cluster-scope-during-the-installation-what-is-meant-by-the-scopes-and-how-does-it-affect-experiments-to-be-performed-outside-or-inside-the-litmus-namespace","text":"The scope of portal control plane (portal) installation tuned by the env PORTAL_SCOPE of litmusportal-server deployment can be kept as a namespace if you want to provide very restricted access to litmus; It's useful in dev environments like Okteto cloud etc. That restricts portal installation along with its agent to a single namespace and the chaos operator, exporter all get installed in a single namespace and can only perform and monitor chaos in that namespace. Other than that there is another key in the control plane\u2019s configmap litmus-portal-admin-config called AgentScope, this is given to allow users to restrict access to the litmus self-agent components self-agent is the agent for your control plane cluster (exporter, operator, etc), you can use both of them in a way to give access as per the requirement. The above holds for the control plane and self agent, for the external agents which can be connected using the litmusctl CLI you can provide the scope of the agent while using the utility to connect your other cluster to the control plane with access to just a single namespace or cluster-wide access. Using a combination of AgentScope: cluster and PORTAL_SCOPE env set to cluster would give you cluster-admin privileges to inject chaos on all namespaces where the control plane/portal is installed. For external agents just selecting the scope of installation as cluster would be sufficient via litmusctl.","title":"I encountered the concept of namespace and cluster scope during the installation. What is meant by the scopes, and how does it affect experiments to be performed outside or inside the litmus Namespace?"},{"location":"experiments/faq/install/#does-litmus-20-maintain-backward-compatibility-with-kubernetes","text":"Yes Litmus maintains a separate CRD manifest to support backward compatibility.","title":"Does Litmus 2.0 maintain backward compatibility with Kubernetes?"},{"location":"experiments/faq/install/#can-i-run-litmuschaos-outside-of-my-kubernetes-clusters","text":"You can run the chaos experiments outside of the k8s cluster(as a container) which is dockerized. But other components such as chaos-operator,chaos-exporter, and runner are Kubernetes native. They require k8s cluster to run on it.","title":"Can I run LitmusChaos Outside of my Kubernetes clusters?"},{"location":"experiments/faq/install/#what-is-the-minimum-system-requirement-to-run-portal-and-agent-together","text":"To run LitmusPortal you need to have a minimum of 1 GiB memory and 1 core of CPU free.","title":"What is the minimum system requirement to run Portal and agent together?"},{"location":"experiments/faq/install/#can-i-use-litmuschaos-in-production","text":"Yes, you can use Litmuschaos in production. Litmus has a wide variety of experiments and is designed as per the principles of chaos. But, if you are new to Chaos Engineering, we would recommend you to first try Litmus on your dev environment, and then after getting the confidence, you should use it in Production.","title":"Can I use LitmusChaos in Production?"},{"location":"experiments/faq/install/#why-should-i-use-litmus-what-is-its-distinctive-feature","text":"Litmus is a toolset to do cloud-native Chaos Engineering. Litmus provides tools to orchestrate chaos on Kubernetes to help developers and SREs find weaknesses in their application deployments. Litmus can be used to run chaos experiments initially in the staging environment and eventually in production to find bugs, vulnerabilities. Fixing the weaknesses leads to increased resilience of the system. Litmus adopts a \u201cKubernetes-native\u201d approach to define chaos intent in a declarative manner via custom resources.","title":"Why should I use Litmus? What is its distinctive feature?"},{"location":"experiments/faq/install/#what-licensing-model-does-litmus-use","text":"Litmus is developed under Apache License 2.0 license at the project level. Some components of the projects are derived from the other Open Source projects and are distributed under their respective licenses.","title":"What licensing model does Litmus use?"},{"location":"experiments/faq/install/#what-are-the-prerequisites-to-get-started-with-litmus","text":"For getting started with Litmus the only prerequisites is to have Kubernetes 1.11+ cluster. While most pod/container level experiments are supported on any Kubernetes platform, some of the infrastructure chaos experiments are supported on specific platforms. To find the list of supported platforms for an experiment, view the \"Platforms\" section on the sidebar in the experiment page.","title":"What are the prerequisites to get started with Litmus?"},{"location":"experiments/faq/install/#how-to-install-litmus-on-the-kubernetes-cluster","text":"You can install/deploy stable litmus using this command: kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-latest.yaml","title":"How to Install Litmus on the Kubernetes Cluster?"},{"location":"experiments/faq/portal/","text":"Litmus Portal \u00b6 Table of Contents \u00b6 Can we host MongoDB outside the cluster? What connection string is supported? Is SSL connection supported? What does failed status of workflow means in LitmusPortal? How can I setup a chaoshub of my gitlab repo in Litmus Portal? How to achieve High Availability of MongoDB and how can we add persistence to MongoDB? Can I create workflows without using a dashboard? Does Litmusctl support actions that are currently performed from the portal dashboard? How is resilience score is Calculated? Can we host MongoDB outside the cluster? What connection string is supported? Is SSL connection supported? \u00b6 Yes we can host Mongodb outside the cluster, the mongo string can be updated accordingly DataBaseServer: \"mongodb://mongo-service:27017\" We use the same connection string for both authentication server and graphql server containers in litmus portal-server deployment, also there are the db user and db password keys that can be tuned in the configmap like DB_USER: \"admin\" and DB_PASSWORD: \"1234\". We can connect with SSL if the certificate is optional. If our requirement is ca.cert auth for the SSL connection, then this is not available on the portal What does failed status of workflow means in LitmusPortal? \u00b6 Failed status indicates that either there is some misconfiguration in the workflow or the default hypothesis of the experiment was disproved and some of the experiments in the workflow failed, In such case, the resiliency score will be less than 100. How can I setup a chaoshub of my gitlab repo in Litmus Portal? \u00b6 In the litmus portal when you go to the chaoshub section and you click on connect new hub button, you can see that there are two modes of authentication i.e public mode and private mode. For public mode, you only have to provide the git URL and branch name. For private mode, we have two types of authentication; Access token and SSH key. For the access token, go to the settings of GitLab and in the Access token section, add a token with read repository permission. After getting the token, go to the Litmus portal and provide the GitLab URL and branch name along with the access token. After submitting, your own chaos hub is connected to the Litmus portal. For the second mode of authentication i.e; SSH key, In SSH key once you click on the SSH, It will generate a public key. You have to use the public key and put it in the GitLab setting. Just go to the settings of GitLab, you can see the SSH key section, go to the SSH key section and add your public key. After adding the public key. Get the ssh type URL of the git repository and put it in the Litmusportal along with the branch, after submitting your chaoshub is connected to the Litmus Portal. How to achieve High Availability of MongoDB and how can we add persistence to MongoDB? \u00b6 Currently, the MongoDB instance is not HA, we can install the MongoDB operator along with mongo to achieve HA. This MongoDB CRD allows for specifying the desired size and version as well as several other advanced options. Along with the MongoDB operator, we will use the MongoDB sts with PV to add persistence. Can I create workflows without using a dashboard? \u00b6 Currently, you can\u2019t.But We are working on it. Shortly we will publish samples for doing this via API/SDK and litmusctl. Does Litmusctl support actions that are currently performed from the portal dashboard? \u00b6 For now you can create agents and projects, also you can get the agents and project details by using litmusctl. To know more about litmusctl please refer to the documentation of litmusctl. How is resilience score is Calculated? \u00b6 The Resilience score is calculated on the basis of the weightage and the Probe Success Percentage of the experiment. Resilience for one single experiment is the multiplication of the weight given to that experiment and the Probe Success Percentage. Then we get the total test result by adding the resilience score of all the experiments. The Final Resilience Score is calculated by dividing the total test result by the sum of the weights of all the experiments combined in the single workflow. For more detail refer to this blog.","title":"Portal"},{"location":"experiments/faq/portal/#litmus-portal","text":"","title":"Litmus Portal"},{"location":"experiments/faq/portal/#table-of-contents","text":"Can we host MongoDB outside the cluster? What connection string is supported? Is SSL connection supported? What does failed status of workflow means in LitmusPortal? How can I setup a chaoshub of my gitlab repo in Litmus Portal? How to achieve High Availability of MongoDB and how can we add persistence to MongoDB? Can I create workflows without using a dashboard? Does Litmusctl support actions that are currently performed from the portal dashboard? How is resilience score is Calculated?","title":"Table of Contents"},{"location":"experiments/faq/portal/#can-we-host-mongodb-outside-the-cluster-what-connection-string-is-supported-is-ssl-connection-supported","text":"Yes we can host Mongodb outside the cluster, the mongo string can be updated accordingly DataBaseServer: \"mongodb://mongo-service:27017\" We use the same connection string for both authentication server and graphql server containers in litmus portal-server deployment, also there are the db user and db password keys that can be tuned in the configmap like DB_USER: \"admin\" and DB_PASSWORD: \"1234\". We can connect with SSL if the certificate is optional. If our requirement is ca.cert auth for the SSL connection, then this is not available on the portal","title":"Can we host MongoDB outside the cluster? What connection string is supported? Is SSL connection supported?"},{"location":"experiments/faq/portal/#what-does-failed-status-of-workflow-means-in-litmusportal","text":"Failed status indicates that either there is some misconfiguration in the workflow or the default hypothesis of the experiment was disproved and some of the experiments in the workflow failed, In such case, the resiliency score will be less than 100.","title":"What does failed status of workflow means in LitmusPortal?"},{"location":"experiments/faq/portal/#how-can-i-setup-a-chaoshub-of-my-gitlab-repo-in-litmus-portal","text":"In the litmus portal when you go to the chaoshub section and you click on connect new hub button, you can see that there are two modes of authentication i.e public mode and private mode. For public mode, you only have to provide the git URL and branch name. For private mode, we have two types of authentication; Access token and SSH key. For the access token, go to the settings of GitLab and in the Access token section, add a token with read repository permission. After getting the token, go to the Litmus portal and provide the GitLab URL and branch name along with the access token. After submitting, your own chaos hub is connected to the Litmus portal. For the second mode of authentication i.e; SSH key, In SSH key once you click on the SSH, It will generate a public key. You have to use the public key and put it in the GitLab setting. Just go to the settings of GitLab, you can see the SSH key section, go to the SSH key section and add your public key. After adding the public key. Get the ssh type URL of the git repository and put it in the Litmusportal along with the branch, after submitting your chaoshub is connected to the Litmus Portal.","title":"How can I setup a chaoshub of my gitlab repo in Litmus Portal?"},{"location":"experiments/faq/portal/#how-to-achieve-high-availability-of-mongodb-and-how-can-we-add-persistence-to-mongodb","text":"Currently, the MongoDB instance is not HA, we can install the MongoDB operator along with mongo to achieve HA. This MongoDB CRD allows for specifying the desired size and version as well as several other advanced options. Along with the MongoDB operator, we will use the MongoDB sts with PV to add persistence.","title":"How to achieve High Availability of MongoDB and how can we add persistence to MongoDB?"},{"location":"experiments/faq/portal/#can-i-create-workflows-without-using-a-dashboard","text":"Currently, you can\u2019t.But We are working on it. Shortly we will publish samples for doing this via API/SDK and litmusctl.","title":"Can I create workflows without using a dashboard?"},{"location":"experiments/faq/portal/#does-litmusctl-support-actions-that-are-currently-performed-from-the-portal-dashboard","text":"For now you can create agents and projects, also you can get the agents and project details by using litmusctl. To know more about litmusctl please refer to the documentation of litmusctl.","title":"Does Litmusctl support actions that are currently performed from the portal dashboard?"},{"location":"experiments/faq/portal/#how-is-resilience-score-is-calculated","text":"The Resilience score is calculated on the basis of the weightage and the Probe Success Percentage of the experiment. Resilience for one single experiment is the multiplication of the weight given to that experiment and the Probe Success Percentage. Then we get the total test result by adding the resilience score of all the experiments. The Final Resilience Score is calculated by dividing the total test result by the sum of the weights of all the experiments combined in the single workflow. For more detail refer to this blog.","title":"How is resilience score is Calculated?"},{"location":"experiments/faq/scheduler/","text":"Chaos Scheduler \u00b6 Table of Contents \u00b6 What is ChaosScheduler? How is ChaosScheduler different from ChaosOperator? What are the pre-requisites for ChaosScheduler? How to install ChaosScheduler? How to schedule the chaos using ChaosScheduler? What are the different techniques of scheduling the chaos? What fields of spec.schedule are to be specified with spec.schedule.type=now? What fields of spec.schedule are to be specified with spec.schedule.type=once? What fields of spec.schedule are to be specified with spec.schedule.type=repeat? How to run ChaosScheduler in Namespaced mode? What is ChaosScheduler? \u00b6 ChaosScheduler is an operator built on top of the operator-sdk framework. It keeps on watching resources of kind ChaosSchedule and based on the scheduling parameters automates the formation of ChaosEngines, to be observed by ChaosOperator, instead of manually forming the ChaosEngine every time we wish to inject chaos in the cluster. How is ChaosScheduler different from ChaosOperator? \u00b6 ChaosOperator operates on chaosengines while ChaosScheduler operates on chaosschedules which in turn forms chaosengines, through some scheduling techniques, to be observed by ChaosOperator. So ChaosOperator is a basic building block used to inject chaos in a cluster while ChaosScheduler is just a scheduling strategy that injects chaos in some form of pattern using ChaosOperator only. ChaosScheduler can not be used independently of ChaosOperator. What are the pre-requisites for ChaosScheduler? \u00b6 For getting started with ChaosScheduler, we should just have ChaosOperator and all the litmus infrastructure components installed in the cluster beforehand. How to install ChaosScheduler? \u00b6 Firstly install the rbac and crd - kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/crds/chaosschedule_crd.yaml Install ChaosScheduler operator afterwards - kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/chaos-scheduler.yaml How to schedule the chaos using ChaosScheduler? \u00b6 This depends on which type of schedule we want to use for injecting chaos. For basic understanding refer constructing schedule What are the different techniques of scheduling the chaos? \u00b6 As of now, there are 3 scheduling techniques which can be selected based on the parameter passed to spec.schedule.type type=now type=once type=repeat What fields of spec.schedule are to be specified with spec.schedule.type=now? \u00b6 No fields are needed to be specified for this as it launches the desired chaosengine immediately. What fields of spec.schedule are to be specified with spec.schedule.type=once? \u00b6 We just need to pass spec.executionTime. Scheduler will launch the chaosengine exactly at the point of time mentioned in this parameter. What fields of spec.schedule are to be specified with spec.schedule.type=repeat? \u00b6 All the fields of spec.schedule except spec.schedule.executionTime are needed to be specified. startTime endTime minChaosInterval includedHours includedDays It schedules chaosengines to be launched according to the parameters passed. It works just as a cronjob does, having superior functionalities such as we can control when the schedule will start and end. How to run ChaosScheduler in Namespaced mode? \u00b6 Firstly install the crd - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-scheduler-namespaced-crd.yaml Secondly install the rbac in the desired Namespace - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-scheduler-ns-rbac.yaml -n <namespace> Install ChaosScheduler operator in the desired Namespace afterwards - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-namespaced-scheduler.yaml -n <namespace> Execute ChaosScheduler with an experiment in the desired Namespace afterward. Note : The ChaosServiceAccount used within the embedded ChaosEngine template needs to be chosen appropriately depending on the experiment scope. - ```yaml apiVersion: litmuschaos.io/v1alpha1 kind: ChaosSchedule metadata: name: schedule-nginx namespace: spec: schedule: repeat: timeRange: startTime: \"2020-05-12T05:47:00Z\" #should be modified according to current UTC Time, for type=repeat endTime: \"2020-09-13T02:58:00Z\" #should be modified according to current UTC Time, for type=repeat properties: minChaosInterval: \"2m\" #format should be like \"10m\" or \"2h\" accordingly for minutes and hours, for type=repeat workHours: includedHours: 0-12 workDays: includedDays: \"Mon,Tue,Wed,Sat,Sun\" #should be set for type=repeat engineTemplateSpec: appinfo: appns: 'default' applabel: 'app=nginx' appkind: 'deployment' # It can be true/false annotationCheck: 'false' # It can be active/stop engineState: 'active' #ex. values: ns1:name=percona,ns2:run=nginx auxiliaryAppInfo: '' chaosServiceAccount: pod-delete-sa # It can be delete/retain jobCleanUpPolicy: 'delete' experiments: - name: pod-delete spec: components: env: # set chaos duration (in sec) as desired - name: TOTAL_CHAOS_DURATION value: '30' # set chaos interval (in sec) as desired - name: CHAOS_INTERVAL value: '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name: FORCE value: 'false'","title":"Scheduler"},{"location":"experiments/faq/scheduler/#chaos-scheduler","text":"","title":"Chaos Scheduler"},{"location":"experiments/faq/scheduler/#table-of-contents","text":"What is ChaosScheduler? How is ChaosScheduler different from ChaosOperator? What are the pre-requisites for ChaosScheduler? How to install ChaosScheduler? How to schedule the chaos using ChaosScheduler? What are the different techniques of scheduling the chaos? What fields of spec.schedule are to be specified with spec.schedule.type=now? What fields of spec.schedule are to be specified with spec.schedule.type=once? What fields of spec.schedule are to be specified with spec.schedule.type=repeat? How to run ChaosScheduler in Namespaced mode?","title":"Table of Contents"},{"location":"experiments/faq/scheduler/#what-is-chaosscheduler","text":"ChaosScheduler is an operator built on top of the operator-sdk framework. It keeps on watching resources of kind ChaosSchedule and based on the scheduling parameters automates the formation of ChaosEngines, to be observed by ChaosOperator, instead of manually forming the ChaosEngine every time we wish to inject chaos in the cluster.","title":"What is ChaosScheduler?"},{"location":"experiments/faq/scheduler/#how-is-chaosscheduler-different-from-chaosoperator","text":"ChaosOperator operates on chaosengines while ChaosScheduler operates on chaosschedules which in turn forms chaosengines, through some scheduling techniques, to be observed by ChaosOperator. So ChaosOperator is a basic building block used to inject chaos in a cluster while ChaosScheduler is just a scheduling strategy that injects chaos in some form of pattern using ChaosOperator only. ChaosScheduler can not be used independently of ChaosOperator.","title":"How is ChaosScheduler different from ChaosOperator?"},{"location":"experiments/faq/scheduler/#what-are-the-pre-requisites-for-chaosscheduler","text":"For getting started with ChaosScheduler, we should just have ChaosOperator and all the litmus infrastructure components installed in the cluster beforehand.","title":"What are the pre-requisites for ChaosScheduler?"},{"location":"experiments/faq/scheduler/#how-to-install-chaosscheduler","text":"Firstly install the rbac and crd - kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/rbac.yaml kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/crds/chaosschedule_crd.yaml Install ChaosScheduler operator afterwards - kubectl apply -f https://raw.githubusercontent.com/litmuschaos/chaos-scheduler/master/deploy/chaos-scheduler.yaml","title":"How to install ChaosScheduler?"},{"location":"experiments/faq/scheduler/#how-to-schedule-the-chaos-using-chaosscheduler","text":"This depends on which type of schedule we want to use for injecting chaos. For basic understanding refer constructing schedule","title":"How to schedule the chaos using ChaosScheduler?"},{"location":"experiments/faq/scheduler/#what-are-the-different-techniques-of-scheduling-the-chaos","text":"As of now, there are 3 scheduling techniques which can be selected based on the parameter passed to spec.schedule.type type=now type=once type=repeat","title":"What are the different techniques of scheduling the chaos?"},{"location":"experiments/faq/scheduler/#what-fields-of-specschedule-are-to-be-specified-with-specscheduletypenow","text":"No fields are needed to be specified for this as it launches the desired chaosengine immediately.","title":"What fields of spec.schedule are to be specified with spec.schedule.type=now?"},{"location":"experiments/faq/scheduler/#what-fields-of-specschedule-are-to-be-specified-with-specscheduletypeonce","text":"We just need to pass spec.executionTime. Scheduler will launch the chaosengine exactly at the point of time mentioned in this parameter.","title":"What fields of spec.schedule are to be specified with spec.schedule.type=once?"},{"location":"experiments/faq/scheduler/#what-fields-of-specschedule-are-to-be-specified-with-specscheduletyperepeat","text":"All the fields of spec.schedule except spec.schedule.executionTime are needed to be specified. startTime endTime minChaosInterval includedHours includedDays It schedules chaosengines to be launched according to the parameters passed. It works just as a cronjob does, having superior functionalities such as we can control when the schedule will start and end.","title":"What fields of spec.schedule are to be specified with spec.schedule.type=repeat?"},{"location":"experiments/faq/scheduler/#how-to-run-chaosscheduler-in-namespaced-mode","text":"Firstly install the crd - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-scheduler-namespaced-crd.yaml Secondly install the rbac in the desired Namespace - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-scheduler-ns-rbac.yaml -n <namespace> Install ChaosScheduler operator in the desired Namespace afterwards - kubectl apply -f https://github.com/litmuschaos/litmus/tree/master/mkdocs/docs/litmus-namespaced-scope/litmus-namespaced-scheduler.yaml -n <namespace> Execute ChaosScheduler with an experiment in the desired Namespace afterward. Note : The ChaosServiceAccount used within the embedded ChaosEngine template needs to be chosen appropriately depending on the experiment scope. - ```yaml apiVersion: litmuschaos.io/v1alpha1 kind: ChaosSchedule metadata: name: schedule-nginx namespace: spec: schedule: repeat: timeRange: startTime: \"2020-05-12T05:47:00Z\" #should be modified according to current UTC Time, for type=repeat endTime: \"2020-09-13T02:58:00Z\" #should be modified according to current UTC Time, for type=repeat properties: minChaosInterval: \"2m\" #format should be like \"10m\" or \"2h\" accordingly for minutes and hours, for type=repeat workHours: includedHours: 0-12 workDays: includedDays: \"Mon,Tue,Wed,Sat,Sun\" #should be set for type=repeat engineTemplateSpec: appinfo: appns: 'default' applabel: 'app=nginx' appkind: 'deployment' # It can be true/false annotationCheck: 'false' # It can be active/stop engineState: 'active' #ex. values: ns1:name=percona,ns2:run=nginx auxiliaryAppInfo: '' chaosServiceAccount: pod-delete-sa # It can be delete/retain jobCleanUpPolicy: 'delete' experiments: - name: pod-delete spec: components: env: # set chaos duration (in sec) as desired - name: TOTAL_CHAOS_DURATION value: '30' # set chaos interval (in sec) as desired - name: CHAOS_INTERVAL value: '10' # pod failures without '--force' & default terminationGracePeriodSeconds - name: FORCE value: 'false'","title":"How to run ChaosScheduler in Namespaced mode?"},{"location":"experiments/faq/security/","text":"Security \u00b6 Table of Contents \u00b6 How does litmus enable execution for security practices? How does litmus enable execution for security practices? \u00b6 Litmus supports different modes of operations - the most used ones are the standard/regular modes & admin mode. Latter is used when users have autonomy over the cluster and generally have cluster-wide permissions (say, either dev test purposes on small/transient clusters or SREs with singular access to protected environments). Here there is an all-encompassing svcAccount that can run all experiments (which is shipped with admin mode installation via helm OR can be installed via kubectl using a public manifest) w/o the need to create a chaosServiceAccount or select one for each experiment. So, if you opt for this mode, you need not bother about creating a svcAccount before you run each exp :) In the case of the regular/standard mode (also what is explained as default in the get-started/experiment documentation - for illustrative purposes), the litmus infra or control plane is expected to be setup by an admin persona - a post which developers can run chaos experiments in their respective namespaces by using (creating/selecting) a ChaosServiceAccount that has just enough permissions necessary for their exp. Each experiment in the chaoshub/docs is accompanied by such a reference/example RBAC. This model holds for some staging clusters managed by a cluster-admin persona, but being liberally accessed and used by different developers or service-owners w/ their own chaos needs. Now, in this std mode, the emphasis is on what kind of permissions the developer/user is entrusted with within your self-service environment. If the persona has visibility to other serviceaccounts that [do not belong to] / [have not been created by] him/her, OR has permissions to impact beyond one/more namespaces - then that is a conscious choice. They will be able to use these other SA. The constraint being, the SA has enough permissions to run the chosen experiment, the lack of which can cause failure. On how to ensure security, part - it is expected that these environments are set up in such a way as to allow restricted visibility and actions for the individual users. What litmus provides is an option to still enable them to run experiments in such environments by clearly defining what min. permissions are needed to run a given experiment - so they can end up with a catalog that works for them. We have seen various use-cases around this model in the litmuschaos community. In truly multi-tenant environments that have very strict boundaries & role enforcements, litmus also allows a wholly namespaced mode of execution - where not only the experiment resources but the entire control plane can be created inside a particular namespace (for ex: Okteto Cloud) to ease management and visibility. Here too a single serviceaccount is sufficient to run most pod-level chaos experiments, there being a natural/inherent deterrent for running experiments with cluster-wide/multi-service blast radius.","title":"Security"},{"location":"experiments/faq/security/#security","text":"","title":"Security"},{"location":"experiments/faq/security/#table-of-contents","text":"How does litmus enable execution for security practices?","title":"Table of Contents"},{"location":"experiments/faq/security/#how-does-litmus-enable-execution-for-security-practices","text":"Litmus supports different modes of operations - the most used ones are the standard/regular modes & admin mode. Latter is used when users have autonomy over the cluster and generally have cluster-wide permissions (say, either dev test purposes on small/transient clusters or SREs with singular access to protected environments). Here there is an all-encompassing svcAccount that can run all experiments (which is shipped with admin mode installation via helm OR can be installed via kubectl using a public manifest) w/o the need to create a chaosServiceAccount or select one for each experiment. So, if you opt for this mode, you need not bother about creating a svcAccount before you run each exp :) In the case of the regular/standard mode (also what is explained as default in the get-started/experiment documentation - for illustrative purposes), the litmus infra or control plane is expected to be setup by an admin persona - a post which developers can run chaos experiments in their respective namespaces by using (creating/selecting) a ChaosServiceAccount that has just enough permissions necessary for their exp. Each experiment in the chaoshub/docs is accompanied by such a reference/example RBAC. This model holds for some staging clusters managed by a cluster-admin persona, but being liberally accessed and used by different developers or service-owners w/ their own chaos needs. Now, in this std mode, the emphasis is on what kind of permissions the developer/user is entrusted with within your self-service environment. If the persona has visibility to other serviceaccounts that [do not belong to] / [have not been created by] him/her, OR has permissions to impact beyond one/more namespaces - then that is a conscious choice. They will be able to use these other SA. The constraint being, the SA has enough permissions to run the chosen experiment, the lack of which can cause failure. On how to ensure security, part - it is expected that these environments are set up in such a way as to allow restricted visibility and actions for the individual users. What litmus provides is an option to still enable them to run experiments in such environments by clearly defining what min. permissions are needed to run a given experiment - so they can end up with a catalog that works for them. We have seen various use-cases around this model in the litmuschaos community. In truly multi-tenant environments that have very strict boundaries & role enforcements, litmus also allows a wholly namespaced mode of execution - where not only the experiment resources but the entire control plane can be created inside a particular namespace (for ex: Okteto Cloud) to ease management and visibility. Here too a single serviceaccount is sufficient to run most pod-level chaos experiments, there being a natural/inherent deterrent for running experiments with cluster-wide/multi-service blast radius.","title":"How does litmus enable execution for security practices?"},{"location":"experiments/troubleshooting/experiments/","text":"Litmus Experiments \u00b6 Table of Contents \u00b6 When I\u2019m executing an experiment the experiment's pod failed with the exec format error Nothing happens (no pods created) when the chaosengine resource is created? The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? The experiment pod enters completed state w/o the desired chaos being injected? Observing experiment results using describe chaosresult is showing NotFound error? The helper pod is getting in a failed state due to container runtime issue Disk Fill fail with the error message Disk Fill failed with error Disk fill experiment fails with an error pointing to the helper pods being unable to finish in the given duration The infra experiments like node drain, node taint, kubelet service kill to act on the litmus pods only AWS experiments failed with the following error In AWS SSM Chaos I have provided the aws in secret but still not able to inject the SSM chaos on the target instance GCP VM Disk Loss experiment fails unexpectedly where the disk gets detached successfully but fails to attach back to the instance. What can be the reason? In pod level stress chaos experiments like pod memory hog or pod io stress after the chaos is injected successfully the helper fails with an error message Experiment failed for the istio enabled namespaces When I\u2019m executing an experiment the experiment's pod failed with the exec format error \u00b6 View the error message standard_init_linux.go:211: exec user process caused \"exec format error\": There could be multiple reasons for this. The most common one is mismatched in the binary and the platform on which it is running, try to check out the image binary you're using should have the support for the platform on which you\u2019re trying to run the experiment. Nothing happens (no pods created) when the chaosengine resource is created? \u00b6 If the ChaosEngine creation results in no action at all, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Specifically look for the event reason ChaosResourcesOperationFailed . Typically, these events consist of messages pointing to the problem. Some of the common messages include: Unable to filter app by specified info Unable to get chaos resources Unable to update chaosengine Check the logs of the chaos-operator pod using the following command to get more details (on failed creation of chaos resources). The below example uses litmus namespace, which is the default mode of installation. Please provide the namespace into which the operator has been deployed: kubectl logs -f <chaos-operator-(hash)-(hash)>-runner -n litmus Some of the possible reasons for these errors include: \u00b6 The annotationCheck is set to true in the ChaosEngine spec, but the application deployment (AUT) has not been annotated for chaos. If so, please add it using the following command: kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos=\"true\" The annotationCheck is set to true in the ChaosEngine spec and there are multiple chaos candidates that share the same label (as provided in the .spec.appinfo of the ChaosEngine) and are also annotated for chaos. If so, please provide a unique label for the AUT, or remove annotations on other applications with the same label. Litmus, by default, doesn't allow selection of multiple applications. If this is a requirement, set the annotationCheck to false . kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos- The ChaosEngine has the .spec.engineState set to stop , which causes the operator to refrain from creating chaos resources. While it is an unlikely scenario, it is possible to reuse a previously modified ChaosEngine manifest. Verify if the service account used by the Litmus ChaosOperator has enough permissions to launch pods/services (this is available by default if the manifests suggested by the docs have been used). The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? \u00b6 If the chaos-runner enters completed state immediately post creation, i.e., the creation of experiment resources is unsuccessful, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for one of these events: ExperimentNotFound , ExperimentDependencyCheck , EnvParseError Check the logs of the chaos-runner pod logs. kubectl logs -f <chaosengine_name>-runner -n <namespace> Some of the possible reasons may include: \u00b6 The ChaosExperiment CR for the experiment (name) specified in the ChaosEngine .spec.experiments list is not installed. If so, please install the desired experiment from the chaoshub The dependent resources for the ChaosExperiment, such as ConfigMap & secret volumes (as specified in the ChaosExperiment CR or the ChaosEngine CR) may not be present in the cluster (or in the desired namespace). The runner pod doesn\u2019t proceed with creation of experiment resources if the dependencies are unavailable. The values provided for the ENV variables in the ChaosExperiment or the ChaosEngines might be invalid The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment resources (For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs). The experiment pod enters completed state w/o the desired chaos being injected? \u00b6 If the experiment pod enters completed state immediately (or in a few seconds) after creation w/o injecting the desired chaos, perform the following checks: Check the Kubernetes events generated against the ChaosEngine resource kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for the event with reason Summary with message experiment has been failed Check the logs of the chaos-experiment pod. kubectl logs -f <experiment_name_(hash)_(hash)> -n <namespace> Some of the possible reasons may include: \u00b6 The ChaosExperiment CR or the ChaosEngine CR doesn\u2019t include mandatory ENVs (or consists of incorrect values/info) needed by the experiment. Note that each experiment (see docs) specifies a mandatory set of ENVs along with some optional ones, which are necessary for successful execution of the experiment. The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment helper-resources (i.e., some experiments in turn create other K8s resources like Jobs/Daemonsets/Deployments etc.., For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs) The application's (AUT) unique label provided in the ChaosEngine is set only at the parent resource metadata but not propagated to the pod template spec. Note that the Operator uses this label to filter chaos candidates at the parent resource level (deployment/statefulset/daemonset) but the experiment pod uses this to pick application pods into which the chaos is injected. The experiment pre-chaos checks have failed on account of application (AUT) or auxiliary application unavailability Observing experiment results using describe chaosresult is showing NotFound error? \u00b6 Upon observing the ChaosResults by executing the describe command given below, it may give a NotFound error. kubectl describe chaosresult <chaos-engine-name>-<chaos-experiment-name> -n <namespace> Alternatively, running the describe command without specifying the expected ChaosResult name might execute successfully, but does may not show any output. kubectl describe chaosresult -n <namespace>` This can occur sometimes due to the time taken in pulling the image starting the experiment pod (note that the ChaosResult resource is generated by the experiment). For the above commands to execute successfully, you should simply wait for the experiment pod to be created. The waiting time will be based upon resource available (network bandwidth, space availability on the node filesyste The helper pod is getting in a failed state due to container runtime issue \u00b6 View the error message time=\"2021-07-15T10:26:04Z\" level=fatal msg=\"helper pod failed, err: Unable to run command, err: exit status 1; error output: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\" OR time=\"2021-07-16T22:21:02Z\" level=error msg=\"[docker]: Failed to run docker inspect: []\\nError: No such object: 1807fec21ccad1101bbb63a7d412be15414f807316572f9e043b9f4a3e7c4acc\\n\" time=\"2021-07-16T22:21:02Z\" level=fatal msg=\"helper pod failed, err: exit status 1\" The default values for CONTAINER_RUNTIME & SOCKET_PATH env is for docker runtime. Please check if the cluster runtime is other than docker i.e, containerd then update above ENVs as follow: For containerd runtime: CONTAINER_RUNTIME : containerd SOCKET_PATH : /run/containerd/containerd.sock For CRIO runtime: CONTAINER_RUNTIME : crio SOCKET_PATH : /run/crio/crio.sock NOTE : The above values are the common ones and may vary based on the cluster you\u2019re using. Disk Fill fail with the error message \u00b6 View the error message time=\"2021-08-12T05:27:39Z\" level=fatal msg=\"helper pod failed, err: either provide ephemeral storage limit inside target container or define EPHEMERAL_STORAGE_MEBIBYTES ENV\" The disk fill experiment needs to have either ephemeral storage limit defined in the application or you can provide the value in mebibytes using EPHEMERAL_STORAGE_MEBIBYTES ENV in the chaos engine. Either of them is required. For more details refer: FILL_PERCENTAGE and EPHEMERAL_STORAGE_MEBIBYTES Disk Fill failed with error: \u00b6 View the error message time=\"2021-08-12T05:41:45Z\" level=error msg=\"du: /diskfill/8a1088e3fd50a31d5f0d383ae2258d9975f1df152ff92b3efd570a44e952a732: No such file or directory\\n\" time=\"2021-08-12T05:41:45Z\" level=fatal msg=\"helper pod failed, err: exit status 1\" This could be due to multiple issues in filling the disk of a container the most common one is invalid CONTAINER_PATH env set in the chaosengine. The default container path env is common for most of the use-cases and that is /var/lib/docker/containers Disk fill experiment fails with an error pointing to the helper pods being unable to finish in the given duration. \u00b6 This could be possible when the provided block size is quite less and the empirical storage value is high. In this case, it may need more time than the given chaos duration to fill the disk. The infra experiments like node drain, node taint, kubelet service kill to act on the litmus pods only. \u00b6 Ans: These are the infra level experiments, we need to cordon the target node so that the application pods don\u2019t get scheduled on it and use node selector in the chaos engine to specify the nodes for the experiment pods. Refer to the this to know how to schedule experiments on a certain node. AWS experiments failed with the following error \u00b6 View the error message time=\"2021-08-12T10:25:57Z\" level=error msg=\"failed perform ssm api calls, err: UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 68f0c2e8-a7ed-4576-8c75-0a3ed497efb9\" The AWS experiment needs authentication to connect & perform actions on the aws services we can provide this with the help of the secret as shown below: View the secret manifest apiVersion: v1 kind: Secret metadata: name: cloud-secret type: Opaque stringData: cloud_config.yml: |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX Make sure you have all the required permissions attached with your IAM to perform the chaos operation on the given service. If you are running the experiment in an EKS cluster then you have one more option than creating a secret, you can map the IAM role with the service account refer to this for more details. In AWS SSM Chaos I have provided the aws in secret but still not able to inject the SSM chaos on the target instance \u00b6 View the error message time='2021-08-13T09:30:47Z' level=error msg='failed perform ssm api calls, err: error: the instance id-qqw2-123-12- might not have suitable permission or IAM attached to it. use \\'aws ssm describe-instance-information\\' to check the available instances' Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs GCP VM Disk Loss experiment fails unexpectedly where the disk gets detached successfully but fails to attach back to the instance. What can be the reason? \u00b6 The GCP VM Disk Loss experiment requires a GCP Service Account having a Project Editor or higher permission to execute. This could be because of an issue in the GCP GoLang Compute Engine API, which fails to attach the disk using the attachDisk method with a Compute Admin or lower permission. In pod level stress chaos experiments like pod memory hog or pod io stress after the chaos is injected successfully the helper fails with an error message \u00b6 View the error message Error: process exited before the actual cleanup The error message indicates that the stress process inside the target container is somehow removed before the actual cleanup. There could be multiple reasons for this: the target container might have just got restarted due to excessive load on the container which it can\u2019t handle and the kubelet terminated that replica and launches a new one (if applicable) and reports an OOM event on the older one. Experiment failed for the istio enabled namespaces \u00b6 View the error message W0817 06:32:26.531145 1 client_config.go:541] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. time=\"2021-08-17T06:32:26Z\" level=error msg=\"unable to get ChaosEngineUID, error: unable to get ChaosEngine name: pod-delete-chaos, in namespace: default, error: Get \\\" https://10.100.0.1:443/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/pod-delete-chaos \\\": dial tcp 10.100.0.1:443: connect: connection refused\" If istio is enabled for the chaos-namespace , it will launch the chaos-runner and chaos-experiment pods with the istio sidecar. Which may block/delay the external traffic of those pods for the intial few seconds. Which can fail the experiment. We can fix the above failure by avoiding istio sidecar for the chaos pods. Refer the following manifest: View the ChaosEngine manifest with the required annotations apiVersion: litmuschaos.io/v1alpha1 kind: ChaosEngine metadata: name: engine-nginx spec: components: runner: # annotation for the chaos-runner runnerAnnotations: sidecar.istio.io/inject: \"false\" engineState: \"active\" annotationCheck: \"false\" appinfo: appns: \"default\" applabel: \"app=nginx\" appkind: \"deployment\" chaosServiceAccount: container-kill-sa experiments: - name: container-kill spec: components: #annotations for the experiment pod experimentAnnotations: sidecar.istio.io/inject: \"false\" env: - name: TOTAL_CHAOS_DURATION VALUE: '60'","title":"Experiments"},{"location":"experiments/troubleshooting/experiments/#litmus-experiments","text":"","title":"Litmus Experiments"},{"location":"experiments/troubleshooting/experiments/#table-of-contents","text":"When I\u2019m executing an experiment the experiment's pod failed with the exec format error Nothing happens (no pods created) when the chaosengine resource is created? The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? The experiment pod enters completed state w/o the desired chaos being injected? Observing experiment results using describe chaosresult is showing NotFound error? The helper pod is getting in a failed state due to container runtime issue Disk Fill fail with the error message Disk Fill failed with error Disk fill experiment fails with an error pointing to the helper pods being unable to finish in the given duration The infra experiments like node drain, node taint, kubelet service kill to act on the litmus pods only AWS experiments failed with the following error In AWS SSM Chaos I have provided the aws in secret but still not able to inject the SSM chaos on the target instance GCP VM Disk Loss experiment fails unexpectedly where the disk gets detached successfully but fails to attach back to the instance. What can be the reason? In pod level stress chaos experiments like pod memory hog or pod io stress after the chaos is injected successfully the helper fails with an error message Experiment failed for the istio enabled namespaces","title":"Table of Contents"},{"location":"experiments/troubleshooting/experiments/#when-im-executing-an-experiment-the-experiments-pod-failed-with-the-exec-format-error","text":"View the error message standard_init_linux.go:211: exec user process caused \"exec format error\": There could be multiple reasons for this. The most common one is mismatched in the binary and the platform on which it is running, try to check out the image binary you're using should have the support for the platform on which you\u2019re trying to run the experiment.","title":"When I\u2019m executing an experiment the experiment's pod failed with the exec format error"},{"location":"experiments/troubleshooting/experiments/#nothing-happens-no-pods-created-when-the-chaosengine-resource-is-created","text":"If the ChaosEngine creation results in no action at all, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Specifically look for the event reason ChaosResourcesOperationFailed . Typically, these events consist of messages pointing to the problem. Some of the common messages include: Unable to filter app by specified info Unable to get chaos resources Unable to update chaosengine Check the logs of the chaos-operator pod using the following command to get more details (on failed creation of chaos resources). The below example uses litmus namespace, which is the default mode of installation. Please provide the namespace into which the operator has been deployed: kubectl logs -f <chaos-operator-(hash)-(hash)>-runner -n litmus","title":"Nothing happens (no pods created) when the chaosengine resource is created?"},{"location":"experiments/troubleshooting/experiments/#some-of-the-possible-reasons-for-these-errors-include","text":"The annotationCheck is set to true in the ChaosEngine spec, but the application deployment (AUT) has not been annotated for chaos. If so, please add it using the following command: kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos=\"true\" The annotationCheck is set to true in the ChaosEngine spec and there are multiple chaos candidates that share the same label (as provided in the .spec.appinfo of the ChaosEngine) and are also annotated for chaos. If so, please provide a unique label for the AUT, or remove annotations on other applications with the same label. Litmus, by default, doesn't allow selection of multiple applications. If this is a requirement, set the annotationCheck to false . kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos- The ChaosEngine has the .spec.engineState set to stop , which causes the operator to refrain from creating chaos resources. While it is an unlikely scenario, it is possible to reuse a previously modified ChaosEngine manifest. Verify if the service account used by the Litmus ChaosOperator has enough permissions to launch pods/services (this is available by default if the manifests suggested by the docs have been used).","title":"Some of the possible reasons for these errors include:"},{"location":"experiments/troubleshooting/experiments/#the-chaos-runner-pod-enters-completed-state-seconds-after-getting-created-no-experiment-jobs-are-created","text":"If the chaos-runner enters completed state immediately post creation, i.e., the creation of experiment resources is unsuccessful, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for one of these events: ExperimentNotFound , ExperimentDependencyCheck , EnvParseError Check the logs of the chaos-runner pod logs. kubectl logs -f <chaosengine_name>-runner -n <namespace>","title":"The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created?"},{"location":"experiments/troubleshooting/experiments/#some-of-the-possible-reasons-may-include","text":"The ChaosExperiment CR for the experiment (name) specified in the ChaosEngine .spec.experiments list is not installed. If so, please install the desired experiment from the chaoshub The dependent resources for the ChaosExperiment, such as ConfigMap & secret volumes (as specified in the ChaosExperiment CR or the ChaosEngine CR) may not be present in the cluster (or in the desired namespace). The runner pod doesn\u2019t proceed with creation of experiment resources if the dependencies are unavailable. The values provided for the ENV variables in the ChaosExperiment or the ChaosEngines might be invalid The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment resources (For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs).","title":"Some of the possible reasons may include:"},{"location":"experiments/troubleshooting/experiments/#the-experiment-pod-enters-completed-state-wo-the-desired-chaos-being-injected","text":"If the experiment pod enters completed state immediately (or in a few seconds) after creation w/o injecting the desired chaos, perform the following checks: Check the Kubernetes events generated against the ChaosEngine resource kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for the event with reason Summary with message experiment has been failed Check the logs of the chaos-experiment pod. kubectl logs -f <experiment_name_(hash)_(hash)> -n <namespace>","title":"The experiment pod enters completed state w/o the desired chaos being injected?"},{"location":"experiments/troubleshooting/experiments/#some-of-the-possible-reasons-may-include_1","text":"The ChaosExperiment CR or the ChaosEngine CR doesn\u2019t include mandatory ENVs (or consists of incorrect values/info) needed by the experiment. Note that each experiment (see docs) specifies a mandatory set of ENVs along with some optional ones, which are necessary for successful execution of the experiment. The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment helper-resources (i.e., some experiments in turn create other K8s resources like Jobs/Daemonsets/Deployments etc.., For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs) The application's (AUT) unique label provided in the ChaosEngine is set only at the parent resource metadata but not propagated to the pod template spec. Note that the Operator uses this label to filter chaos candidates at the parent resource level (deployment/statefulset/daemonset) but the experiment pod uses this to pick application pods into which the chaos is injected. The experiment pre-chaos checks have failed on account of application (AUT) or auxiliary application unavailability","title":"Some of the possible reasons may include:"},{"location":"experiments/troubleshooting/experiments/#observing-experiment-results-using-describe-chaosresult-is-showing-notfound-error","text":"Upon observing the ChaosResults by executing the describe command given below, it may give a NotFound error. kubectl describe chaosresult <chaos-engine-name>-<chaos-experiment-name> -n <namespace> Alternatively, running the describe command without specifying the expected ChaosResult name might execute successfully, but does may not show any output. kubectl describe chaosresult -n <namespace>` This can occur sometimes due to the time taken in pulling the image starting the experiment pod (note that the ChaosResult resource is generated by the experiment). For the above commands to execute successfully, you should simply wait for the experiment pod to be created. The waiting time will be based upon resource available (network bandwidth, space availability on the node filesyste","title":"Observing experiment results using describe chaosresult is showing NotFound error?"},{"location":"experiments/troubleshooting/experiments/#the-helper-pod-is-getting-in-a-failed-state-due-to-container-runtime-issue","text":"View the error message time=\"2021-07-15T10:26:04Z\" level=fatal msg=\"helper pod failed, err: Unable to run command, err: exit status 1; error output: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\" OR time=\"2021-07-16T22:21:02Z\" level=error msg=\"[docker]: Failed to run docker inspect: []\\nError: No such object: 1807fec21ccad1101bbb63a7d412be15414f807316572f9e043b9f4a3e7c4acc\\n\" time=\"2021-07-16T22:21:02Z\" level=fatal msg=\"helper pod failed, err: exit status 1\" The default values for CONTAINER_RUNTIME & SOCKET_PATH env is for docker runtime. Please check if the cluster runtime is other than docker i.e, containerd then update above ENVs as follow: For containerd runtime: CONTAINER_RUNTIME : containerd SOCKET_PATH : /run/containerd/containerd.sock For CRIO runtime: CONTAINER_RUNTIME : crio SOCKET_PATH : /run/crio/crio.sock NOTE : The above values are the common ones and may vary based on the cluster you\u2019re using.","title":"The helper pod is getting in a failed state due to container runtime issue"},{"location":"experiments/troubleshooting/experiments/#disk-fill-fail-with-the-error-message","text":"View the error message time=\"2021-08-12T05:27:39Z\" level=fatal msg=\"helper pod failed, err: either provide ephemeral storage limit inside target container or define EPHEMERAL_STORAGE_MEBIBYTES ENV\" The disk fill experiment needs to have either ephemeral storage limit defined in the application or you can provide the value in mebibytes using EPHEMERAL_STORAGE_MEBIBYTES ENV in the chaos engine. Either of them is required. For more details refer: FILL_PERCENTAGE and EPHEMERAL_STORAGE_MEBIBYTES","title":"Disk Fill fail with the error message"},{"location":"experiments/troubleshooting/experiments/#disk-fill-failed-with-error","text":"View the error message time=\"2021-08-12T05:41:45Z\" level=error msg=\"du: /diskfill/8a1088e3fd50a31d5f0d383ae2258d9975f1df152ff92b3efd570a44e952a732: No such file or directory\\n\" time=\"2021-08-12T05:41:45Z\" level=fatal msg=\"helper pod failed, err: exit status 1\" This could be due to multiple issues in filling the disk of a container the most common one is invalid CONTAINER_PATH env set in the chaosengine. The default container path env is common for most of the use-cases and that is /var/lib/docker/containers","title":"Disk Fill failed with error:"},{"location":"experiments/troubleshooting/experiments/#disk-fill-experiment-fails-with-an-error-pointing-to-the-helper-pods-being-unable-to-finish-in-the-given-duration","text":"This could be possible when the provided block size is quite less and the empirical storage value is high. In this case, it may need more time than the given chaos duration to fill the disk.","title":"Disk fill experiment fails with an error pointing to the helper pods being unable to finish in the given duration."},{"location":"experiments/troubleshooting/experiments/#the-infra-experiments-like-node-drain-node-taint-kubelet-service-kill-to-act-on-the-litmus-pods-only","text":"Ans: These are the infra level experiments, we need to cordon the target node so that the application pods don\u2019t get scheduled on it and use node selector in the chaos engine to specify the nodes for the experiment pods. Refer to the this to know how to schedule experiments on a certain node.","title":"The infra experiments like node drain, node taint, kubelet service kill to act on the litmus pods only."},{"location":"experiments/troubleshooting/experiments/#aws-experiments-failed-with-the-following-error","text":"View the error message time=\"2021-08-12T10:25:57Z\" level=error msg=\"failed perform ssm api calls, err: UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 68f0c2e8-a7ed-4576-8c75-0a3ed497efb9\" The AWS experiment needs authentication to connect & perform actions on the aws services we can provide this with the help of the secret as shown below: View the secret manifest apiVersion: v1 kind: Secret metadata: name: cloud-secret type: Opaque stringData: cloud_config.yml: |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX Make sure you have all the required permissions attached with your IAM to perform the chaos operation on the given service. If you are running the experiment in an EKS cluster then you have one more option than creating a secret, you can map the IAM role with the service account refer to this for more details.","title":"AWS experiments failed with the following error"},{"location":"experiments/troubleshooting/experiments/#in-aws-ssm-chaos-i-have-provided-the-aws-in-secret-but-still-not-able-to-inject-the-ssm-chaos-on-the-target-instance","text":"View the error message time='2021-08-13T09:30:47Z' level=error msg='failed perform ssm api calls, err: error: the instance id-qqw2-123-12- might not have suitable permission or IAM attached to it. use \\'aws ssm describe-instance-information\\' to check the available instances' Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs","title":"In AWS SSM Chaos I have provided the aws in secret but still not able to inject the SSM chaos on the target instance"},{"location":"experiments/troubleshooting/experiments/#gcp-vm-disk-loss-experiment-fails-unexpectedly-where-the-disk-gets-detached-successfully-but-fails-to-attach-back-to-the-instance-what-can-be-the-reason","text":"The GCP VM Disk Loss experiment requires a GCP Service Account having a Project Editor or higher permission to execute. This could be because of an issue in the GCP GoLang Compute Engine API, which fails to attach the disk using the attachDisk method with a Compute Admin or lower permission.","title":"GCP VM Disk Loss experiment fails unexpectedly where the disk gets detached successfully but fails to attach back to the instance. What can be the reason?"},{"location":"experiments/troubleshooting/experiments/#in-pod-level-stress-chaos-experiments-like-pod-memory-hog-or-pod-io-stress-after-the-chaos-is-injected-successfully-the-helper-fails-with-an-error-message","text":"View the error message Error: process exited before the actual cleanup The error message indicates that the stress process inside the target container is somehow removed before the actual cleanup. There could be multiple reasons for this: the target container might have just got restarted due to excessive load on the container which it can\u2019t handle and the kubelet terminated that replica and launches a new one (if applicable) and reports an OOM event on the older one.","title":"In pod level stress chaos experiments like pod memory hog or pod io stress after the chaos is injected successfully the helper fails with an error message"},{"location":"experiments/troubleshooting/experiments/#experiment-failed-for-the-istio-enabled-namespaces","text":"View the error message W0817 06:32:26.531145 1 client_config.go:541] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. time=\"2021-08-17T06:32:26Z\" level=error msg=\"unable to get ChaosEngineUID, error: unable to get ChaosEngine name: pod-delete-chaos, in namespace: default, error: Get \\\" https://10.100.0.1:443/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/pod-delete-chaos \\\": dial tcp 10.100.0.1:443: connect: connection refused\" If istio is enabled for the chaos-namespace , it will launch the chaos-runner and chaos-experiment pods with the istio sidecar. Which may block/delay the external traffic of those pods for the intial few seconds. Which can fail the experiment. We can fix the above failure by avoiding istio sidecar for the chaos pods. Refer the following manifest: View the ChaosEngine manifest with the required annotations apiVersion: litmuschaos.io/v1alpha1 kind: ChaosEngine metadata: name: engine-nginx spec: components: runner: # annotation for the chaos-runner runnerAnnotations: sidecar.istio.io/inject: \"false\" engineState: \"active\" annotationCheck: \"false\" appinfo: appns: \"default\" applabel: \"app=nginx\" appkind: \"deployment\" chaosServiceAccount: container-kill-sa experiments: - name: container-kill spec: components: #annotations for the experiment pod experimentAnnotations: sidecar.istio.io/inject: \"false\" env: - name: TOTAL_CHAOS_DURATION VALUE: '60'","title":"Experiment failed for the istio enabled namespaces"},{"location":"experiments/troubleshooting/install/","text":"Install \u00b6 Table of Contents \u00b6 The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation? Litmus uninstallation is not successful and namespace is stuck in terminating state? The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation? \u00b6 Verify if the ChaosEngine custom resource definition (CRD) has been installed in the cluster. This can be verified with the following commands: kubectl get crds | grep chaos kubectl api-resources | grep chaos If not created, install it from here Litmus uninstallation is not successful and namespace is stuck in terminating state? \u00b6 Under typical operating conditions, the ChaosOperator makes use of finalizers to ensure that the ChaosEngine is deleted only after chaos resources (chaos-runner, experiment pod, any other helper pods) are removed. When uninstalling Litmus via the operator manifest (which contains the namespace, operator, crd specifictions in a single YAML) without deleting the existing chaosengine resources first, the ChaosOperator deployment may get deleted before the CRD removal is attempted. Since the stale chaosengines have the finalizer present on them, their deletion (triggered by the CRD delete) and by consequence, the deletion of the chaosengine CRD itself is \"stuck\". In such cases, manually remove the finalizer entries on the stale chaosengines to facilitate their successful delete. To get the chaosengine, run: kubectl get chaosengine -n <namespace> followed by: kubectl edit chaosengine <chaosengine-name> -n <namespace> and remove the finalizer entry chaosengine.litmuschaos.io/finalizer Repeat this on all the stale chaosengine CRs to remove the CRDs successfully & complete uninstallation process. If however, the litmus namespace deletion remains stuck despite the above actions, follow the procedure described here to complete the uninstallation.","title":"Install"},{"location":"experiments/troubleshooting/install/#install","text":"","title":"Install"},{"location":"experiments/troubleshooting/install/#table-of-contents","text":"The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation? Litmus uninstallation is not successful and namespace is stuck in terminating state?","title":"Table of Contents"},{"location":"experiments/troubleshooting/install/#the-litmus-chaosoperator-is-seen-to-be-in-crashloopbackoff-state-immediately-after-installation","text":"Verify if the ChaosEngine custom resource definition (CRD) has been installed in the cluster. This can be verified with the following commands: kubectl get crds | grep chaos kubectl api-resources | grep chaos If not created, install it from here","title":"The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation?"},{"location":"experiments/troubleshooting/install/#litmus-uninstallation-is-not-successful-and-namespace-is-stuck-in-terminating-state","text":"Under typical operating conditions, the ChaosOperator makes use of finalizers to ensure that the ChaosEngine is deleted only after chaos resources (chaos-runner, experiment pod, any other helper pods) are removed. When uninstalling Litmus via the operator manifest (which contains the namespace, operator, crd specifictions in a single YAML) without deleting the existing chaosengine resources first, the ChaosOperator deployment may get deleted before the CRD removal is attempted. Since the stale chaosengines have the finalizer present on them, their deletion (triggered by the CRD delete) and by consequence, the deletion of the chaosengine CRD itself is \"stuck\". In such cases, manually remove the finalizer entries on the stale chaosengines to facilitate their successful delete. To get the chaosengine, run: kubectl get chaosengine -n <namespace> followed by: kubectl edit chaosengine <chaosengine-name> -n <namespace> and remove the finalizer entry chaosengine.litmuschaos.io/finalizer Repeat this on all the stale chaosengine CRs to remove the CRDs successfully & complete uninstallation process. If however, the litmus namespace deletion remains stuck despite the above actions, follow the procedure described here to complete the uninstallation.","title":"Litmus uninstallation is not successful and namespace is stuck in terminating state?"},{"location":"experiments/troubleshooting/portal/","text":"Litmus Portal \u00b6 Table of Contents \u00b6 We were setting up a Litmus Portal, however, Self-Agent status is showing pending. Any idea why is happening? After logging in for the first time to the portal, /get-started page kept loading after I provided the new password Subscriber is crashing with the error dial:websocket: bad handshake Not able to connect to the LitmusChaos Control Plane hosted on GKE cluster I forgot my Litmus portal password. How can I reset my credentials? While Uninstalling Litmus portal using helm, some components like subscriber, exporter, event, workflows, etc, are not removed Unable to Install Litmus portal using helm. Server pod and mongo pod are in CrashLoopBackOff state. Got this error while checking the logs of mongo container chown: changing ownership of '/data/db/.snapshot': Read-only file system Pre-defined workflow Bank Of Anthos showing bus error for accounts-db or ledger-db pod? We were setting up a Litmus Portal, however, Self-Agent status is showing pending. Any idea why is happening? \u00b6 The litmusportal-server-service might not be reachable due to inbound rules. You can enable the traffic to it if on GKE/EKS/AKS (by adding the port to inbound rules for traffic). You have to check the logs of the subscriber pod and expose the port mentioned for communication with the server. After logging in for the first time to the portal, /get-started page kept loading after I provided the new password. \u00b6 First, try to clear the browser cache and cookies and refresh the page, this might solve your problem. If your problem persists then delete all the cluster role bindings,PV, and PVC used by litmus and try to reinstall the litmus again. Subscriber is crashing with the error dial:websocket: bad handshake \u00b6 It is a network issue. It seems your subscriber is unable to access the server. While installing the agent, It creates a config called agent-config to store some metadata like server endpoint, accesskey, etc. That server endpoint can be generated in many ways: Ingress (If INGRESS=true in server deployment envs) Loadbalancer (it generates lb type of IP based on the server svc type) NodePort (it generates nodeport type of IP based on the server svc type) ClusterIP (it generates clusterip type of IP based on the server svc type) So, you can edit the agent-config and update the node IP. Once edited, restart the subscriber. We suggest using ingress, so that if the endpoint IP changes, then it won't affect your agent. Not able to connect to the LitmusChaos Control Plane hosted on GKE cluster. \u00b6 In GKE you have to setup a firewall rule to allow TCP traffic on the node port.You can use the following command: gcloud compute firewall-rules create test-node-port --allow tcp:port If this firewall rule is set up, it may be accessible on nodeIp:port where nodeIp is the external IP address of your node. I forgot my Litmus portal password. How can I reset my credentials? \u00b6 You can reset by running the followin command: kubectl exec -it mongo-0 -n litmus -- mongo -u admin -p 1234 <<< $'use auth\\ndb.usercredentials.update({username:\"admin\"},{$set:{password:\"$2a$15$sNuQl9y/Ok92N19UORcro.3wulEyFi0FfJrnN/akOQe3uxTZAzQ0C\"}})\\nexit\\n' Make sure to update the namespace and mongo pod name according to your setup,the rest should remain the same. This command will update the password to litmus. While Uninstalling Litmus portal using helm, some components like subscriber, exporter, event, workflows, etc, are not removed. \u00b6 These are agent components, which are launched by the control plane server, so first disconnect the agent from the portal then uninstall the portal using helm. Unable to Install Litmus portal using helm. Server pod and mongo pod are in CrashLoopBackOff state. Got this error while checking the logs of mongo container chown: changing ownership of '/data/db/.snapshot': Read-only file system \u00b6 It seems the directory somehow existed before litmus installation and might be used by some other application. You have to change the mount path from /consul/config to /consul/myconfig in mongo statefulset then you can successfully deploy the litmus. Pre-defined workflow Bank Of Anthos showing bus error for accounts-db or ledger-db pod? \u00b6 Bank of anthos is using PostgreSQL and wouldn't fall back properly to not using huge pages. With given possible solution if same scenario occur can be resolve. Modify the docker image to be able to set huge_pages = off in /usr/share/postgresql/postgresql.conf.sample before initdb was ran (this is what I did). Turn off huge page support on the system (vm.nr_hugepages = 0 in /etc/sysctl.conf). Fix Postgres's fallback mechanism when huge_pages = try is set (the default). Modify the k8s manifest to enable huge page support ( https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/ ). Modify k8s to show that huge pages are not supported on the system, when they are not enabled for a specific container.","title":"Portal"},{"location":"experiments/troubleshooting/portal/#litmus-portal","text":"","title":"Litmus Portal"},{"location":"experiments/troubleshooting/portal/#table-of-contents","text":"We were setting up a Litmus Portal, however, Self-Agent status is showing pending. Any idea why is happening? After logging in for the first time to the portal, /get-started page kept loading after I provided the new password Subscriber is crashing with the error dial:websocket: bad handshake Not able to connect to the LitmusChaos Control Plane hosted on GKE cluster I forgot my Litmus portal password. How can I reset my credentials? While Uninstalling Litmus portal using helm, some components like subscriber, exporter, event, workflows, etc, are not removed Unable to Install Litmus portal using helm. Server pod and mongo pod are in CrashLoopBackOff state. Got this error while checking the logs of mongo container chown: changing ownership of '/data/db/.snapshot': Read-only file system Pre-defined workflow Bank Of Anthos showing bus error for accounts-db or ledger-db pod?","title":"Table of Contents"},{"location":"experiments/troubleshooting/portal/#we-were-setting-up-a-litmus-portal-however-self-agent-status-is-showing-pending-any-idea-why-is-happening","text":"The litmusportal-server-service might not be reachable due to inbound rules. You can enable the traffic to it if on GKE/EKS/AKS (by adding the port to inbound rules for traffic). You have to check the logs of the subscriber pod and expose the port mentioned for communication with the server.","title":"We were setting up a Litmus Portal, however, Self-Agent status is showing pending. Any idea why is happening?"},{"location":"experiments/troubleshooting/portal/#after-logging-in-for-the-first-time-to-the-portal-get-started-page-kept-loading-after-i-provided-the-new-password","text":"First, try to clear the browser cache and cookies and refresh the page, this might solve your problem. If your problem persists then delete all the cluster role bindings,PV, and PVC used by litmus and try to reinstall the litmus again.","title":"After logging in for the first time to the portal, /get-started page kept loading after I provided the new password."},{"location":"experiments/troubleshooting/portal/#subscriber-is-crashing-with-the-error-dialwebsocket-bad-handshake","text":"It is a network issue. It seems your subscriber is unable to access the server. While installing the agent, It creates a config called agent-config to store some metadata like server endpoint, accesskey, etc. That server endpoint can be generated in many ways: Ingress (If INGRESS=true in server deployment envs) Loadbalancer (it generates lb type of IP based on the server svc type) NodePort (it generates nodeport type of IP based on the server svc type) ClusterIP (it generates clusterip type of IP based on the server svc type) So, you can edit the agent-config and update the node IP. Once edited, restart the subscriber. We suggest using ingress, so that if the endpoint IP changes, then it won't affect your agent.","title":"Subscriber is crashing with the error dial:websocket: bad handshake"},{"location":"experiments/troubleshooting/portal/#not-able-to-connect-to-the-litmuschaos-control-plane-hosted-on-gke-cluster","text":"In GKE you have to setup a firewall rule to allow TCP traffic on the node port.You can use the following command: gcloud compute firewall-rules create test-node-port --allow tcp:port If this firewall rule is set up, it may be accessible on nodeIp:port where nodeIp is the external IP address of your node.","title":"Not able to connect to the LitmusChaos Control Plane hosted on GKE cluster."},{"location":"experiments/troubleshooting/portal/#i-forgot-my-litmus-portal-password-how-can-i-reset-my-credentials","text":"You can reset by running the followin command: kubectl exec -it mongo-0 -n litmus -- mongo -u admin -p 1234 <<< $'use auth\\ndb.usercredentials.update({username:\"admin\"},{$set:{password:\"$2a$15$sNuQl9y/Ok92N19UORcro.3wulEyFi0FfJrnN/akOQe3uxTZAzQ0C\"}})\\nexit\\n' Make sure to update the namespace and mongo pod name according to your setup,the rest should remain the same. This command will update the password to litmus.","title":"I forgot my Litmus portal password. How can I reset my credentials?"},{"location":"experiments/troubleshooting/portal/#while-uninstalling-litmus-portal-using-helm-some-components-like-subscriber-exporter-event-workflows-etc-are-not-removed","text":"These are agent components, which are launched by the control plane server, so first disconnect the agent from the portal then uninstall the portal using helm.","title":"While Uninstalling Litmus portal using helm, some components like subscriber, exporter, event, workflows, etc, are not removed."},{"location":"experiments/troubleshooting/portal/#unable-to-install-litmus-portal-using-helm-server-pod-and-mongo-pod-are-in-crashloopbackoff-state-got-this-error-while-checking-the-logs-of-mongo-container-chown-changing-ownership-of-datadbsnapshot-read-only-file-system","text":"It seems the directory somehow existed before litmus installation and might be used by some other application. You have to change the mount path from /consul/config to /consul/myconfig in mongo statefulset then you can successfully deploy the litmus.","title":"Unable to Install Litmus portal using helm. Server pod and mongo pod are in CrashLoopBackOff state. Got this error while checking the logs of mongo container chown: changing ownership of '/data/db/.snapshot': Read-only file system"},{"location":"experiments/troubleshooting/portal/#pre-defined-workflow-bank-of-anthos-showing-bus-error-for-accounts-db-or-ledger-db-pod","text":"Bank of anthos is using PostgreSQL and wouldn't fall back properly to not using huge pages. With given possible solution if same scenario occur can be resolve. Modify the docker image to be able to set huge_pages = off in /usr/share/postgresql/postgresql.conf.sample before initdb was ran (this is what I did). Turn off huge page support on the system (vm.nr_hugepages = 0 in /etc/sysctl.conf). Fix Postgres's fallback mechanism when huge_pages = try is set (the default). Modify the k8s manifest to enable huge page support ( https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/ ). Modify k8s to show that huge pages are not supported on the system, when they are not enabled for a specific container.","title":"Pre-defined workflow Bank Of Anthos showing bus error for accounts-db or ledger-db pod?"},{"location":"experiments/troubleshooting/scheduler/","text":"Chaos Scheduler \u00b6 Table of Contents \u00b6 Scheduler not creating chaosengines for type=repeat? Scheduler not creating chaosengines for type=repeat? \u00b6 If the ChaosSchedule has been created successfully created in the cluster and ChaosEngine is not being formed, the most common problem is that either start or end time has been wrongly specified. We should verify the times. We can identify if this is the problem or not by changing to type=now . If the ChaosEngine is formed successfully then the problem is with the specified time ranges, if ChaosEngine is still not formed, then the problem is with engineSpec .","title":"Scheduler"},{"location":"experiments/troubleshooting/scheduler/#chaos-scheduler","text":"","title":"Chaos Scheduler"},{"location":"experiments/troubleshooting/scheduler/#table-of-contents","text":"Scheduler not creating chaosengines for type=repeat?","title":"Table of Contents"},{"location":"experiments/troubleshooting/scheduler/#scheduler-not-creating-chaosengines-for-typerepeat","text":"If the ChaosSchedule has been created successfully created in the cluster and ChaosEngine is not being formed, the most common problem is that either start or end time has been wrongly specified. We should verify the times. We can identify if this is the problem or not by changing to type=now . If the ChaosEngine is formed successfully then the problem is with the specified time ranges, if ChaosEngine is still not formed, then the problem is with engineSpec .","title":"Scheduler not creating chaosengines for type=repeat?"}]}