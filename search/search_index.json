{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ROADMAP/","text":"LITMUS ROADMAP \u00b6 This document captures only the high level roadmap items. For the detailed backlog, see issues list . Completed \u00b6 Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure In-Progress (Under Active Development) \u00b6 Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward Backlog \u00b6 Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Roadmap"},{"location":"ROADMAP/#litmus-roadmap","text":"This document captures only the high level roadmap items. For the detailed backlog, see issues list .","title":"LITMUS ROADMAP"},{"location":"ROADMAP/#completed","text":"Declarative Chaos Intent via custom resources Chaos Operator to orchestrate chaos experiments Off the shelf / ready chaos experiments for general Kubernetes chaos Self sufficient, Centralized Hub for chaos experiments Per-experiment minimal RBAC permissions definition Creation of 'scenarios' involving multiple faults via Argo-based Chaos Workflows (with examples for microservices apps like podtato-head and sock-shop) Cross-Cloud Control Plane (Litmus Portal) to perform chaos against remote clusters Helm3 charts for LitmusChaos (control plane and experiments) Support for admin mode (centralized chaos management) as well as namespaced mode (multi-tenant clusters) Continuous chaos via flexible schedules, with support to halt/resume or (manual/conditional) abort experiments Generation of observability data via Prometheus metrics and Kubernetes chaos events for experiments Steady-State hypothesis validation before, during and after chaos injection via different probe types Support for Docker, Containerd & CRI-O runtime Support for scheduling policies (nodeSelector, tolerations) and resource definitions for chaos pods Support for ARM64 nodes Scaffolding scripts (SDK) to help bootstrap a new chaos experiment in Go, Ansible Support orchestration of non-native chaos libraries via the BYOC (Bring-Your-Own-Chaos) model Support for OpenShift platform Integration tests & e2e framework creation for control plane components and chaos experiments Documentation (usage guide for chaos operator, resources & developer guide for new experiment creation) Add architecture details & design resources Define community sync up cadence and structure","title":"Completed"},{"location":"ROADMAP/#in-progress-under-active-development","text":"Chaos experiments against virtual machines and cloud infrastructure (AWS, GCP, Azure, VMWare, Baremetal) Improved documentation and tutorials for Litmus Portal based execution flow Scaffolding scripts (SDK) to bootstrap experiments in Python Off the shelf chaos-integrated monitoring dashboards for application chaos categories Support for user defined chaos experiment result definition Increased fault injection types (IOChaos, HTTPChaos, JVMChaos) Improved runtime validation of chaos dependencies via litmus admission controllers Special Interest Groups (SIGs) around specific areas in the project to take the roadmap forward","title":"In-Progress (Under Active Development)"},{"location":"ROADMAP/#backlog","text":"Pre-defined chaos workflows to inject chaos during application benchmark runs Support for cloudevents compliant chaos events Improved application Chaos Suites for various CNCF projects","title":"Backlog"},{"location":"experiments/categories/getstarted/","text":"Experiments \u00b6 The experiment execution is triggered upon creation of the ChaosEngine resource (various examples of which are provided under the respective experiments). Typically, these chaosengines are embedded within the 'steps' of a Litmus Chaos Workflow here . However, one may also create the chaos engines directly by hand, and the chaos-operator reconciles this resource and triggers the experiment execution. Provided below are tables with links to the individual experiment docs for easy navigation Kubernetes Experiments \u00b6 It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available: Generic \u00b6 Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart Pod Chaos \u00b6 Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container by invoking a utility within the app container base image pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container by invoking a utility within the app container base image pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss Node Chaos \u00b6 Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint Application Chaos \u00b6 While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete Cloud Infrastructure \u00b6 Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available: AWS \u00b6 Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag GCP \u00b6 Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss Azure \u00b6 Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop VMWare \u00b6 Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"Contents"},{"location":"experiments/categories/getstarted/#experiments","text":"The experiment execution is triggered upon creation of the ChaosEngine resource (various examples of which are provided under the respective experiments). Typically, these chaosengines are embedded within the 'steps' of a Litmus Chaos Workflow here . However, one may also create the chaos engines directly by hand, and the chaos-operator reconciles this resource and triggers the experiment execution. Provided below are tables with links to the individual experiment docs for easy navigation","title":"Experiments"},{"location":"experiments/categories/getstarted/#kubernetes-experiments","text":"It contains chaos experiments which apply on the resources, which are running on the kubernetes cluster. It contains Generic , Kafka , Cassandra experiments. Following Kubernetes Chaos experiments are available:","title":"Kubernetes Experiments"},{"location":"experiments/categories/getstarted/#generic","text":"Chaos actions that apply to generic Kubernetes resources are classified into this category. Following chaos experiments are supported under Generic Chaos Chart","title":"Generic"},{"location":"experiments/categories/getstarted/#pod-chaos","text":"Experiment Name Description User Guide Container Kill Kills the container in the application pod container-kill Disk Fill Fillup Ephemeral Storage of a Resourced disk-fill Pod Autoscaler Scales the application replicas and test the node autoscaling on cluster pod-autoscaler Pod CPU Hog Exec Consumes CPU resources on the application container by invoking a utility within the app container base image pod-cpu-hog-exec Pod CPU Hog Consumes CPU resources on the application container pod-cpu-hog Pod Delete Deletes the application pod pod-delete Pod DNS Error Disrupt dns resolution in kubernetes po pod-dns-error Pod DNS Spoof Spoof dns resolution in kubernetes pod pod-dns-spoof Pod IO Stress Injects IO stress resources on the application container pod-io-stress Pod Memory Hog Exec Consumes Memory resources on the application container by invoking a utility within the app container base image pod-memory-hog-exec Pod Memory Hog Consumes Memory resources on the application container pod-memory-hog Pod Network Corruption Injects Network Packet Corruption into Application Pod pod-network-corruption Pod Network Duplication Injects Network Packet Duplication into Application Pod pod-network-duplication Pod Network Latency Injects Network latency into Application Pod pod-network-latency Pod Network Loss Injects Network loss into Application Pod pod-network-loss","title":"Pod Chaos"},{"location":"experiments/categories/getstarted/#node-chaos","text":"Experiment Name Description User Guide Docker Service Kill Kills the docker service on the application node docker-service-kill Kubelet Service Kill Kills the kubelet service on the application node kubelet-service-kill Node CPU Hog Exhaust CPU resources on the Kubernetes Node node-cpu-hog Node Drain Drains the target node node-drain Node IO Stress Injects IO stress resources on the application node node-io-stress Node Memory Hog Exhaust Memory resources on the Kubernetes Node node-memory-hog Node Restart Restarts the target node node-restart Node Taint Taints the target node node-taint","title":"Node Chaos"},{"location":"experiments/categories/getstarted/#application-chaos","text":"While Chaos Experiments under the Generic category offer the ability to induce chaos into Kubernetes resources, it is difficult to analyze and conclude if the chaos induced found a weakness in a given application. The application specific chaos experiments are built with some checks on pre-conditions and some expected outcomes after the chaos injection. The result of the chaos experiment is determined by matching the outcome with the expected outcome. Experiment Name Description User Guide Kafka Broker Pod Failure Kills the kafka broker pod kafka-broker-pod-failure Cassandra Pod Delete Kills the cassandra pod cassandra-pod-delete","title":"Application Chaos"},{"location":"experiments/categories/getstarted/#cloud-infrastructure","text":"Chaos experiments that inject chaos into the platform resources of Kubernetes are classified into this category. Management of platform resources vary significantly from each other, Chaos Charts may be maintained separately for each platform (For example, AWS, GCP, Azure, etc) Following Platform Chaos experiments are available:","title":"Cloud Infrastructure"},{"location":"experiments/categories/getstarted/#aws","text":"Experiment Name Description User Guide EC2 Terminate By ID Terminate the ec2 instance matched by instance id ec2-terminate-by-id EC2 Terminate By Tag Terminate the ec2 instance matched by instance tag ec2-terminate-by-tag EBS Loss By ID Detach the EBS volume matched by volume id ebs-loss-by-id EBS Loss By Tag Detach the EBS volume matched by volume tag ebs-loss-by-tag","title":"AWS"},{"location":"experiments/categories/getstarted/#gcp","text":"Experiment Name Description User Guide GCP VM Instance Stop Stop the gcp vm instance gcp-vm-instance-stop GCP VM Disk Loss Detach the gcp disk gcp-vm-disk-loss","title":"GCP"},{"location":"experiments/categories/getstarted/#azure","text":"Experiment Name Description User Guide Azure Instance Stop Stop the azure instance azure-instance-stop","title":"Azure"},{"location":"experiments/categories/getstarted/#vmware","text":"Experiment Name Description User Guide VM Poweroff Poweroff the vmware VM vm-poweroff","title":"VMWare"},{"location":"experiments/categories/aws/AWS-experiments-tunables/","text":"It contains the AWS specific experiment tunables. Managed Nodegroup \u00b6 It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"AWS experiments tunables"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#managed-nodegroup","text":"It specifies whether aws instances are part of managed nodeGroups. If instances belong to the managed nodeGroups then provide MANAGED_NODEGROUP as enable else provide it as disable . The default value is disabled . Use the following example to tune this: # it provided as enable if instances are part of self managed groups # it is applicable for [ec2-terminate-by-id, ec2-terminate-by-tag] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # if instance is part of a managed node-group # supports enable and disable values, default value: disable - name : MANAGED_NODEGROUP value : 'enable' # region for the ec2 instance - name : REGION value : '<region for instances>' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Managed Nodegroup"},{"location":"experiments/categories/aws/AWS-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : REGION value : '<region for instances>' - name : INSTANCE_TAG value : 'key:value'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws/ebs-loss-by-id/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_ID Comma separated list of volume IDs subjected to ebs detach chaos Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Detach Volumes By ID \u00b6 It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By ID"},{"location":"experiments/categories/aws/ebs-loss-by-id/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume id. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-id/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-id-sa namespace : default labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-id-sa labels : name : ebs-loss-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-id-sa subjects : - kind : ServiceAccount name : ebs-loss-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-id/#detach-volumes-by-id","text":"It contains comma separated list of volume IDs subjected to ebs detach chaos. It can be tuned via EBS_VOLUME_ID ENV. Use the following example to tune this: # contains ebs volume id apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-id-sa experiments : - name : ebs-loss-by-id spec : components : env : # id of the ebs volume - name : EBS_VOLUME_ID value : 'ebs-vol-1' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By ID"},{"location":"experiments/categories/aws/ebs-loss-by-tag/","text":"Introduction \u00b6 It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EBS volume is attached to the instance. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EBS_VOLUME_TAG Provide the common tag for target volumes. It'll be in form of key:value (Ex: 'team:devops') Optional Fields Variables Description Notes VOLUME_AFFECTED_PERC The Percentage of total ebs volumes to target Defaults to 0 (corresponds to 1 volume), provide numeric value only TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The time duration between the attachment and detachment of the volumes (sec) Defaults to 30s REGION The region name for the target volumes SEQUENCE It defines sequence of chaos execution for multiple volumes Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single volume \u00b6 It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of volumes \u00b6 It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EBS Loss By Tag"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#introduction","text":"It causes chaos to disrupt state of ebs volume by detaching it from the node/ec2 instance for a certain chaos duration using volume tags. In case of EBS persistent volumes, the volumes can get self-attached and experiment skips the re-attachment step. Tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod. Scenario: Detach EBS Volume","title":"Introduction"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ebs-loss-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to attach or detach an ebs volume for the instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#default-validations","text":"View the default validations EBS volume is attached to the instance.","title":"Default Validations"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ebs-loss-by-tag-sa namespace : default labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ebs-loss-by-tag-sa labels : name : ebs-loss-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ebs-loss-by-tag-sa subjects : - kind : ServiceAccount name : ebs-loss-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-single-volume","text":"It will detach a random single ebs volume with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # contains the tags for the ebs volumes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single volume"},{"location":"experiments/categories/aws/ebs-loss-by-tag/#target-percent-of-volumes","text":"It will detach the VOLUME_AFFECTED_PERC percentage of ebs volumes with the given EBS_VOLUME_TAG tag and REGION region. Use the following example to tune this: # target percentage of the ebs volumes with the provided tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ebs-loss-by-tag-sa experiments : - name : ebs-loss-by-tag spec : components : env : # percentage of ebs volumes filter by tag - name : VOLUME_AFFECTED_PERC value : '100' # tag of the ebs volume - name : EBS_VOLUME_TAG value : 'key:value' # region for the ebs volume - name : REGION value : '<region for EBS_VOLUME_TAG>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of volumes"},{"location":"experiments/categories/aws/ec2-terminate-by-id/","text":"Introduction \u00b6 It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Stop Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#introduction","text":"It causes termination of an EC2 instance by instance ID or list of instance IDs before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-id-sa namespace : default labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-id-sa labels : name : ec2-terminate-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-id-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-id/#stop-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: # contains the instance id, to be terminated/stopped apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-id-sa experiments : - name : ec2-terminate-by-id spec : components : env : # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-1' # region for the ec2 instance - name : REGION value : '<region for EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By ID"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/","text":"Introduction \u00b6 It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. WARNING \u00b6 If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes INSTANCE_TAG Instance Tag to filter the target ec2 instance. The INSTANCE_TAG should be provided as key:value ex: team:devops Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination. Defaults to 30s MANAGED_NODEGROUP Set to enable if the target instance is the part of self-managed nodegroups Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS specific tunables \u00b6 Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables. Target single instance \u00b6 It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"EC2 Terminate By Tag"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#introduction","text":"It causes termination of an EC2 instance by tag before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the ec2 instance. When the MANAGED_NODEGROUP is enable then the experiment will not try to start the instance post chaos instead it will check of the addition of the new node instance to the cluster. Scenario: Terminate EC2 Instance","title":"Introduction"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the ec2-terminate-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient AWS access to stop and start an ec2 instance. Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#warning","text":"If the target EC2 instance is a part of a self-managed nodegroup: Make sure to drain the target node if any application is running on it and also ensure to cordon the target node before running the experiment so that the experiment pods do not schedule on it.","title":"WARNING"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : ec2-terminate-by-tag-sa namespace : default labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : ec2-terminate-by-tag-sa labels : name : ec2-terminate-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : ec2-terminate-by-tag-sa subjects : - kind : ServiceAccount name : ec2-terminate-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#common-and-aws-specific-tunables","text":"Refer the common attributes and AWS specific tunable to tune the common tunables for all experiments and aws specific tunables.","title":"Common and AWS specific tunables"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-single-instance","text":"It will stop a random single ec2 instance with the given INSTANCE_TAG tag and the REGION region. Use the following example to tune this: # target the ec2 instances with matching tag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws/ec2-terminate-by-tag/#target-percent-of-instances","text":"It will stop the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given INSTANCE_TAG tag and REGION region. Use the following example to tune this: # percentage of ec2 instances, needs to terminate with provided tags apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : ec2-terminate-by-tag-sa experiments : - name : ec2-terminate-by-tag spec : components : env : # percentage of ec2 instance filterd by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instance - name : INSTANCE_TAG value : 'key:value' # region for the ec2 instance - name : REGION value : '<region for instance>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/","text":"It contains the aws-ssm specific experiment tunables. CPU Cores \u00b6 It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Percentage \u00b6 It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSM Docs \u00b6 It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers Count \u00b6 It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"AWS SSM experiments tunables"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#cpu-cores","text":"It stressed the CPU_CORE cpu cores of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the cpu cores to stress the ec2 instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # cpu cores for the stress - name : CPU_CORE value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"CPU Cores"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#memory-percentage","text":"It stressed the MEMORY_PERCENTAGE percentage of free space of the EC2_INSTANCE_ID ec2 instance and REGION region for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # provide the memory pecentage to stress the instance memory apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # memory percentage for the stress - name : MEMORY_PERCENTAGE value : '80' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Percentage"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#ssm-docs","text":"It contains the details of the SSM docs i.e, name, type, the format of ssm-docs . Use the following example to tune this: ## provide the details of the ssm document details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # name of the ssm docs - name : DOCUMENT_NAME value : 'AWS-SSM-Doc' # format of the ssm docs - name : DOCUMENT_FORMAT value : 'YAML' # type of the ssm docs - name : DOCUMENT_TYPE value : 'command' # path of the ssm docs - name : DOCUMENT_PATH value : '' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSM Docs"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#workers-count","text":"It contains the NUMBER_OF_WORKERS workers for the stress. Use the following example to tune this: # workers details used to stress the instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # number of workers used for stress - name : NUMBER_OF_WORKERS value : '1' # id of the ec2 instance - name : EC2_INSTANCE_ID value : 'instance-01' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers Count"},{"location":"experiments/categories/aws-ssm/AWS-SSM-experiments-tunables/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id specEC2_INSTANCE_ID : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : CPU_CORE value : '1' - name : EC2_INSTANCE_ID value : 'instance-01' - name : REGION value : '<region of the EC2_INSTANCE_ID>'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/","text":"Introduction \u00b6 AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_ID Instance ID of the target ec2 instance. Multiple IDs can also be provided as a comma(,) separated values Multiple IDs can be provided as id1,id2 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Stress Instances By ID \u00b6 It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#introduction","text":"AWS SSM Chaos By ID contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let us perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance ID(s) using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in chaosexperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-id experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-id-sa namespace : default labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-id-sa labels : name : aws-ssm-chaos-by-id-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-id-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-id-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-id/#stress-instances-by-id","text":"It contains comma separated list of instances IDs subjected to ec2 stop chaos. It can be tuned via EC2_INSTANCE_ID ENV. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-id-sa experiments : - name : aws-ssm-chaos-by-id spec : components : env : # comma separated list of ec2 instance id(s) # all instances should belongs to the same region(REGION) - name : EC2_INSTANCE_ID value : 'instance-01,instance-02' # region of the ec2 instance - name : REGION value : '<region of the EC2_INSTANCE_ID>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stress Instances By ID"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/","text":"Introduction \u00b6 AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations EC2 instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes EC2_INSTANCE_TAG Instance Tag to filter the target ec2 instance The EC2_INSTANCE_TAG should be provided as key:value ex: chaos:ssm Optional Fields Variables Description Notes INSTANCE_AFFECTED_PERC The Percentage of total ec2 instance to target Defaults to 0 (corresponds to 1 instance), provide numeric value only TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive chaos injection Defaults to 60s AWS_SHARED_CREDENTIALS_FILE Provide the path for aws secret credentials Defaults to /tmp/cloud_config.yml DOCUMENT_NAME Provide the name of addded ssm docs (if not using the default docs) Default to LitmusChaos-AWS-SSM-Doc DOCUMENT_FORMAT Provide the format of the ssm docs. It can be YAML or JSON Defaults to YAML DOCUMENT_TYPE Provide the document type of added ssm docs (if not using the default docs) Defaults to Command DOCUMENT_PATH Provide the document path if added using configmaps Defaults to the litmus ssm docs path INSTALL_DEPENDENCIES Select to install dependencies used to run stress-ng with default docs. It can be either True or False Defaults to True NUMBER_OF_WORKERS Provide the number of workers to run stress-chaos with default ssm docs Defaults to 1 MEMORY_PERCENTAGE Provide the memory consumption in percentage on the instance for default ssm docs Defaults to 80 CPU_CORE Provide the number of cpu cores to run stress-chaos on EC2 with default ssm docs Defaults to 0. It means it'll consume all the available cpu cores on the instance SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec REGION The region name of the target instace Experiment Examples \u00b6 Common and AWS-SSM specific tunables \u00b6 Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables. Target single instance \u00b6 It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Percent of instances \u00b6 It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"AWS SSM Chaos By Tag"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#introduction","text":"AWS SSM Chaos By Tag contains chaos to disrupt the state of infra resources. The experiment can induce chaos on AWS EC2 instance using Amazon SSM Run Command This is carried out by using SSM Docs that defines the actions performed by Systems Manager on your managed instances (having SSM agent installed) which let you perform chaos experiments on the instances. It causes chaos (like stress, network, disk or IO) on AWS EC2 instances with given instance Tag using SSM docs for a certain chaos duration. For the default execution the experiment uses SSM docs for stress-chaos while you can add your own SSM docs using configMap (.spec.definition.configMaps) in ChaosExperiment CR. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the target application pod(if provided). Scenario: AWS SSM Chaos","title":"Introduction"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the aws-ssm-chaos-by-tag experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have the required AWS access and your target EC2 instances have attached an IAM instance profile. To know more checkout Systems Manager Docs . Ensure to create a Kubernetes secret having the AWS access configuration(key) in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : cloud_config.yml : |- # Add the cloud AWS credentials respectively [default] aws_access_key_id = XXXXXXXXXXXXXXXXXXX aws_secret_access_key = XXXXXXXXXXXXXXX If you change the secret key name (from cloud_config.yml ) please also update the AWS_SHARED_CREDENTIALS_FILE ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#default-validations","text":"View the default validations EC2 instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : aws-ssm-chaos-by-tag-sa namespace : default labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" , \"configmaps\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : aws-ssm-chaos-by-tag-sa labels : name : aws-ssm-chaos-by-tag-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : aws-ssm-chaos-by-tag-sa subjects : - kind : ServiceAccount name : aws-ssm-chaos-by-tag-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#common-and-aws-ssm-specific-tunables","text":"Refer the common attributes and AWS-SSM specific tunable to tune the common tunables for all experiments and aws-ssm specific tunables.","title":"Common and AWS-SSM specific tunables"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-single-instance","text":"It will stress a random single ec2 instance with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target single instance"},{"location":"experiments/categories/aws-ssm/aws-ssm-chaos-by-tag/#target-percent-of-instances","text":"It will stress the INSTANCE_AFFECTED_PERC percentage of ec2 instances with the given EC2_INSTANCE_TAG tag and REGION region. Use the following example to tune this: apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : aws-ssm-chaos-by-tag-sa experiments : - name : aws-ssm-chaos-by-tag spec : components : env : # percentage of the ec2 instances filtered by tags - name : INSTANCE_AFFECTED_PERC value : '100' # tag of the ec2 instances - name : EC2_INSTANCE_TAG value : 'key:value' # region of the ec2 instance - name : REGION value : '<region of the ec2 instances>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Percent of instances"},{"location":"experiments/categories/azure/azure-instance-stop/","text":"Introduction \u00b6 It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name. Default Validations \u00b6 View the default validations Azure instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes AZURE_INSTANCE_NAME Instance name of the target azure instance RESOURCE_GROUP The resource group of the target instance Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance poweroff. Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop Instances By Name \u00b6 It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azore instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Azure Instance Stop"},{"location":"experiments/categories/azure/azure-instance-stop/#introduction","text":"It causes PowerOff an Azure instance before bringing it back to running state after the specified chaos duration. It helps to check the performance of the application/process running on the instance. Scenario: Stop the azure instance","title":"Introduction"},{"location":"experiments/categories/azure/azure-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/azure/azure-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the azure-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Azure access to stop and start the an instance. We will use azure file-based authentication to connect with the instance using azure GO SDK in the experiment. For generating auth file run az ad sp create-for-rbac --sdk-auth > azure.auth Azure CLI command. Ensure to create a Kubernetes secret having the auth file created in the step in CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : azure.auth : |- { \"clientId\": \"XXXXXXXXX\", \"clientSecret\": \"XXXXXXXXX\", \"subscriptionId\": \"XXXXXXXXX\", \"tenantId\": \"XXXXXXXXX\", \"activeDirectoryEndpointUrl\": \"XXXXXXXXX\", \"resourceManagerEndpointUrl\": \"XXXXXXXXX\", \"activeDirectoryGraphResourceId\": \"XXXXXXXXX\", \"sqlManagementEndpointUrl\": \"XXXXXXXXX\", \"galleryEndpointUrl\": \"XXXXXXXXX\", \"managementEndpointUrl\": \"XXXXXXXXX\" } If you change the secret key name (from azure.auth ) please also update the AZURE_AUTH_LOCATION ENV value on experiment.yaml with the same name.","title":"Prerequisites"},{"location":"experiments/categories/azure/azure-instance-stop/#default-validations","text":"View the default validations Azure instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/azure/azure-instance-stop/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : azure-instance-stop-sa namespace : default labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : azure-instance-stop-sa labels : name : azure-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : azure-instance-stop-sa subjects : - kind : ServiceAccount name : azure-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/azure/azure-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/azure/azure-instance-stop/#stop-instances-by-name","text":"It contains comma separated list of instance names subjected to instance stop chaos. It can be tuned via AZURE_INSTANCE_NAME ENV. Use the following example to tune this: ## contains the azure instance details apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # comma separated list of azore instance names - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' # name of the resource group - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop Instances By Name"},{"location":"experiments/categories/azure/azure-instance-stop/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : azure-instance-stop-sa experiments : - name : azure-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '10' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : AZURE_INSTANCE_NAME value : 'instance-01,instance-02' - name : RESOURCE_GROUP value : '<resource group of AZURE_INSTANCE_NAME>'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CASSANDRA_SVC_NAME Cassandra Service Name Defaults value: cassandra KEYSPACE_REPLICATION_FACTOR Value of the Replication factor for the cassandra liveness deploy It needs to create keyspace while checking the livenss of cassandra CASSANDRA_PORT Port of the cassandra statefulset Defaults value: 9042 CASSANDRA_LIVENESS_CHECK It allows to check the liveness of the cassandra statefulset It can be enabled or disabled CASSANDRA_LIVENESS_IMAGE Image of the cassandra liveness deployment Default value: litmuschaos/cassandra-client:latest SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0% (corresponds to 1 replica) CHAOS_INTERVAL Time interval b/w two successive pod failures (sec) Defaults to 5s LIB The chaos lib used to inject the chaos Defaults to litmus . Supported litmus only FORCE Application Pod deletion mode. False indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Cassandra App Details \u00b6 It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Force Delete \u00b6 The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of cassandra \u00b6 The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra Pod Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#introduction","text":"It causes (forced/graceful) pod failure of specific/random replicas of an cassandra statefulset It tests cassandra sanity (replica availability & uninterrupted service) and recovery workflow of the cassandra statefulset. Scenario: Deletes cassandra pod","title":"Introduction"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the cassandra-pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#default-validations","text":"View the default validations Cassandra pods are healthy before chaos injection The load should be distributed on the each replicas.","title":"Default Validations"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#cassandra-app-details","text":"It tunes the cassandra service name at CASSANDRA_SVC_NAME and cassandra port at CASSANDRA_PORT . Use the following example to tune this: ## contains details of cassandra application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # name of the cassandra service - name : CASSANDRA_SVC_NAME value : 'cassandra' # name of the cassandra port - name : CASSANDRA_PORT value : '9042' # percentage of cassandra replicas with matching labels - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Cassandra App Details"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#force-delete","text":"The cassandra pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: ## force env provided to forcefully or gracefully delete the pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # deletes the cassandra pod forcefully or gracefully # supports: true, false. default: false - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Force Delete"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#liveness-check-of-cassandra","text":"The cassandra liveness can be tuned with CASSANDRA_LIVENESS_CHECK env. Provide CASSANDRA_LIVENESS_CHECK as enabled to enable the liveness check and provide CASSANDRA_LIVENESS_CHECK as disabled to skip the liveness check. The default value is disabled. The cassandra liveness image can be provided at CASSANDRA_LIVENESS_IMAGE . The cassandra liveness pod performs the CRUD operations to verify the liveness of cassandra. It creates the keyspace with KEYSPACE_REPLICATION_FACTOR keyspace factor. Use the following example to tune this: ## enable the cassandra liveness check, while injecting chaos ## it continuosly performs cassandra database operations(with cqlsh command) to vefify the liveness status apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # checks the liveness of cassandra while injecting chaos # supports: enabled, disabled. default: disabled - name : CASSANDRA_LIVENESS_CHECK value : 'enabled' # image of the cassandra liveness deployment - name : CASSANDRA_LIVENESS_IMAGE value : 'litmuschaos/cassandra-client:latest' # keyspace replication factor, needed for liveness check - name : KEYSPACE_REPLICATION_FACTOR value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of cassandra"},{"location":"experiments/categories/cassandra/cassandra-pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"cassandra\" applabel : \"app=cassandra\" appkind : \"statefulset\" chaosServiceAccount : casssandra-pod-delete-sa experiments : - name : casssandra-pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/","text":"It contains tunables, which are common for all the experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Duration of the chaos \u00b6 It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Ramp Time \u00b6 It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60' Sequence of chaos execution \u00b6 It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: - parallel : The chaos is injected in all the targets at once. - serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60' Name of chaos library \u00b6 It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60' Instance ID \u00b6 It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60' Image used by the helper pod \u00b6 It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for all experiments"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#duration-of-the-chaos","text":"It defines the total time duration of the chaos injection. It can be tuned with the TOTAL_CHAOS_DURATION ENV. It is provided in a unit of seconds. Use the following example to tune this: # define the total chaos duration apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Duration of the chaos"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#ramp-time","text":"It defines the period to wait before and after the injection of chaos. It can be tuned with the RAMP_TIME ENV. It is provided in a unit of seconds. Use the following example to tune this: # waits for the ramp time before and after injection of chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # waits for the time interval before and after injection of chaos - name : RAMP_TIME value : '10' # in seconds - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Ramp Time"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#sequence-of-chaos-execution","text":"It defines the sequence of the chaos execution in the case of multiple targets. It can be tuned with the SEQUENCE ENV. It supports the following modes: - parallel : The chaos is injected in all the targets at once. - serial : The chaos is injected in all the targets one by one. The default value of SEQUENCE is parallel . Use the following example to tune this: # define the order of execution of chaos in case of multiple targets apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # define the sequence of execution of chaos in case of mutiple targets # supports: serial, parallel. default: parallel - name : SEQUENCE value : 'parallel' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Sequence of chaos execution"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#name-of-chaos-library","text":"It defines the name of the chaos library used for the chaos injection. It can be tuned with the LIB ENV. Use the following example to tune this: # lib for the chaos injection apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # defines the name of the chaoslib used for the experiment - name : LIB value : 'litmus' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Name of chaos library"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#instance-id","text":"It defines a user-defined string that holds metadata/info about the current run/instance of chaos. Ex: 04-05-2020-9-00. This string is appended as a suffix in the chaosresult CR name. It can be tuned with INSTANCE_ID ENV. Use the following example to tune this: # provide to append user-defined suffix in the end of chaosresult name apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # user-defined string appended as suffix in the chaosresult name - name : INSTANCE_ID value : '123' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Instance ID"},{"location":"experiments/categories/common/common-tunables-for-all-experiments/#image-used-by-the-helper-pod","text":"It defines the image, which is used to launch the helper pod, if applicable. It can be tuned with the LIB_IMAGE ENV. It is supported by [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, kubelet-service-kill, docker-service-kill, node-restart] experiments. Use the following example to tune this: # it contains the lib image used for the helper pod # it support [container-kill, network-experiments, stress-experiments, dns-experiments, disk-fill, # kubelet-service-kill, docker-service-kill, node-restart] experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # nane of the lib image - name : LIB_IMAGE value : 'litmuschaos/go-runner:latest' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Image used by the helper pod"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/","text":"Introduction \u00b6 It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations Disk volumes are attached to their respective instances Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID The ID of the GCP Project of which the disk volumes are a part of All the target disk volumes should belong to a single GCP Project DISK_VOLUME_NAMES Target non-boot persistent disk volume names Multiple disk volume names can be provided as disk1,disk2,... DISK_ZONES The zones of respective target disk volumes Provide the zone for every target disk name as zone1,zone2... in the respective order of DISK_VOLUME_NAMES DEVICE_NAMES The device names of respective target disk volumes Provide the device name for every target disk name as deviceName1,deviceName2... in the respective order of DISK_VOLUME_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between the successive chaos iterations (sec) Defaults to 30s SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Detach Volumes By Names \u00b6 It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Disk Loss"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#introduction","text":"It causes chaos to disrupt state of GCP persistent disk volume by detaching it from its VM instance for a certain chaos duration using the disk name. Scenario: detach the gcp disk","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-disk-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that your service account has an editor access or owner access for the GCP project. Ensure the target disk volume to be detached should not be the root volume its instance. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#default-validations","text":"View the default validations Disk volumes are attached to their respective instances","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-disk-loss-sa namespace : default labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-disk-loss-sa labels : name : gcp-vm-disk-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-disk-loss-sa subjects : - kind : ServiceAccount name : gcp-vm-disk-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#detach-volumes-by-names","text":"It contains comma separated list of volume names subjected to disk loss chaos. It will detach all the disks with the given DISK_VOLUME_NAMES disk names and corresponding DISK_ZONES zone names and the DEVICE_NAMES device names in GCP_PROJECT_ID project. It reattached the volume after waiting for the specified TOTAL_CHAOS_DURATION duration. NOTE: The DISK_VOLUME_NAMES contains multiple comma-separated disk names. The comma-separated zone names should be provided in the same order as disk names. Use the following example to tune this: ## details of the gcp disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # comma separated list of disk volume names - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' # comma separated list of zone names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DISK_ZONES value : 'zone-01,zone-02' # comma separated list of device names corresponds to the DISK_VOLUME_NAMES # it should be provided in same order of DISK_VOLUME_NAMES - name : DEVICE_NAMES value : 'device-01,device-02' # gcp project id to which disk volume belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Detach Volumes By Names"},{"location":"experiments/categories/gcp/gcp-vm-disk-loss/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-disk-loss-sa experiments : - name : gcp-vm-disk-loss spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : DISK_VOLUME_NAMES value : 'disk-01,disk-02' - name : DISK_ZONES value : 'zone-01,zone-02' - name : DEVICE_NAMES value : 'device-01,device-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/","text":"Introduction \u00b6 It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url : Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes GCP_PROJECT_ID GCP project ID to which the VM instances belong All the VM instances must belong to a single GCP project VM_INSTANCE_NAMES Name of target VM instances Multiple instance names can be provided as instance1,instance2,... INSTANCE_ZONES The zones of the target VM instances Zone for every instance name has to be provided as zone1,zone2,... in the same order of VM_INSTANCE_NAMES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s CHAOS_INTERVAL The interval (in sec) between successive instance termination Defaults to 30s AUTO_SCALING_GROUP Set to enable if the target instance is the part of a auto-scaling group Defaults to disable SEQUENCE It defines sequence of chaos execution for multiple instance Default value: parallel. Supported: serial, parallel RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Target GCP Instances \u00b6 It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Autoscaling NodeGroup \u00b6 If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"GCP Instance Stop"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#introduction","text":"It causes power-off of a GCP VM instance by instance name or list of instance names before bringing it back to the running state after the specified chaos duration. It helps to check the performance of the application/process running on the VM instance. When the AUTO_SCALING_GROUP is enable then the experiment will not try to start the instance post chaos, instead it will check the addition of the new node instances to the cluster. Scenario: stop the gcp vm","title":"Introduction"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the gcp-vm-instance-stop experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient GCP permissions to stop and start the GCP VM instances. Ensure to create a Kubernetes secret having the GCP service account credentials in the default namespace. A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : cloud-secret type : Opaque stringData : type : project_id : private_key_id : private_key : client_email : client_id : auth_uri : token_uri : auth_provider_x509_cert_url : client_x509_cert_url :","title":"Prerequisites"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : gcp-vm-instance-stop-sa namespace : default labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : gcp-vm-instance-stop-sa labels : name : gcp-vm-instance-stop-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : gcp-vm-instance-stop-sa subjects : - kind : ServiceAccount name : gcp-vm-instance-stop-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#target-gcp-instances","text":"It will stop all the instances with the given VM_INSTANCE_NAMES instance names and corresponding INSTANCE_ZONES zone names in GCP_PROJECT_ID project. NOTE: The VM_INSTANCE_NAMES contains multiple comma-separated vm instances. The comma-separated zone names should be provided in the same order as instance names. Use the following example to tune this: ## details of the gcp instance apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target GCP Instances"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#autoscaling-nodegroup","text":"If vm instances belong to the autoscaling group then provide the AUTO_SCALING_GROUP as enable else provided it as disable . The default value of AUTO_SCALING_GROUP is disable . Use the following example to tune this: ## scale up and down to maintain the available instance counts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # tells if instances are part of autoscaling group # supports: enable, disable. default: disable - name : AUTO_SCALING_GROUP value : 'enable' # comma separated list of vm instance names - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' # comma separated list of zone names corresponds to the VM_INSTANCE_NAMES # it should be provided in same order of VM_INSTANCE_NAMES - name : INSTANCE_ZONES value : 'zone-01,zone-02' # gcp project id to which vm instance belongs - name : GCP_PROJECT_ID value : 'project-id' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Autoscaling NodeGroup"},{"location":"experiments/categories/gcp/gcp-vm-instance-stop/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : gcp-vm-instance-stop-sa experiments : - name : gcp-vm-instance-stop spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : VM_INSTANCE_NAMES value : 'instance-01,instance-02' - name : INSTANCE_ZONES value : 'zone-01,zone-02' - name : GCP_PROJECT_ID value : 'project-id'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/","text":"Introduction \u00b6 It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored. Default Validations \u00b6 View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes KAFKA_NAMESPACE Namespace of Kafka Brokers May be same as value for spec.appinfo.appns KAFKA_LABEL Unique label of Kafka Brokers May be same as value for spec.appinfo.applabel KAFKA_SERVICE Headless service of the Kafka Statefulset KAFKA_PORT Port of the Kafka ClusterIP service ZOOKEEPER_NAMESPACE Namespace of Zookeeper Cluster May be same as value for KAFKA_NAMESPACE or other ZOOKEEPER_LABEL Unique label of Zokeeper statefulset ZOOKEEPER_SERVICE Headless service of the Zookeeper Statefulset ZOOKEEPER_PORT Port of the Zookeeper ClusterIP service Optional Fields Variables Description Notes KAFKA_BROKER Kafka broker pod (name) to be deleted A target selection mode (random/liveness-based/specific) KAFKA_KIND Kafka deployment type Same as spec.appinfo.appkind . Supported: statefulset KAFKA_LIVENESS_STREAM Kafka liveness message stream Supported: enabled , disabled KAFKA_LIVENESS_IMAGE Image used for liveness message stream Set the liveness image as <registry_url>/<repository>:<image-tag> KAFKA_REPLICATION_FACTOR Number of partition replicas for liveness topic partition Necessary if KAFKA_LIVENESS_STREAM is enabled . The replication factor should be less than or equal to number of Kafka brokers KAFKA_INSTANCE_NAME Name of the Kafka chroot path on zookeeper Necessary if installation involves use of such path KAFKA_CONSUMER_TIMEOUT Kafka consumer message timeout, post which it terminates Defaults to 30000ms, Recommended timeout for EKS platform: 60000 ms TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 15s CHAOS_INTERVAL Time interval b/w two successive broker failures (sec) Defaults to 5s Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Kafka And Zookeeper App Details \u00b6 It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60' Liveness check of kafka \u00b6 The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mutiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Kafka Broker Pod Failure"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#introduction","text":"It causes (forced/graceful) pod failure of specific/random Kafka broker pods It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the Kafka cluster It tests unbroken message stream when KAFKA_LIVENESS_STREAM experiment environment variable is set to enabled Scenario: Deletes kafka broker pod","title":"Introduction"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kafka-broker-pod-failure experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that Kafka & Zookeeper are deployed as Statefulsets If Confluent/Kudo Operators have been used to deploy Kafka, note the instance name, which will be used as the value of KAFKA_INSTANCE_NAME experiment environment variable In case of Confluent, specified by the --name flag In case of Kudo, specified by the --instance flag Zookeeper uses this to construct a path in which kafka cluster data is stored.","title":"Prerequisites"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#default-validations","text":"View the default validations Kafka Cluster (comprising the Kafka-broker & Zookeeper Statefulsets) is healthy Kafka Message stream (if enabled) is unbroken","title":"Default Validations"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kafka-broker-pod-failure-sa namespace : default labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kafka-broker-pod-failure-sa labels : name : kafka-broker-pod-failure-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kafka-broker-pod-failure-sa subjects : - kind : ServiceAccount name : kafka-broker-pod-failure-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#kafka-and-zookeeper-app-details","text":"It contains kafka and zookeeper application details: KAFKA_NAMESPACE : Namespace where kafka is installed KAFKA_LABEL : Labels of the kafka application KAFKA_SERVICE : Name of the kafka service KAFKA_PORT : Port of the kafka service ZOOKEEPER_NAMESPACE : Namespace where zookeeper is installed ZOOKEEPER_LABEL : Labels of the zookeeper application ZOOKEEPER_SERVICE : Name of the zookeeper service ZOOKEEPER_PORT : Port of the zookeeper service KAFKA_BROKER : Name of the kafka broker pod KAFKA_REPLICATION_FACTOR : Replication factor of the kafka application Use the following example to tune this: ## details of the kafka and zookeeper apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # namespace where kafka installed - name : KAFKA_NAMESPACE value : 'kafka' # labels of the kafka - name : KAFKA_LABEL value : 'app=cp-kafka' # name of the kafka service - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' # kafka port number - name : KAFKA_PORT value : '9092' # namespace of the zookeeper - name : ZOOKEEPER_NAMESPACE value : 'default' # labels of the zookeeper - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' # name of the zookeeper service - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' # port of the zookeeper service - name : ZOOKEEPER_PORT value : '2181' # name of the kafka broker - name : KAFKA_BROKER value : 'kafka-0' # kafka replication factor - name : KAFKA_REPLICATION_FACTOR value : '3' # duration of the chaos - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kafka And Zookeeper App Details"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#liveness-check-of-kafka","text":"The kafka liveness can be tuned with KAFKA_LIVENESS_STREAM env. Provide KAFKA_LIVENESS_STREAM as enable to enable the liveness check and provide KAFKA_LIVENESS_STREAM as disable to skip the liveness check. The default value is disable . The Kafka liveness image can be provided at KAFKA_LIVENESS_IMAGE . The kafka liveness pod contains producer and consumer to validate the message stream during the chaos. The timeout for the consumer can be tuned with KAFKA_CONSUMER_TIMEOUT . Use the following example to tune this: ## checks the kafka message liveness while injecting chaos ## sets the consumer timeout apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # check for the kafa liveness message stream during chaos # supports: enable, disable. default value: disable - name : KAFKA_LIVENESS_STREAM value : 'enable' # timeout of the kafka consumer - name : KAFKA_CONSUMER_TIMEOUT value : '30000' # in ms # image of the kafka liveness pod - name : KAFKA_LIVENESS_IMAGE value : '' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Liveness check of kafka"},{"location":"experiments/categories/kafka/kafka-broker-pod-failure/#mutiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"kafka\" applabel : \"app=cp-kafka\" appkind : \"statefulset\" chaosServiceAccount : kafka-broker-pod-failure-sa experiments : - name : kafka-broker-pod-failure spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' - name : KAFKA_NAMESPACE value : 'kafka' - name : KAFKA_LABEL value : 'app=cp-kafka' - name : KAFKA_SERVICE value : 'kafka-cp-kafka-headless' - name : KAFKA_PORT value : '9092' - name : ZOOKEEPER_NAMESPACE value : 'default' - name : ZOOKEEPER_LABEL value : 'app=cp-zookeeper' - name : ZOOKEEPER_SERVICE value : 'kafka-cp-zookeeper-headless' - name : ZOOKEEPER_PORT value : '2181'","title":"Mutiple Iterations Of Chaos"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/","text":"It contains tunables, which are common for all the node experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Single Node \u00b6 It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Multiple Nodes \u00b6 It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Nodes With Labels \u00b6 It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60' Node Affected Percentage \u00b6 It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for node experiments"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-single-node","text":"It defines the name of the target node subjected to chaos. The target node can be tuned via TARGET_NODE ENV. It contains only a single node name. NOTE : It is supported by [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] experiments. Use the following example to tune this: ## provide the target node name ## it is applicable for the [node-drain, node-taint, node-restart, kubelet-service-kill, docker-service-kill] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Single Node"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-multiple-nodes","text":"It defines the comma-separated name of the target nodes subjected to chaos. The target nodes can be tuned via TARGET_NODES ENV. NOTE : It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments Use the following example to tune this: ## provide the comma separated target node names ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # comma separated target node names - name : TARGET_NODES value : 'node01,node02' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Multiple Nodes"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#target-nodes-with-labels","text":"It defines the labels of the targeted node(s) subjected to chaos. The node labels can be tuned via NODE_LABEL ENV. It is mutually exclusive with the TARGET_NODE(S) ENV. If TARGET_NODE(S) ENV is set then it will use the nodes provided inside it otherwise, it will derive the node name(s) with matching node labels. Use the following example to tune this: ## provide the labels of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Nodes With Labels"},{"location":"experiments/categories/nodes/common-tunables-for-node-experiments/#node-affected-percentage","text":"It defines the percentage of nodes subjected to chaos with matching node labels. It can be tuned with NODES_AFFECTED_PERC ENV. If NODES_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one node. It is supported by [node-cpu-hog, node-memory-hog, node-io-stress] experiments. The rest of the experiment selects only a single node for the chaos. Use the following example to tune this: ## provide the percentage of nodes to be targeted with matching labels ## it is applicable for the [node-cpu-hog, node-memory-hog, node-io-stress] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # percentage of nodes to be targeted with matching node labels - name : NODES_AFFECTED_PERC value : '100' # labels of the targeted node # it will derive the target nodes if TARGET_NODE(S) ENV is not set - name : NODE_LABEL value : 'key=value' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Affected Percentage"},{"location":"experiments/categories/nodes/docker-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Docker Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Docker Service Kill"},{"location":"experiments/categories/nodes/docker-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to docker service kill The docker service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to docker service down. Scenario: Kill the docker service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/docker-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/docker-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the docker-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which docker service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/docker-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/docker-service-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : docker-service-kill-sa namespace : default labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" , \"litmuschaos.io\" , \"batch\" , \"apps\" ] resources : [ \"pods\" , \"jobs\" , \"pods/log\" , \"events\" , \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : docker-service-kill-sa labels : name : docker-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : docker-service-kill-sa subjects : - kind : ServiceAccount name : docker-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/docker-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/docker-service-kill/#kill-docker-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the docker service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : docker-service-kill-sa experiments : - name : docker-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Docker Service"},{"location":"experiments/categories/nodes/kubelet-service-kill/","text":"Introduction \u00b6 This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the target node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE The lib image used to inject kubelet kill chaos the image should have systemd installed in it. Defaults to ubuntu:16.04 RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Kill Kubelet Service \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kubelet Service Kill"},{"location":"experiments/categories/nodes/kubelet-service-kill/#introduction","text":"This experiment Causes the application to become unreachable on account of node turning unschedulable (NotReady) due to kubelet service kill. The kubelet service has been stopped/killed on a node to make it unschedulable for a certain duration i.e TOTAL_CHAOS_DURATION. The application node should be healthy after the chaos injection and the services should be reaccessable. The application implies services. Can be reframed as: Test application resiliency upon replica getting unreachable caused due to kubelet service down. Scenario: Kill the kubelet service of the node","title":"Introduction"},{"location":"experiments/categories/nodes/kubelet-service-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/kubelet-service-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the kubelet-service-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node for which kubelet service need to be killed) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/kubelet-service-kill/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/kubelet-service-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : kubelet-service-kill-sa namespace : default labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : kubelet-service-kill-sa labels : name : kubelet-service-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : kubelet-service-kill-sa subjects : - kind : ServiceAccount name : kubelet-service-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/kubelet-service-kill/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/kubelet-service-kill/#kill-kubelet-service","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # kill the kubelet service of the target node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : kubelet-service-kill-sa experiments : - name : kubelet-service-kill spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Kubelet Service"},{"location":"experiments/categories/nodes/node-cpu-hog/","text":"Introduction \u00b6 This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node cpu hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus LIB_IMAGE Image used to run the stress command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before & after injection of chaos in sec Optional NODE_CPU_CORE Number of cores of node CPU to be consumed Defaults to 2 NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Node CPU Cores \u00b6 It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Hog"},{"location":"experiments/categories/nodes/node-cpu-hog/#introduction","text":"This experiment causes CPU resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of CPU resources. The CPU chaos is injected using a helper pod running the linux stress tool (a workload generator). The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of CPU resources Scenario: Stress the CPU of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-cpu-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-cpu-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-cpu-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-cpu-hog-sa namespace : default labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-cpu-hog-sa labels : name : node-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-cpu-hog-sa subjects : - kind : ServiceAccount name : node-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-cpu-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-cpu-hog/#node-cpu-cores","text":"It contains number of cores of node CPU to be consumed. It can be tuned via NODE_CPU_CORE ENV. Use the following example to tune this: # stress the cpu of the targeted nodes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-cpu-hog-sa experiments : - name : node-cpu-hog spec : components : env : # number of cpu cores to be stressed - name : NODE_CPU_CORE value : '2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node CPU Cores"},{"location":"experiments/categories/nodes/node-drain/","text":"Introduction \u00b6 It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Drain Node \u00b6 It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Drain"},{"location":"experiments/categories/nodes/node-drain/#introduction","text":"It drain the node. The resources which are running on the target node should be reschedule on the other nodes. Scenario: Drain the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-drain/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-drain/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-drain experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be drained) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-drain/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-drain/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-drain-sa namespace : default labels : name : node-drain-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-drain-sa labels : name : node-drain-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-drain-sa subjects : - kind : ServiceAccount name : node-drain-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-drain/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-drain/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-drain/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-drain/#drain-node","text":"It contains name of target node subjected to the chaos. It can be tuned via TARGET_NODE ENV. Use the following example to tune this: # drain the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-drain-sa experiments : - name : node-drain spec : components : env : # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Drain Node"},{"location":"experiments/categories/nodes/node-io-stress/","text":"Introduction \u00b6 This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node io stress chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120 FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. CPU Number of core of CPU to be used Default to 1 NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 VM_WORKERS It is the number vm workers involved in IO disk stress Default to 1 LIB The chaos lib used to inject the chaos Default to litmus LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec NODES_AFFECTED_PERC The Percentage of total nodes to target Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Limit CPU Utilization \u00b6 The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this: Workers For Stress \u00b6 The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node IO Stress"},{"location":"experiments/categories/nodes/node-io-stress/#introduction","text":"This experiment causes io stress on the Kubernetes node. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes. The amount of io stress can be either specifed as the size in percentage of the total free space on the file system or simply in Gigabytes(GB). When provided both it will execute with the utilization percentage specified and non of them are provided it will execute with default value of 10%. It tests application resiliency upon replica evictions caused due IO stress on the available Disk space. Scenario: Stress the IO of Node","title":"Introduction"},{"location":"experiments/categories/nodes/node-io-stress/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-io-stress/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-io-stress/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-io-stress-sa namespace : default labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-io-stress-sa labels : name : node-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-io-stress-sa subjects : - kind : ServiceAccount name : node-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-io-stress/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the node. Use the following example to tune this: # stress the i/o of the targeted node with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # percentage of total free space of file system - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/nodes/node-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted node. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted node with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # file system to be stress in GB - name : FILESYSTEM_UTILIZATION_BYTES value : '500' # in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/nodes/node-io-stress/#limit-cpu-utilization","text":"The CPU usage can be limit to CPU cpu while performing io stress. It can be tuned via CPU ENV. Use the following example to tune this:","title":"Limit CPU Utilization"},{"location":"experiments/categories/nodes/node-io-stress/#workers-for-stress","text":"The i/o and VM workers count for the stress can be tuned with NUMBER_OF_WORKERS and VM_WORKERS ENV respectively. Use the following example to tune this: # define the workers count for the i/o and vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-io-stress-sa experiments : - name : node-io-stress spec : components : env : # total number of io workers involved in stress - name : NUMBER_OF_WORKERS value : '4' # total number of vm workers involved in stress - name : VM_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-memory-hog/","text":"Introduction \u00b6 This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODES Comma separated list of nodes, subjected to node memory hog chaos NODE_LABEL It contains node label, which will be used to filter the target nodes if TARGET_NODES ENV is not set It is mutually exclusive with the TARGET_NODES ENV. If both are provided then it will use the TARGET_NODES Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in seconds) Optional Defaults to 120 LIB The chaos lib used to inject the chaos Optional Defaults to litmus LIB_IMAGE Image used to run the stress command Optional Defaults to litmuschaos/go-runner:latest MEMORY_CONSUMPTION_PERCENTAGE Percent of the total node memory capacity Optional Defaults to 30 MEMORY_CONSUMPTION_MEBIBYTES The size in Mebibytes of total available memory. When using this we need to keep MEMORY_CONSUMPTION_PERCENTAGE empty as the percentage have more precedence Optional NUMBER_OF_WORKERS It is the number of VM workers involved in IO disk stress Optional Default to 1 RAMP_TIME Period to wait before and after injection of chaos in sec Optional NODES_AFFECTED_PERC The Percentage of total nodes to target Optional Defaults to 0 (corresponds to 1 node), provide numeric value only SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Memory Consumption Percentage \u00b6 It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Memory Consumption Mebibytes \u00b6 It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Memory Hog"},{"location":"experiments/categories/nodes/node-memory-hog/#introduction","text":"This experiment causes Memory resource exhaustion on the Kubernetes node. The experiment aims to verify resiliency of applications whose replicas may be evicted on account on nodes turning unschedulable (Not Ready) due to lack of Memory resources. The Memory chaos is injected using a helper pod running the linux stress-ng tool (a workload generator)- The chaos is effected for a period equalling the TOTAL_CHAOS_DURATION and upto MEMORY_CONSUMPTION_PERCENTAGE(out of 100) or MEMORY_CONSUMPTION_MEBIBYTES(in Mebibytes out of total available memory). Application implies services. Can be reframed as: Tests application resiliency upon replica evictions caused due to lack of Memory resources Scenario: Stress the memory of node","title":"Introduction"},{"location":"experiments/categories/nodes/node-memory-hog/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-memory-hog/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-memory-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-memory-hog-sa namespace : default labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-memory-hog-sa labels : name : node-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-memory-hog-sa subjects : - kind : ServiceAccount name : node-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-memory-hog/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-percentage","text":"It stresses the MEMORY_CONSUMPTION_PERCENTAGE percentage of total node capacity of the targeted node. Use the following example to tune this: # stress the memory of the targeted node with MEMORY_CONSUMPTION_PERCENTAGE of node capacity # it is mutually exclusive with the MEMORY_CONSUMPTION_MEBIBYTES. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # percentage of total node capacity to be stressed - name : MEMORY_CONSUMPTION_PERCENTAGE value : '10' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Percentage"},{"location":"experiments/categories/nodes/node-memory-hog/#memory-consumption-mebibytes","text":"It stresses the MEMORY_CONSUMPTION_MEBIBYTES MiBi of the memory of the targeted node. It is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE ENV. If MEMORY_CONSUMPTION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on MEMORY_CONSUMPTION_MEBIBYTES ENV. Use the following example to tune this: # stress the memory of the targeted node with given MEMORY_CONSUMPTION_MEBIBYTES # it is mutually exclusive with the MEMORY_CONSUMPTION_PERCENTAGE. # if both are provided then it will use MEMORY_CONSUMPTION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # node memory to be stressed - name : MEMORY_CONSUMPTION_MEBIBYTES value : '500' # in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Memory Consumption Mebibytes"},{"location":"experiments/categories/nodes/node-memory-hog/#workers-for-stress","text":"The workers count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # provide for the workers count for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-memory-hog-sa experiments : - name : node-memory-hog spec : components : env : # total number of workers involved in stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/nodes/node-restart/","text":"Introduction \u00b6 It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment. Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of target node, subjected to chaos. If not provided it will select the random node NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE Optional Fields Variables Description Notes LIB_IMAGE The image used to restart the node Defaults to litmuschaos/go-runner:latest SSH_USER name of ssh user Defaults to root TARGET_NODE_IP Internal IP of the target node, subjected to chaos. If not provided, the experiment will lookup the node IP of the TARGET_NODE node Defaults to empty REBOOT_COMMAND Command used for reboot Defaults to sudo systemctl reboot TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Reboot Command \u00b6 It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' SSH User \u00b6 It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Node Internal IP \u00b6 It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Restart"},{"location":"experiments/categories/nodes/node-restart/#introduction","text":"It causes chaos to disrupt state of node by restarting it. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflows of the application pod Scenario: Restart the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-restart/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-restart/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-restart experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Create a Kubernetes secret named id-rsa where the experiment will run, where its contents will be the private SSH key for SSH_USER used to connect to the node that hosts the target pod in the secret field ssh-privatekey . A sample secret is shown below: apiVersion : v1 kind : Secret metadata : name : id-rsa type : kubernetes.io/ssh-auth stringData : ssh-privatekey : |- # SSH private key for ssh contained here Creating the RSA key pair for remote SSH access should be a trivial exercise for those who are already familiar with an ssh client, which entails the following actions: Create a new key pair and store the keys in a file named my-id-rsa-key and my-id-rsa-key.pub for the private and public keys respectively: ssh-keygen -f ~/my-id-rsa-key -t rsa -b 4096 For each node available, run this following command to copy the public key of my-id-rsa-key : ssh-copy-id -i my-id-rsa-key user@node For further details, please check this documentation . Once you have copied the public key to all nodes and created the secret described earlier, you are ready to start your experiment.","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-restart/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-restart/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-restart-sa namespace : default labels : name : node-restart-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-restart-sa labels : name : node-restart-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-restart-sa subjects : - kind : ServiceAccount name : node-restart-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-restart/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-restart/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-restart/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-restart/#reboot-command","text":"It defines the command used to restart the targeted node. It can be tuned via REBOOT_COMMAND ENV. Use the following example to tune this: # provide the reboot command apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # command used for the reboot - name : REBOOT_COMMAND value : 'sudo systemctl reboot' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Reboot Command"},{"location":"experiments/categories/nodes/node-restart/#ssh-user","text":"It defines the name of the SSH user for the targeted node. It can be tuned via SSH_USER ENV. Use the following example to tune this: # name of the ssh user used to ssh into targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # name of the ssh user - name : SSH_USER value : 'root' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"SSH User"},{"location":"experiments/categories/nodes/node-restart/#target-node-internal-ip","text":"It defines the internal IP of the targeted node. It is an optional field, if internal IP is not provided then it will derive the internal IP of the targeted node. It can be tuned via TARGET_NODE_IP ENV. Use the following example to tune this: # internal ip of the targeted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-restart-sa experiments : - name : node-restart spec : components : env : # internal ip of the targeted node - name : TARGET_NODE_IP value : '<ip of node01>' # name of the target node - name : TARGET_NODE value : 'node01' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Node Internal IP"},{"location":"experiments/categories/nodes/node-taint/","text":"Introduction \u00b6 It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename> Default Validations \u00b6 View the default validations The target nodes should be in ready state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes TARGET_NODE Name of the node to be tainted NODE_LABEL It contains node label, which will be used to filter the target node if TARGET_NODE ENV is not set It is mutually exclusive with the TARGET_NODE ENV. If both are provided then it will use the TARGET_NODE TAINT_LABEL Label and effect to be tainted on application node Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before injection of chaos in sec Experiment Examples \u00b6 Common and Node specific tunables \u00b6 Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables. Taint Label \u00b6 It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Node Taint"},{"location":"experiments/categories/nodes/node-taint/#introduction","text":"It taints the node to apply the desired effect. The resources which contains the correspoing tolerations can only bypass the taints. Scenario: Taint the node","title":"Introduction"},{"location":"experiments/categories/nodes/node-taint/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/nodes/node-taint/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the node-taint experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that the node specified in the experiment ENV variable TARGET_NODE (the node which will be tainted) should be cordoned before execution of the chaos experiment (before applying the chaosengine manifest) to ensure that the litmus experiment runner pods are not scheduled on it / subjected to eviction. This can be achieved with the following steps: Get node names against the applications pods: kubectl get pods -o wide Cordon the node kubectl cordon <nodename>","title":"Prerequisites"},{"location":"experiments/categories/nodes/node-taint/#default-validations","text":"View the default validations The target nodes should be in ready state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/nodes/node-taint/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : node-taint-sa namespace : default labels : name : node-taint-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"pods/eviction\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"daemonsets\" ] verbs : [ \"list\" , \"get\" , \"delete\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"patch\" , \"get\" , \"list\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-taint-sa labels : name : node-taint-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : node-taint-sa subjects : - kind : ServiceAccount name : node-taint-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/nodes/node-taint/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/nodes/node-taint/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/nodes/node-taint/#common-and-node-specific-tunables","text":"Refer the common attributes and Node specific tunable to tune the common tunables for all experiments and node specific tunables.","title":"Common and Node specific tunables"},{"location":"experiments/categories/nodes/node-taint/#taint-label","text":"It contains label and effect to be tainted on application node. It can be tuned via TAINT_LABEL ENV. Use the following example to tune this: # node tainted with provided key and effect apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : node-taint-sa experiments : - name : node-taint spec : components : env : # label and effect to be tainted on the targeted node - name : TAINT_LABEL value : 'key=value:effect' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Taint Label"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/","text":"It contains tunables, which are common for all pod-level experiments. These tunables can be provided at .spec.experiment[*].spec.components.env in chaosengine. Target Specific Pods \u00b6 It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pod Affected Percentage \u00b6 It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60' Target Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Common tunables for pod experiments"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-pods","text":"It defines the comma-separated name of the target pods subjected to chaos. The target pods can be tuned via TARGET_PODS ENV. Use the following example to tune this: ## it contains comma separated target pod names apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : ## comma separated target pod names - name : TARGET_PODS value : 'pod1,pod2' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Pods"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#pod-affected-percentage","text":"It defines the percentage of pods subjected to chaos with matching labels provided at .spec.appinfo.applabel inside chaosengine. It can be tuned with PODS_AFFECTED_PERC ENV. If PODS_AFFECTED_PERC is provided as empty or 0 then it will target a minimum of one pod. Use the following example to tune this: ## it contains percentage of application pods to be targeted with matching labels or names in the application namespace ## supported for all pod-level experiment expect pod-autoscaler apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # percentage of application pods - name : PODS_AFFECTED_PERC value : '100' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Affected Percentage"},{"location":"experiments/categories/pods/common-tunables-for-pod-experiments/#target-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. Use the following example to tune this: ## name of the target container ## it will use first container as target container if TARGET_CONTAINER is provided as empty apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Target Specific Container"},{"location":"experiments/categories/pods/container-kill/","text":"Introduction \u00b6 It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER The name of container to be killed inside the pod If the TARGET_CONTAINER is not provided it will delete the first container CHAOS_INTERVAL Time interval b/w two successive container kill (in sec) If the CHAOS_INTERVAL is not provided it will take the default value of 10s TOTAL_CHAOS_DURATION The time duration for chaos injection (seconds) Defaults to 20s PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only TARGET_PODS Comma separated list of application pod name subjected to container kill chaos If not provided, it will select target pods randomly based on provided appLabels LIB_IMAGE LIB Image used to kill the container Defaults to litmuschaos/go-runner:latest LIB The category of lib use to inject chaos Default value: litmus, supported values: pumba and litmus RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel SIGNAL It contains termination signal used for container kill Default value: SIGKILL SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Kill Specific Container \u00b6 It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Signal For Kill \u00b6 It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Kill"},{"location":"experiments/categories/pods/container-kill/#introduction","text":"It Causes container failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Good for testing recovery of pods having side-car containers Scenario: Kill target container","title":"Introduction"},{"location":"experiments/categories/pods/container-kill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/container-kill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the container-kill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/container-kill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/container-kill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : container-kill-sa namespace : default labels : name : container-kill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : container-kill-sa subjects : - kind : ServiceAccount name : container-kill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/container-kill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/container-kill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/container-kill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/container-kill/#kill-specific-container","text":"It defines the name of the targeted container subjected to chaos. It can be tuned via TARGET_CONTAINER ENV. If TARGET_CONTAINER is provided as empty then it will use the first container of the targeted pod. # kill the specific target container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the target container - name : TARGET_CONTAINER value : 'nginx' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Kill Specific Container"},{"location":"experiments/categories/pods/container-kill/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/container-kill/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/container-kill/#signal-for-kill","text":"It defines the Linux signal passed while killing the container. It can be tuned via SIGNAL ENV. It defaults to the SIGTERM . # specific linux signal passed while kiiling container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # signal passed while killing container # defaults to SIGTERM - name : SIGNAL value : 'SIGKILL' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Signal For Kill"},{"location":"experiments/categories/pods/container-kill/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . # pumba chaoslib used to kill the container apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : container-kill-sa experiments : - name : container-kill spec : components : env : # name of the lib # supoorts pumba and litmus - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/disk-fill/","text":"Introduction \u00b6 It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes FILL_PERCENTAGE Percentage to fill the Ephemeral storage limit Can be set to more than 100 also, to force evict the pod. The ephemeral-storage limits must be set in targeted pod to use this ENV. EPHEMERAL_STORAGE_MEBIBYTES Ephemeral storage which need to fill (unit: MiBi) It is mutually exclusive with the FILL_PERCENTAGE ENV. If both are provided then it will use the FILL_PERCENTAGE Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to disk-fill If not provided, the first container in the targeted pod will be subject to chaos CONTAINER_PATH Storage Location of containers Defaults to '/var/lib/docker/containers' TOTAL_CHAOS_DURATION The time duration for chaos insertion (sec) Defaults to 60s TARGET_PODS Comma separated list of application pod name subjected to disk fill chaos If not provided, it will select target pods randomly based on provided appLabels DATA_BLOCK_SIZE It contains data block size used to fill the disk(in KB) Defaults to 256, it supports unit as KB only PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Defaults to litmus supported litmus only LIB_IMAGE The image used to fill the disk Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Disk Fill Percentage \u00b6 It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60' Disk Fill Mebibytes \u00b6 It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60' Data Block Size \u00b6 It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Path \u00b6 It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill"},{"location":"experiments/categories/pods/disk-fill/#introduction","text":"It causes Disk Stress by filling up the ephemeral storage of the pod on any given node. It causes the application pod to get evicted if the capacity filled exceeds the pod's ephemeral storage limit. It tests the Ephemeral Storage Limits, to ensure those parameters are sufficient. It tests the application's resiliency to disk stress/replica evictions. Scenario: Fill ephemeral-storage","title":"Introduction"},{"location":"experiments/categories/pods/disk-fill/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/disk-fill/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the disk-fill experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Appropriate Ephemeral Storage Requests and Limits should be set for the application before running the experiment. An example specification is shown below: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : db image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"password\" resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : wp image : wordpress resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\"","title":"Prerequisites"},{"location":"experiments/categories/pods/disk-fill/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/disk-fill/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"list\" , \"get\" , \"create\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : disk-fill-sa namespace : default labels : name : disk-fill-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : disk-fill-sa subjects : - kind : ServiceAccount name : disk-fill-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/disk-fill/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/disk-fill/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/disk-fill/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-percentage","text":"It fills the FILL_PERCENTAGE percentage of the ephemeral-storage limit specified at resource.limits.ephemeral-storage inside the target application. Use the following example to tune this: ## percentage of ephemeral storage limit specified at `resource.limits.ephemeral-storage` inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## percentage of ephemeral storage limit, which needs to be filled - name : FILL_PERCENTAGE value : '80' # in percentage - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Percentage"},{"location":"experiments/categories/pods/disk-fill/#disk-fill-mebibytes","text":"It fills the EPHEMERAL_STORAGE_MEBIBYTES MiBi of ephemeral storage of the targeted pod. It is mutually exclusive with the FILL_PERCENTAGE ENV. If FILL_PERCENTAGE ENV is set then it will use the percentage for the fill otherwise, it will fill the ephemeral storage based on EPHEMERAL_STORAGE_MEBIBYTES ENV. Use the following example to tune this: # ephemeral storage which needs to fill in will application # if ephemeral-storage limits is not specified inside target application apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## ephemeral storage size, which needs to be filled - name : EPHEMERAL_STORAGE_MEBIBYTES value : '256' #in MiBi - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Disk Fill Mebibytes"},{"location":"experiments/categories/pods/disk-fill/#data-block-size","text":"It defines the size of the data block used to fill the ephemeral storage of the targeted pod. It can be tuned via DATA_BLOCK_SIZE ENV. Its unit is KB . The default value of DATA_BLOCK_SIZE is 256 . Use the following example to tune this: # size of the data block used to fill the disk apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : ## size of data block used to fill the disk - name : DATA_BLOCK_SIZE value : '256' #in KB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Data Block Size"},{"location":"experiments/categories/pods/disk-fill/#container-path","text":"It defines the storage location of the containers inside the host(node/VM). It can be tuned via CONTAINER_PATH ENV. Use the following example to tune this: # path inside node/vm where containers are present apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : disk-fill-sa experiments : - name : disk-fill spec : components : env : # storage location of the containers - name : CONTAINER_PATH value : '/var/lib/docker/containers' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Path"},{"location":"experiments/categories/pods/pod-autoscaler/","text":"Introduction \u00b6 The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes REPLICA_COUNT Number of replicas upto which we want to scale nil Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The timeout for the chaos experiment (in seconds) Defaults to 60 LIB The chaos lib used to inject the chaos Defaults to litmus RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Replica counts \u00b6 It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Autoscaler"},{"location":"experiments/categories/pods/pod-autoscaler/#introduction","text":"The experiment aims to check the ability of nodes to accommodate the number of replicas a given application pod. This experiment can be used for other scenarios as well, such as for checking the Node auto-scaling feature. For example, check if the pods are successfully rescheduled within a specified period in cases where the existing nodes are already running at the specified limits. Scenario: Scale the replicas","title":"Introduction"},{"location":"experiments/categories/pods/pod-autoscaler/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-autoscaler/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-autoscaler experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-autoscaler/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-autoscaler/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-autoscaler-sa namespace : default labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" ] verbs : [ \"list\" , \"get\" , \"patch\" , \"update\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-autoscaler-sa labels : name : pod-autoscaler-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : pod-autoscaler-sa subjects : - kind : ServiceAccount name : pod-autoscaler-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-autoscaler/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-autoscaler/#replica-counts","text":"It defines the number of replicas, which should be present in the targeted application during the chaos. It can be tuned via REPLICA_COUNT ENV. Use the following example to tune this: # provide the number of replicas apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-autoscaler-sa experiments : - name : pod-autoscaler spec : components : env : # number of replica, needs to scale - name : REPLICA_COUNT value : '3' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Replica counts"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Default to litmus TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CHAOS_INJECT_COMMAND The command to inject the cpu chaos Default to md5sum /dev/zero CHAOS_KILL_COMMAND The command to kill the chaos process Default to kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}') . Another useful one that generally works (in case the default doesn't) is kill -9 \\((ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print\\) 1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Chaos Inject and Kill Commands \u00b6 It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: - CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog Exec"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-exec-sa namespace : default labels : name : pod-cpu-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-exec-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog-exec/#chaos-inject-and-kill-commands","text":"It defines the CHAOS_INJECT_COMMAND and CHAOS_KILL_COMMAND ENV to set the chaos inject and chaos kill commands respectively. Default values of commands: - CHAOS_INJECT_COMMAND : \"md5sum /dev/zero\" - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" Use the following example to tune this: # provide the chaos kill, used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-exec-sa experiments : - name : pod-cpu-hog-exec spec : components : env : # command to create the md5sum process to stress the cpu - name : CHAOS_INJECT_COMMAND value : 'md5sum /dev/zero' # command to kill the md5sum process # alternative command: \"kill -9 $(ps afx | grep \\\"[md5sum] /dev/zero\\\" | awk '{print$1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/md5sum' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}')\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Inject and Kill Commands"},{"location":"experiments/categories/pods/pod-cpu-hog/","text":"Introduction \u00b6 This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes CPU_CORES Number of the cpu cores subjected to CPU stress Default to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Default to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod cpu hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. CPU Cores \u00b6 It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod CPU Hog"},{"location":"experiments/categories/pods/pod-cpu-hog/#introduction","text":"This experiment consumes the CPU resources of the application container It simulates conditions where app pods experience CPU spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. It can test the application's resilience to potential slowness/unavailability of some replicas due to high CPU load Scenario: Stress the CPU","title":"Introduction"},{"location":"experiments/categories/pods/pod-cpu-hog/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems. Injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in application traffic or in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.So this category of chaos experiment helps to build the immunity on the application undergoing any such stress scenario","title":"Uses"},{"location":"experiments/categories/pods/pod-cpu-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-cpu-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-cpu-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-cpu-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-cpu-hog-sa namespace : default labels : name : pod-cpu-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-cpu-hog-sa subjects : - kind : ServiceAccount name : pod-cpu-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-cpu-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-cpu-hog/#cpu-cores","text":"It stresses the CPU_CORE cpu cores of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # cpu cores for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # cpu cores for stress - name : CPU_CORES value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"CPU Cores"},{"location":"experiments/categories/pods/pod-cpu-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-cpu-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-cpu-hog-sa experiments : - name : pod-cpu-hog spec : components : env : # name of chaos lib # supports litmus and pumba - name : LIB value : 'pumba' # stress image - applicable for pumba only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-delete/","text":"Introduction \u00b6 It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod Uses \u00b6 View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application. Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The time duration for chaos insertion (in sec) Defaults to 15s, NOTE: Overall run duration of the experiment may exceed the TOTAL_CHAOS_DURATION by a few min CHAOS_INTERVAL Time interval b/w two successive pod failures (in sec) Defaults to 5s RANDOMNESS Introduces randomness to pod deletions with a minimum period defined by CHAOS_INTERVAL It supports true or false. Default value: false FORCE Application Pod deletion mode. false indicates graceful deletion with default termination period of 30s. true indicates an immediate forceful deletion with 0s grace period Default to true , With terminationGracePeriodSeconds=0 TARGET_PODS Comma separated list of application pod name subjected to pod delete chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Force Delete \u00b6 The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' Multiple Iterations Of Chaos \u00b6 The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60' Random Interval \u00b6 The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. - If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. - If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Pod Delete"},{"location":"experiments/categories/pods/pod-delete/#introduction","text":"It Causes (forced/graceful) pod failure of specific/random replicas of an application resources. It tests deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application Scenario: Deletes kubernetes pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-delete/#uses","text":"View the uses of the experiment In the distributed system like kubernetes it is very likely that your application replicas may not be sufficient to manage the traffic (indicated by SLIs) when some of the replicas are unavailable due to any failure (can be system or application) the application needs to meet the SLO(service level objectives) for this, we need to make sure that the applications have minimum number of available replicas. One of the common application failures is when the pressure on other replicas increases then to how the horizontal pod autoscaler scales based on observed resource utilization and also how much PV mount takes time upon rescheduling. The other important aspects to test are the MTTR for the application replica, re-elections of leader or follower like in kafka application the selection of broker leader, validating minimum quorum to run the application for example in applications like percona, resync/redistribution of data. This experiment helps to reproduce such a scenario with forced/graceful pod failure on specific or random replicas of an application resource and checks the deployment sanity (replica availability & uninterrupted service) and recovery workflow of the application.","title":"Uses"},{"location":"experiments/categories/pods/pod-delete/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-delete experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-delete/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-delete/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-delete-sa namespace : default labels : name : pod-delete-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-delete-sa subjects : - kind : ServiceAccount name : pod-delete-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-delete/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-delete/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-delete/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-delete/#force-delete","text":"The targeted pod can be deleted forcefully or gracefully . It can be tuned with the FORCE env. It will delete the pod forcefully if FORCE is provided as true and it will delete the pod gracefully if FORCE is provided as false . Use the following example to tune this: # tune the deletion of target pods forcefully or gracefully apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # provided as true for the force deletion of pod # supports true and false value - name : FORCE value : 'true' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Force Delete"},{"location":"experiments/categories/pods/pod-delete/#multiple-iterations-of-chaos","text":"The multiple iterations of chaos can be tuned via setting CHAOS_INTERVAL ENV. Which defines the delay between each iteration of chaos. Use the following example to tune this: # defines delay between each successive iteration of the chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # delay between each iteration of chaos - name : CHAOS_INTERVAL value : '15' # time duration for the chaos execution - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Multiple Iterations Of Chaos"},{"location":"experiments/categories/pods/pod-delete/#random-interval","text":"The randomness in the chaos interval can be enabled via setting RANDOMNESS ENV to true . It supports boolean values. The default value is false . The chaos interval can be tuned via CHAOS_INTERVAL ENV. - If CHAOS_INTERVAL is set in the form of l-r i.e, 5-10 then it will select a random interval between l & r. - If CHAOS_INTERVAL is set in the form of value i.e, 10 then it will select a random interval between 0 & value. Use the following example to tune this: # contains random chaos interval with lower and upper bound of range i.e [l,r] apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : env : # randomness enables iterations at random time interval # it supports true and false value - name : RANDOMNESS value : 'true' - name : TOTAL_CHAOS_DURATION value : '60' # it will select a random interval within this range # if only one value is provided then it will select a random interval within 0-CHAOS_INTERVAL range - name : CHAOS_INTERVAL value : '5-10'","title":"Random Interval"},{"location":"experiments/categories/pods/pod-dns-error/","text":"Introduction \u00b6 Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns-error None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_HOSTNAMES List of the target hostnames or keywords eg. '[\"litmuschaos\",\"chaosnative.com\"]' If not provided, all hostnames/domains will be targeted MATCH_SCHEME Determines whether the dns query has to match exactly with one of the targets or can have any of the targets as substring. Can be either exact or substring if not provided, it will be set as exact PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Target Host Names \u00b6 It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60' Match Scheme \u00b6 It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Error"},{"location":"experiments/categories/pods/pod-dns-error/#introduction","text":"Pod-dns-error injects chaos to disrupt dns resolution in kubernetes pods. It causes loss of access to services by blocking dns resolution of hostnames/domains Scenario: DNS error for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-error/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-error/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-error experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-error/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-error/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-error-sa namespace : default labels : name : pod-dns-error-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-error-sa subjects : - kind : ServiceAccount name : pod-dns-error-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-error/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-error/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-error/#target-host-names","text":"It defines the comma-separated name of the target hosts subjected to chaos. It can be tuned with the TARGET_HOSTNAMES ENV. If TARGET_HOSTNAMES not provided then all hostnames/domains will be targeted. Use the following example to tune this: # contains the target host names for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## comma separated list of host names ## if not provided, all hostnames/domains will be targeted - name : TARGET_HOSTNAMES value : '[\"litmuschaos\",\"chaosnative.com\"]' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Target Host Names"},{"location":"experiments/categories/pods/pod-dns-error/#match-scheme","text":"It determines whether the DNS query has to match exactly with one of the targets or can have any of the targets as a substring. It can be tuned with MATCH_SCHEME ENV. It supports exact or substring values. Use the following example to tune this: # contains match scheme for the dns error apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : ## it supports 'exact' and 'substring' values - name : MATCH_SCHEME value : 'exact' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Match Scheme"},{"location":"experiments/categories/pods/pod-dns-error/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-error-sa experiments : - name : pod-dns-error spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-dns-spoof/","text":"Introduction \u00b6 Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes TARGET_CONTAINER Name of container which is subjected to dns spoof None TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) SPOOF_MAP Map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. If not provided, no hostnames/domains will be spoofed PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock LIB The chaos lib used to inject the chaos Default value: litmus, supported values: litmus LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Spoof Map \u00b6 It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod Dns Spoof"},{"location":"experiments/categories/pods/pod-dns-spoof/#introduction","text":"Pod-dns-spoof injects chaos to spoof dns resolution in kubernetes pods. It causes dns resolution of target hostnames/domains to wrong IPs as specified by SPOOF_MAP in the engine config. Scenario: DNS spoof for the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-dns-spoof/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-dns-spoof/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-dns-spoof experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-dns-spoof/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-dns-spoof/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-dns-spoof-sa namespace : default labels : name : pod-dns-spoof-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-dns-spoof-sa subjects : - kind : ServiceAccount name : pod-dns-spoof-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-dns-spoof/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-dns-spoof/#spoof-map","text":"It defines the map of the target hostnames eg. '{\"abc.com\":\"spoofabc.com\"}' where the key is the hostname that needs to be spoofed and value is the hostname where it will be spoofed/redirected to. It can be tuned via SPOOF_MAP ENV. Use the following example to tune this: # contains the spoof map for the dns spoofing apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Spoof Map"},{"location":"experiments/categories/pods/pod-dns-spoof/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker runtime only. SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-dns-spoof-sa experiments : - name : pod-dns-spoof spec : components : env : # runtime for the container # supports docker - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' # map of host names - name : SPOOF_MAP value : '{\"abc.com\":\"spoofabc.com\"}' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/","text":"Introduction \u00b6 This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod Uses \u00b6 View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems Stressing the disk with continuous and heavy IO for example can cause degradation in reads written by other microservices that use this shared disk for example modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out. Another issue is the amount of scratch space eaten up on a node which leads to the lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes FILESYSTEM_UTILIZATION_PERCENTAGE Specify the size as percentage of free space on the file system Default to 10% FILESYSTEM_UTILIZATION_BYTES Specify the size in GigaBytes(GB). FILESYSTEM_UTILIZATION_PERCENTAGE & FILESYSTEM_UTILIZATION_BYTES are mutually exclusive. If both are provided, FILESYSTEM_UTILIZATION_PERCENTAGE is prioritized. NUMBER_OF_WORKERS It is the number of IO workers involved in IO disk stress Default to 4 TOTAL_CHAOS_DURATION The time duration for chaos (seconds) Default to 120s VOLUME_MOUNT_PATH Fill the given volume mount path LIB The chaos lib used to inject the chaos Default to litmus . Available litmus and pumba. LIB_IMAGE Image used to run the stress command Default to litmuschaos/go-runner:latest TARGET_PODS Comma separated list of application pod name subjected to pod io stress chaos If not provided, it will select target pods randomly based on provided appLabels PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Filesystem Utilization Percentage \u00b6 It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Filesystem Utilization Bytes \u00b6 It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Mount Path \u00b6 The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pod IO Stress"},{"location":"experiments/categories/pods/pod-io-stress/#introduction","text":"This experiment causes disk stress on the application pod. The experiment aims to verify the resiliency of applications that share this disk resource for ephemeral or persistent storage purposes Scenario: Stress the IO of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-io-stress/#uses","text":"View the uses of the experiment Disk Pressure or CPU hogs is another very common and frequent scenario we find in kubernetes applications that can result in the eviction of the application replica and impact its delivery. Such scenarios that can still occur despite whatever availability aids K8s provides. These problems are generally referred to as \"Noisy Neighbour\" problems Stressing the disk with continuous and heavy IO for example can cause degradation in reads written by other microservices that use this shared disk for example modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out. Another issue is the amount of scratch space eaten up on a node which leads to the lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes","title":"Uses"},{"location":"experiments/categories/pods/pod-io-stress/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-io-stress experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-io-stress/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-io-stress/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-io-stress-sa namespace : default labels : name : pod-io-stress-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-io-stress-sa subjects : - kind : ServiceAccount name : pod-io-stress-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-io-stress/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-io-stress/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-percentage","text":"It stresses the FILESYSTEM_UTILIZATION_PERCENTAGE percentage of total free space available in the pod. Use the following example to tune this: # stress the i/o of the targeted pod with FILESYSTEM_UTILIZATION_PERCENTAGE of total free space # it is mutually exclusive with the FILESYSTEM_UTILIZATION_BYTES. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # percentage of free space of file system, need to be stressed - name : FILESYSTEM_UTILIZATION_PERCENTAGE value : '10' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Percentage"},{"location":"experiments/categories/pods/pod-io-stress/#filesystem-utilization-bytes","text":"It stresses the FILESYSTEM_UTILIZATION_BYTES GB of the i/o of the targeted pod. It is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE ENV. If FILESYSTEM_UTILIZATION_PERCENTAGE ENV is set then it will use the percentage for the stress otherwise, it will stress the i/o based on FILESYSTEM_UTILIZATION_BYTES ENV. Use the following example to tune this: # stress the i/o of the targeted pod with given FILESYSTEM_UTILIZATION_BYTES # it is mutually exclusive with the FILESYSTEM_UTILIZATION_PERCENTAGE. # if both are provided then it will use FILESYSTEM_UTILIZATION_PERCENTAGE for stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # size of io to be stressed - name : FILESYSTEM_UTILIZATION_BYTES value : '1' #in GB - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Filesystem Utilization Bytes"},{"location":"experiments/categories/pods/pod-io-stress/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-io-stress/#mount-path","text":"The volume mount path, which needs to be filled. It can be tuned with VOLUME_MOUNT_PATH ENV. Use the following example to tune this: # provide the volume mount path, which needs to be filled apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # path need to be stressed/filled - name : VOLUME_MOUNT_PATH value : '10' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Mount Path"},{"location":"experiments/categories/pods/pod-io-stress/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # number of io workers - name : NUMBER_OF_WORKERS value : '4' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-io-stress/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Use the following example to tune this: # use the pumba lib for io stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-io-stress-sa experiments : - name : pod-io-stress spec : components : env : # name of lib # it supports litmus and pumba lib - name : LIB value : 'pumba' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-memory-hog-exec/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB (Up to 2000MB) TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus Defaults to litmus TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos If not provided, it will select the first container of the target pod CHAOS_KILL_COMMAND The command to kill the chaos process Defaults to kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1) . Another useful one that generally works (in case the default doesn't) is kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ') . In case neither works, please check whether the target pod's base image offers a shell. If yes, identify appropriate shell command to kill the chaos process PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Chaos Kill Commands \u00b6 It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog Exec"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#uses","text":"View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog-exec experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-exec-sa namespace : default labels : name : pod-memory-hog-exec-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-exec-sa subjects : - kind : ServiceAccount name : pod-memory-hog-exec-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. The memory consumption limit is 2000MB Use the following example to tune this: # memory to be stressed in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consuption value in MB # it is limited to 2000MB - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog-exec/#chaos-kill-commands","text":"It defines the CHAOS_KILL_COMMAND ENV to set the chaos kill command. Default values of CHAOS_KILL_COMMAND command: - CHAOS_KILL_COMMAND : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" Use the following example to tune this: # provide the chaos kill command used to kill the chaos process apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-exec-sa experiments : - name : pod-memory-hog-exec spec : components : env : # command to kill the dd process # alternative command: \"kill -9 $(ps afx | grep \\\"[dd] if=/dev/zero\\\" | awk '{print $1}' | tr '\\n' ' ')\" - name : CHAOS_KILL_COMMAND value : \"kill $(find /proc -name exe -lname '*/dd' 2>&1 | grep -v 'Permission denied' | awk -F/ '{print $(NF-1)}' | head -n 1)\" - name : TOTAL_CHAOS_DURATION value : '60'","title":"Chaos Kill Commands"},{"location":"experiments/categories/pods/pod-memory-hog/","text":"Introduction \u00b6 This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory Uses \u00b6 View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes MEMORY_CONSUMPTION The amount of memory used of hogging a Kubernetes pod (megabytes) Defaults to 500MB (Up to 2000MB) NUMBER_OF_WORKERS The number of workers used to run the stress process Defaults to 1 TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Defaults to 60s LIB The chaos lib used to inject the chaos. Available libs are litmus and pumba Defaults to litmus LIB_IMAGE Image used to run the helper pod. Defaults to litmuschaos/go-runner:1.13.8 STRESS_IMAGE Container run on the node at runtime by the pumba lib to inject stressors. Only used in LIB pumba Default to alexeiled/stress-ng:latest-ubuntu TARGET_PODS Comma separated list of application pod name subjected to pod memory hog chaos If not provided, it will select target pods randomly based on provided appLabels TARGET_CONTAINER Name of the target container under chaos. If not provided, it will select the first container of the target pod CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only RAMP_TIME Period to wait before injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Memory Consumption \u00b6 It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60' Workers For Stress \u00b6 The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Memory Hog"},{"location":"experiments/categories/pods/pod-memory-hog/#introduction","text":"This experiment consumes the Memory resources on the application container on specified memory in megabytes. It simulates conditions where app pods experience Memory spikes either due to expected/undesired processes thereby testing how the overall application stack behaves when this occurs. Scenario: Stress the Memory","title":"Introduction"},{"location":"experiments/categories/pods/pod-memory-hog/#uses","text":"View the uses of the experiment Memory usage within containers is subject to various constraints in Kubernetes. If the limits are specified in their spec, exceeding them can cause termination of the container (due to OOMKill of the primary process, often pid 1) - the restart of the container by kubelet, subject to the policy specified. For containers with no limits placed, the memory usage is uninhibited until such time as the Node level OOM Behaviour takes over. In this case, containers on the node can be killed based on their oom_score and the QoS class a given pod belongs to (bestEffort ones are first to be targeted). This eval is extended to all pods running on the node - thereby causing a bigger blast radius. This experiment launches a stress process within the target container - which can cause either the primary process in the container to be resource constrained in cases where the limits are enforced OR eat up available system memory on the node in cases where the limits are not specified","title":"Uses"},{"location":"experiments/categories/pods/pod-memory-hog/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-memory-hog experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-memory-hog/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-memory-hog/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-memory-hog-sa namespace : default labels : name : pod-memory-hog-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-memory-hog-sa subjects : - kind : ServiceAccount name : pod-memory-hog-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-memory-hog/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-memory-hog/#memory-consumption","text":"It stresses the MEMORY_CONSUMPTION MB memory of the targeted pod for the TOTAL_CHAOS_DURATION duration. Use the following example to tune this: # define the memory consumption in MB apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # memory consumption value - name : MEMORY_CONSUMPTION value : '500' #in MB - name : TOTAL_CHAOS_DURATION value : '60'","title":"Memory Consumption"},{"location":"experiments/categories/pods/pod-memory-hog/#workers-for-stress","text":"The worker's count for the stress can be tuned with NUMBER_OF_WORKERS ENV. Use the following example to tune this: # number of workers used for the stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # number of workers for stress - name : NUMBER_OF_WORKERS value : '1' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Workers For Stress"},{"location":"experiments/categories/pods/pod-memory-hog/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path.","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-memory-hog/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the stress image via STRESS_IMAGE ENV for the pumba library. Use the following example to tune this: # use the pumba lib for the memory stress apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-memory-hog-sa experiments : - name : pod-memory-hog spec : components : env : # name of chaoslib # it supports litmus and pumba lib - name : LIB value : 'pumba' # stress image - applicable for pumba lib only - name : STRESS_IMAGE value : 'alexeiled/stress-ng:latest-ubuntu' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-corruption/","text":"Introduction \u00b6 It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network corruption Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_CORRUPTION_PERCENTAGE Packet corruption in percentage Default (100) CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Corruption \u00b6 It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#introduction","text":"It injects packet corruption on the specified container by starting a traffic control (tc) process with netem rules to add egress packet corruption It can test the application's resilience to lossy/flaky network Scenario: Corrupt the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-corruption/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-corruption/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-corruption experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-corruption/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-corruption/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-corruption-sa namespace : default labels : name : pod-network-corruption-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-corruption-sa subjects : - kind : ServiceAccount name : pod-network-corruption-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-corruption/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-corruption/#network-packet-corruption","text":"It defines the network packet corruption percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_CORRUPTION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-corruption for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # network packet corruption percentage - name : NETWORK_PACKET_CORRUPTION_PERCENTAGE value : '100' #in percentage - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Corruption"},{"location":"experiments/categories/pods/pod-network-corruption/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-corruption/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-corruption/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-corruption/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-corruption-sa experiments : - name : pod-network-corruption spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-duplication/","text":"Introduction \u00b6 It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_DUPLICATION_PERCENTAGE The packet duplication in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Duplication \u00b6 It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#introduction","text":"It injects chaos to disrupt network connectivity to kubernetes pods. It causes Injection of network duplication on the specified container by starting a traffic control (tc) process with netem rules to add egress delays. It Can test the application's resilience to duplicate network. Scenario: Duplicate the network packets of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-duplication/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-duplication/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-duplication experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-duplication/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-duplication/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-duplication-sa namespace : default labels : name : pod-network-duplication-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-duplication-sa subjects : - kind : ServiceAccount name : pod-network-duplication-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-duplication/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-duplication/#network-packet-duplication","text":"It defines the network packet duplication percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_DUPLICATION_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-duplication for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # network packet duplication percentage - name : NETWORK_PACKET_DUPLICATION_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Duplication"},{"location":"experiments/categories/pods/pod-network-duplication/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-duplication/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-duplication/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-duplication/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-duplication-sa experiments : - name : pod-network-duplication spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-latency/","text":"Introduction \u00b6 It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod Uses \u00b6 View the uses of the experiment The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container). The idea of this experiment is to simulate issues within your pod network OR microservice communication across services in different availability zones/regions etc. Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The experiment provides DESTINATION_IPS or DESTINATION_HOSTS so that you can control the chaos against specific services within or outside the cluster. The applications may stall or get corrupted while they wait endlessly for a packet. The experiment limits the impact (blast radius) to only the traffic you want to test by specifying IP addresses or application information.This experiment will help to improve the resilience of your services over time Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network latency Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_LATENCY The latency/delay in milliseconds Optional Default 2000, provide numeric value only CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Latency \u00b6 It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#introduction","text":"It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce letency in the network of target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-latency/#uses","text":"View the uses of the experiment The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container). The idea of this experiment is to simulate issues within your pod network OR microservice communication across services in different availability zones/regions etc. Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The experiment provides DESTINATION_IPS or DESTINATION_HOSTS so that you can control the chaos against specific services within or outside the cluster. The applications may stall or get corrupted while they wait endlessly for a packet. The experiment limits the impact (blast radius) to only the traffic you want to test by specifying IP addresses or application information.This experiment will help to improve the resilience of your services over time","title":"Uses"},{"location":"experiments/categories/pods/pod-network-latency/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-latency experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-latency/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-latency/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-latency-sa namespace : default labels : name : pod-network-latency-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-latency-sa subjects : - kind : ServiceAccount name : pod-network-latency-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-latency/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-latency/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-latency/#network-latency","text":"It defines the network latency(in ms) to be injected in the targeted application. It can be tuned via NETWORK_LATENCY ENV. Use the following example to tune this: # it inject the network-latency for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # network latency to be injected - name : NETWORK_LATENCY value : '2000' #in ms - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Latency"},{"location":"experiments/categories/pods/pod-network-latency/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-latency/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-latency/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-latency/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-latency-sa experiments : - name : pod-network-latency spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/pods/pod-network-loss/","text":"Introduction \u00b6 It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Default Validations \u00b6 View the default validations The application pods should be in running state before and after chaos injection. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Optional Fields Variables Description Notes NETWORK_INTERFACE Name of ethernet interface considered for shaping traffic TARGET_CONTAINER Name of container which is subjected to network loss Optional Applicable for containerd & CRI-O runtime only. Even with these runtimes, if the value is not provided, it injects chaos on the first container of the pod NETWORK_PACKET_LOSS_PERCENTAGE The packet loss in percentage Optional Default to 100 percentage CONTAINER_RUNTIME container runtime interface for the cluster Defaults to docker, supported values: docker, containerd and crio for litmus and only docker for pumba LIB SOCKET_PATH Path of the containerd/crio/docker socket file Defaults to /var/run/docker.sock TOTAL_CHAOS_DURATION The time duration for chaos insertion (seconds) Default (60s) TARGET_PODS Comma separated list of application pod name subjected to pod network corruption chaos If not provided, it will select target pods randomly based on provided appLabels DESTINATION_IPS IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted comma separated IP(S) or CIDR(S) can be provided. if not provided, it will induce network chaos for all ips/destinations DESTINATION_HOSTS DNS Names/FQDN names of the services, the accessibility to which, is impacted if not provided, it will induce network chaos for all ips/destinations or DESTINATION_IPS if already defined PODS_AFFECTED_PERC The Percentage of total pods to target Defaults to 0 (corresponds to 1 replica), provide numeric value only LIB The chaos lib used to inject the chaos Default value: litmus, supported values: pumba and litmus TC_IMAGE Image used for traffic control in linux default value is gaiadocker/iproute2 LIB_IMAGE Image used to run the netem command Defaults to litmuschaos/go-runner:latest RAMP_TIME Period to wait before and after injection of chaos in sec SEQUENCE It defines sequence of chaos execution for multiple target pods Default value: parallel. Supported: serial, parallel Experiment Examples \u00b6 Common and Pod specific tunables \u00b6 Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables. Network Packet Loss \u00b6 It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60' Destination IPs And Destination Hosts \u00b6 The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60' Network Interface \u00b6 The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60' Container Runtime Socket Path \u00b6 It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60' Pumba Chaos Library \u00b6 It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pod Network Loss"},{"location":"experiments/categories/pods/pod-network-loss/#introduction","text":"It injects latency on the specified container by starting a traffic control (tc) process with netem rules to add egress delays It can test the application's resilience to lossy/flaky network Scenario: Induce network loss of the target pod","title":"Introduction"},{"location":"experiments/categories/pods/pod-network-loss/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/pods/pod-network-loss/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the pod-network-loss experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here","title":"Prerequisites"},{"location":"experiments/categories/pods/pod-network-loss/#default-validations","text":"View the default validations The application pods should be in running state before and after chaos injection.","title":"Default Validations"},{"location":"experiments/categories/pods/pod-network-loss/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions apiVersion : v1 kind : ServiceAccount metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" , \"replicationcontrollers\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" , \"replicasets\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"apps.openshift.io\" ] resources : [ \"deploymentconfigs\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"argoproj.io\" ] resources : [ \"rollouts\" ] verbs : [ \"list\" , \"get\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-network-loss-sa namespace : default labels : name : pod-network-loss-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : pod-network-loss-sa subjects : - kind : ServiceAccount name : pod-network-loss-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/pods/pod-network-loss/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/pods/pod-network-loss/#common-and-pod-specific-tunables","text":"Refer the common attributes and Pod specific tunable to tune the common tunables for all experiments and pod specific tunables.","title":"Common and Pod specific tunables"},{"location":"experiments/categories/pods/pod-network-loss/#network-packet-loss","text":"It defines the network packet loss percentage to be injected in the targeted application. It can be tuned via NETWORK_PACKET_LOSS_PERCENTAGE ENV. Use the following example to tune this: # it inject the network-loss for the ingrees and egress traffic apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # network packet loss percentage - name : NETWORK_PACKET_LOSS_PERCENTAGE value : '100' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Packet Loss"},{"location":"experiments/categories/pods/pod-network-loss/#destination-ips-and-destination-hosts","text":"The network experiments interrupt traffic for all the IPs/hosts by default. The interruption of specific IPs/Hosts can be tuned via DESTINATION_IPS and DESTINATION_HOSTS ENV. DESTINATION_IPS : It contains the IP addresses of the services or pods or the CIDR blocks(range of IPs), the accessibility to which is impacted. DESTINATION_HOSTS : It contains the DNS Names/FQDN names of the services, the accessibility to which, is impacted. Use the following example to tune this: # it inject the chaos for the ingrees and egress traffic for specific ips/hosts apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # supports comma separated destination ips - name : DESTINATION_IPS value : '8.8.8.8,192.168.5.6' # supports comma separated destination hosts - name : DESTINATION_HOSTS value : 'nginx.default.svc.cluster.local,google.com' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Destination IPs And Destination Hosts"},{"location":"experiments/categories/pods/pod-network-loss/#network-interface","text":"The defined name of the ethernet interface, which is considered for shaping traffic. It can be tuned via NETWORK_INTERFACE ENV. Its default value is eth0 . Use the following example to tune this: # provide the network interface apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the network interface - name : NETWORK_INTERFACE value : 'eth0' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Network Interface"},{"location":"experiments/categories/pods/pod-network-loss/#container-runtime-socket-path","text":"It defines the CONTAINER_RUNTIME and SOCKET_PATH ENV to set the container runtime and socket file path. - CONTAINER_RUNTIME : It supports docker , containerd , and crio runtimes. The default value is docker . SOCKET_PATH : It contains path of docker socket file by default( /var/run/docker.sock ). For other runtimes provide the appropriate path. Use the following example to tune this: ## provide the container runtime and socket file path apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # runtime for the container # supports docker, containerd, crio - name : CONTAINER_RUNTIME value : 'docker' # path of the socket file - name : SOCKET_PATH value : '/var/run/docker.sock' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Container Runtime Socket Path"},{"location":"experiments/categories/pods/pod-network-loss/#pumba-chaos-library","text":"It specifies the Pumba chaos library for the chaos injection. It can be tuned via LIB ENV. The defaults chaos library is litmus . Provide the traffic control image via TC_IMAGE ENV for the pumba library. Use the following example to tune this: # use pumba chaoslib for the network chaos apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-network-loss-sa experiments : - name : pod-network-loss spec : components : env : # name of the chaoslib # supports litmus and pumba lib - name : LIB value : 'pumba' # image used for the traffic control in linux # applicable for pumba lib only - name : TC_IMAGE value : '' - name : TOTAL_CHAOS_DURATION value : '60'","title":"Pumba Chaos Library"},{"location":"experiments/categories/vmware/vm-poweroff/","text":"Introduction \u00b6 It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm Uses \u00b6 View the uses of the experiment coming soon Prerequisites \u00b6 Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable. Default Validations \u00b6 View the default validations VM instance should be in healthy state. Minimal RBAC configuration example (optional) \u00b6 View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment. Experiment tunables \u00b6 check the experiment tunables Mandatory Fields Variables Description Notes APP_VM_MOID Moid of the vmware instance Once you open VM in vCenter WebClient, you can find MOID in address field (VirtualMachine:vm-5365). Eg: vm-5365 Optional Fields Variables Description Notes TOTAL_CHAOS_DURATION The total time duration for chaos insertion (sec) Defaults to 30s RAMP_TIME Period to wait before and after injection of chaos in sec Experiment Examples \u00b6 Common Experiment Tunables \u00b6 Refer the common attributes to tune the common tunables for all the experiments. Stop/Poweroff VM By MOID \u00b6 It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"VM Poweroff"},{"location":"experiments/categories/vmware/vm-poweroff/#introduction","text":"It causes Stops/PowerOff a VM before bringing it back to running state after a specified chaos duration Experiment uses vmware api's to start/stop the target vm. It helps to check the performance of the application/process running on the vmware server. Scenario: poweroff the vm","title":"Introduction"},{"location":"experiments/categories/vmware/vm-poweroff/#uses","text":"View the uses of the experiment coming soon","title":"Uses"},{"location":"experiments/categories/vmware/vm-poweroff/#prerequisites","text":"Verify the prerequisites Ensure that Kubernetes Version > 1.16 Ensure that the Litmus Chaos Operator is running by executing kubectl get pods in operator namespace (typically, litmus ).If not, install from here Ensure that the vm-poweroff experiment resource is available in the cluster by executing kubectl get chaosexperiments in the desired namespace. If not, install from here Ensure that you have sufficient Vcenter access to stop and start the vm. (Optional) Ensure to create a Kubernetes secret having the Vcenter credentials in the CHAOS_NAMESPACE . A sample secret file looks like: apiVersion : v1 kind : Secret metadata : name : vcenter-secret namespace : litmus type : Opaque stringData : VCENTERSERVER : XXXXXXXXXXX VCENTERUSER : XXXXXXXXXXXXX VCENTERPASS : XXXXXXXXXXXXX Note: You can pass the VM credentials as secrets or as an chaosengine ENV variable.","title":"Prerequisites"},{"location":"experiments/categories/vmware/vm-poweroff/#default-validations","text":"View the default validations VM instance should be in healthy state.","title":"Default Validations"},{"location":"experiments/categories/vmware/vm-poweroff/#minimal-rbac-configuration-example-optional","text":"View the Minimal RBAC permissions --- apiVersion : v1 kind : ServiceAccount metadata : name : vm-poweroff-sa namespace : default labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"events\" , \"secrets\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" , \"pods/log\" ] verbs : [ \"create\" , \"list\" , \"get\" ] - apiGroups : [ \"batch\" ] resources : [ \"jobs\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"delete\" , \"deletecollection\" ] - apiGroups : [ \"litmuschaos.io\" ] resources : [ \"chaosengines\" , \"chaosexperiments\" , \"chaosresults\" ] verbs : [ \"create\" , \"list\" , \"get\" , \"patch\" , \"update\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vm-poweroff-sa labels : name : vm-poweroff-sa app.kubernetes.io/part-of : litmus roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vm-poweroff-sa subjects : - kind : ServiceAccount name : vm-poweroff-sa namespace : default Use this sample RBAC manifest to create a chaosServiceAccount in the desired (app) namespace. This example consists of the minimum necessary role permissions to execute the experiment.","title":"Minimal RBAC configuration example (optional)"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-tunables","text":"check the experiment tunables","title":"Experiment tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#experiment-examples","text":"","title":"Experiment Examples"},{"location":"experiments/categories/vmware/vm-poweroff/#common-experiment-tunables","text":"Refer the common attributes to tune the common tunables for all the experiments.","title":"Common Experiment Tunables"},{"location":"experiments/categories/vmware/vm-poweroff/#stoppoweroff-vm-by-moid","text":"It contains moid of the vm instance. It can be tuned via APP_VM_MOID ENV. Use the following example to tune this: # power-off the vmware vm apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" chaosServiceAccount : vm-poweroff-sa experiments : - name : vm-poweroff spec : components : env : # moid of the vm instance - name : APP_VM_MOID value : 'vm-5365' - name : TOTAL_CHAOS_DURATION VALUE : '60'","title":"Stop/Poweroff VM By MOID"},{"location":"experiments/chaos-resources/contents/","text":"Chaos Resources \u00b6 At the heart of the Litmus Platform are the chaos custom resources. This section consists of the specification (details of each field within the .spec & .status of the resources) as well as standard examples for tuning the supported parameters. Chaos Resource Name Description User Guide ChaosEngine Contains the ChaosEngine specifications user-guide ChaosEngine ChaosExperiment Contains the ChaosExperiment specifications user-guide ChaosExperiment ChaosResult Contains the ChaosResult specifications user-guide ChaosResult ChaosScheduler Contains the ChaosScheduler specifications user-guide ChaosScheduler Probes Contains the Probes specifications user-guide Probes","title":"Contents"},{"location":"experiments/chaos-resources/contents/#chaos-resources","text":"At the heart of the Litmus Platform are the chaos custom resources. This section consists of the specification (details of each field within the .spec & .status of the resources) as well as standard examples for tuning the supported parameters. Chaos Resource Name Description User Guide ChaosEngine Contains the ChaosEngine specifications user-guide ChaosEngine ChaosExperiment Contains the ChaosExperiment specifications user-guide ChaosExperiment ChaosResult Contains the ChaosResult specifications user-guide ChaosResult ChaosScheduler Contains the ChaosScheduler specifications user-guide ChaosScheduler Probes Contains the Probes specifications user-guide Probes","title":"Chaos Resources"},{"location":"experiments/chaos-resources/chaos-engine/application-details/","text":"It defines the appns , applabel , and appkind to set the namespace, labels, and kind of the application under test. - appkind : It supports deployment , statefulset , daemonset , deploymentconfig , and rollout . It is mandatory for the pod-level experiments and optional for the rest of the experiments. Use the following example to tune this: # contains details of the AUT(application under test) # appns: name of the application # applabel: label of the applicaton # appkind: kind of the application. supports: deployment, statefulset, daemonset, rollout, deploymentconfig apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # AUT details appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Auxiliary Application Info \u00b6 The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Application Specifications"},{"location":"experiments/chaos-resources/chaos-engine/application-details/#auxiliary-application-info","text":"The contains a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In the case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. It can be tuned via auxiliaryAppInfo field. It supports input the below format: - auxiliaryAppInfo : <key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2> Use the following example to tune this: # contains the comma seperated list of auxiliary applications details # it is provide in `<key1>=<value1>:<namespace1>,<key2>=<value2>:<namespace2>` format apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # provide the comma separated auxiliary applications details auxiliaryAppInfo : \"app=nginx:nginx,app=busybox:default\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Auxiliary Application Info"},{"location":"experiments/chaos-resources/chaos-engine/contents/","text":"Chaos Engine Specifications \u00b6 Bind an instance of a given app with one or more chaos experiments, define run characteristics, override chaos defaults, define steady-state hypothesis, reconciled by Litmus Chaos Operator. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same. Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications State Specification \u00b6 View the state specification schema Field .spec.engineState Description Flag to control the state of the chaosengine Type Mandatory Range active , stop Default active Notes The engineState in the spec is a user defined flag to trigger chaos. Setting it to active ensures successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on actual state of the ChaosEngine. Application Specification \u00b6 View the application specification schema Field .spec.appinfo.appns Description Flag to specify namespace of application under test Type Optional Range user-defined (type: string) Default n/a Notes The appns in the spec specifies the namespace of the AUT. Usually provided as a quoted string. It is optional for the infra chaos. Field .spec.appinfo.applabel Description Flag to specify unique label of application under test Type Optional Range user-defined (type: string)(pattern: \"label_key=label_value\") Default n/a Notes The applabel in the spec specifies a unique label of the AUT. Usually provided as a quoted string of pattern key=value. Note that if multiple applications share the same label within a given namespace, the AUT is filtered based on the presence of the chaos annotation litmuschaos.io/chaos: \"true\" . If, however, the annotationCheck is disabled, then a random application (pod) sharing the specified label is selected for chaos. It is optional for the infra chaos. Field .spec.appinfo.appkind Description Flag to specify resource kind of application under test Type Optional Range deployment , statefulset , daemonset , deploymentconfig , rollout Default n/a (depends on app type) Notes The appkind in the spec specifies the Kubernetes resource type of the app deployment. The Litmus ChaosOperator supports chaos on deployments, statefulsets and daemonsets. Application health check routines are dependent on the resource types, in case of some experiments. It is optional for the infra chaos Field .spec.auxiliaryAppInfo Description Flag to specify one or more app namespace-label pairs whose health is also monitored as part of the chaos experiment, in addition to a primary application specified in the .spec.appInfo . NOTE : If the auxiliary applications are deployed in namespaces other than the AUT, ensure that the chaosServiceAccount is bound to a cluster role and has adequate permissions to list pods on other namespaces. Type Optional Range user-defined (type: string)(pattern: \"namespace:label_key=label_value\"). Default n/a Notes The auxiliaryAppInfo in the spec specifies a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary. RBAC Specification \u00b6 View the RBAC specification schema Field .spec.chaosServiceAccount Description Flag to specify serviceaccount used for chaos experiment Type Mandatory Range user-defined (type: string) Default n/a Notes The chaosServiceAccount in the spec specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment is provided in the .spec.definition.permissions field of the respective chaosexperiment CR. Runtime Specification \u00b6 View the runtime specification schema Field .spec.annotationCheck Description Flag to control annotationChecks on applications as prerequisites for chaos Type Optional Range true , false Default true Notes The annotationCheck in the spec controls whether or not the operator checks for the annotation \"litmuschaos.io/chaos\" to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated, while setting it to false suppresses this check and proceeds with chaos injection. Field .spec.terminationGracePeriodSeconds Description Flag to control terminationGracePeriodSeconds for the chaos pods(abort case) Type Optional Range integer value Default 30 Notes The terminationGracePeriodSeconds in the spec controls the terminationGracePeriodSeconds for the chaos resources in abort case. Chaos pods contains chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before completely terminated. Field .spec.jobCleanUpPolicy Description Flag to control cleanup of chaos experiment job post execution of chaos Type Optional Range delete , retain Default delete Notes <The jobCleanUpPolicy controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). Runner Specification \u00b6 View the runner specification schema Field .spec.components.runner.image Description Flag to specify image of ChaosRunner pod Type Optional Range user-defined (type: string) Default n/a (refer Notes ) Notes The .components.runner.image allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE Field .spec.components.runner.imagePullPolicy Description Flag to specify imagePullPolicy for the ChaosRunner Type Optional Range Always , IfNotPresent Default IfNotPresent Notes The .components.runner.imagePullPolicy allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Field .spec.components.runner.imagePullSecrets Description Flag to specify imagePullSecrets for the ChaosRunner Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.imagePullSecrets allows developers to specify the imagePullSecret name for ChaosRunner. Field .spec.components.runner.runnerAnnotations Description Annotations that needs to be provided in the pod which will be created (runner-pod) Type Optional Range user-defined (type: map[string]string) Default n/a Notes The .components.runner.runnerAnnotation allows developers to specify the custom annotations for the runner pod. Field .spec.components.runner.args Description Specify the args for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.args allows developers to specify their own debug runner args. Field .spec.components.runner.command Description Specify the commands for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.command allows developers to specify their own debug runner commands. Field .spec.components.runner.configMaps Description Configmaps passed to the chaos runner pod Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.configMaps provides for a means to insert config information into the runner pod. Field .spec.components.runner.secrets Description Kubernetes secrets passed to the chaos runner pod. Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the chaos runner pod. These are especially useful in case of platform-level/infra-level chaos experiments. Field .spec.components.runner.nodeSelector Description Node selectors for the runner pod Type Optional Range Labels in the from of label key=value Default n/a Notes The .spec.components.runner.nodeSelector The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.components.runner.resources Description Specify the resource requirements for the ChaosRunner pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The .spec.components.runner.resources contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. Field .spec.components.runner.tolerations Description Toleration for the runner pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.runner.tolerations Provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. Experiment Specification \u00b6 View the experiment specification schema Field .spec.experiments[].spec.components.configMaps Description Configmaps passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.configMaps provides for a means to insert config information into the experiment. The configmaps definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.secrets Description Kubernetes secrets passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the experiment pods. These are especially useful in case of platform-level/infra-level chaos experiments. The secrets definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.experimentImage Description Override the image of the chaos experiment Type Optional Range string Default n/a Notes The experiment[].spec.components.experimentImage overrides the experiment image for the chaoexperiment. Field .spec.experiments[].spec.components.experimentImagePullSecrets Description Flag to specify imagePullSecrets for the ChaosExperiment Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.experimentImagePullSecrets allows developers to specify the imagePullSecret name for ChaosExperiment. Field .spec.experiments[].spec.components.nodeSelector Description Provide the node selector for the experiment pod Type Optional Range Labels in the from of label key=value Default n/a Notes The experiment[].spec.components.nodeSelector The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.experiments[].spec.components.statusCheckTimeouts Description Provides the timeout and retry values for the status checks. Defaults to 180s & 90 retries (2s per retry) Type Optional Range It contains values in the form {delay: int, timeout: int} Default delay: 2s and timeout: 180s Notes The experiment[].spec.components.statusCheckTimeouts The statusCheckTimeouts override the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. Field .spec.experiments[].spec.components.resources Description Specify the resource requirements for the ChaosExperiment pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The experiment[].spec.components.resources contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. Field .spec.experiments[].spec.components.experimentAnnotations Description Annotations that needs to be provided in the pod which will be created (experiment-pod) Type Optional Range user-defined (type: label key=value) Default n/a Notes The .spec.components.experimentAnnotation allows developers to specify the custom annotations for the experiment pod. Field .spec.experiments[].spec.components.tolerations Description Toleration for the experiment pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.tolerations Tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos.","title":"Contents"},{"location":"experiments/chaos-resources/chaos-engine/contents/#chaos-engine-specifications","text":"Bind an instance of a given app with one or more chaos experiments, define run characteristics, override chaos defaults, define steady-state hypothesis, reconciled by Litmus Chaos Operator. This section describes the fields in the ChaosEngine spec and the possible values that can be set against the same. Field Name Description User Guide State Specification It defines the state of the chaosengine State Specifications Application Specification It defines the details of AUT and auxiliary applications Application Specifications RBAC Specification It defines the chaos-service-account name RBAC Specifications Runtime Specification It defines the runtime details of the chaosengine Runtime Specifications Runner Specification It defines the runner pod specifications Runner Specifications Experiment Specification It defines the experiment pod specifications Experiment Specifications","title":"Chaos Engine Specifications"},{"location":"experiments/chaos-resources/chaos-engine/contents/#state-specification","text":"View the state specification schema Field .spec.engineState Description Flag to control the state of the chaosengine Type Mandatory Range active , stop Default active Notes The engineState in the spec is a user defined flag to trigger chaos. Setting it to active ensures successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on actual state of the ChaosEngine.","title":"State Specification"},{"location":"experiments/chaos-resources/chaos-engine/contents/#application-specification","text":"View the application specification schema Field .spec.appinfo.appns Description Flag to specify namespace of application under test Type Optional Range user-defined (type: string) Default n/a Notes The appns in the spec specifies the namespace of the AUT. Usually provided as a quoted string. It is optional for the infra chaos. Field .spec.appinfo.applabel Description Flag to specify unique label of application under test Type Optional Range user-defined (type: string)(pattern: \"label_key=label_value\") Default n/a Notes The applabel in the spec specifies a unique label of the AUT. Usually provided as a quoted string of pattern key=value. Note that if multiple applications share the same label within a given namespace, the AUT is filtered based on the presence of the chaos annotation litmuschaos.io/chaos: \"true\" . If, however, the annotationCheck is disabled, then a random application (pod) sharing the specified label is selected for chaos. It is optional for the infra chaos. Field .spec.appinfo.appkind Description Flag to specify resource kind of application under test Type Optional Range deployment , statefulset , daemonset , deploymentconfig , rollout Default n/a (depends on app type) Notes The appkind in the spec specifies the Kubernetes resource type of the app deployment. The Litmus ChaosOperator supports chaos on deployments, statefulsets and daemonsets. Application health check routines are dependent on the resource types, in case of some experiments. It is optional for the infra chaos Field .spec.auxiliaryAppInfo Description Flag to specify one or more app namespace-label pairs whose health is also monitored as part of the chaos experiment, in addition to a primary application specified in the .spec.appInfo . NOTE : If the auxiliary applications are deployed in namespaces other than the AUT, ensure that the chaosServiceAccount is bound to a cluster role and has adequate permissions to list pods on other namespaces. Type Optional Range user-defined (type: string)(pattern: \"namespace:label_key=label_value\"). Default n/a Notes The auxiliaryAppInfo in the spec specifies a (comma-separated) list of namespace-label pairs for downstream (dependent) apps of the primary app specified in .spec.appInfo in case of pod-level chaos experiments. In case of infra-level chaos experiments, this flag specifies those apps that may be directly impacted by chaos and upon which health checks are necessary.","title":"Application Specification"},{"location":"experiments/chaos-resources/chaos-engine/contents/#rbac-specification","text":"View the RBAC specification schema Field .spec.chaosServiceAccount Description Flag to specify serviceaccount used for chaos experiment Type Mandatory Range user-defined (type: string) Default n/a Notes The chaosServiceAccount in the spec specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment is provided in the .spec.definition.permissions field of the respective chaosexperiment CR.","title":"RBAC Specification"},{"location":"experiments/chaos-resources/chaos-engine/contents/#runtime-specification","text":"View the runtime specification schema Field .spec.annotationCheck Description Flag to control annotationChecks on applications as prerequisites for chaos Type Optional Range true , false Default true Notes The annotationCheck in the spec controls whether or not the operator checks for the annotation \"litmuschaos.io/chaos\" to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated, while setting it to false suppresses this check and proceeds with chaos injection. Field .spec.terminationGracePeriodSeconds Description Flag to control terminationGracePeriodSeconds for the chaos pods(abort case) Type Optional Range integer value Default 30 Notes The terminationGracePeriodSeconds in the spec controls the terminationGracePeriodSeconds for the chaos resources in abort case. Chaos pods contains chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before completely terminated. Field .spec.jobCleanUpPolicy Description Flag to control cleanup of chaos experiment job post execution of chaos Type Optional Range delete , retain Default delete Notes <The jobCleanUpPolicy controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms).","title":"Runtime Specification"},{"location":"experiments/chaos-resources/chaos-engine/contents/#runner-specification","text":"View the runner specification schema Field .spec.components.runner.image Description Flag to specify image of ChaosRunner pod Type Optional Range user-defined (type: string) Default n/a (refer Notes ) Notes The .components.runner.image allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE Field .spec.components.runner.imagePullPolicy Description Flag to specify imagePullPolicy for the ChaosRunner Type Optional Range Always , IfNotPresent Default IfNotPresent Notes The .components.runner.imagePullPolicy allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Field .spec.components.runner.imagePullSecrets Description Flag to specify imagePullSecrets for the ChaosRunner Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.imagePullSecrets allows developers to specify the imagePullSecret name for ChaosRunner. Field .spec.components.runner.runnerAnnotations Description Annotations that needs to be provided in the pod which will be created (runner-pod) Type Optional Range user-defined (type: map[string]string) Default n/a Notes The .components.runner.runnerAnnotation allows developers to specify the custom annotations for the runner pod. Field .spec.components.runner.args Description Specify the args for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.args allows developers to specify their own debug runner args. Field .spec.components.runner.command Description Specify the commands for the ChaosRunner Pod Type Optional Range user-defined (type: []string) Default n/a Notes The .components.runner.command allows developers to specify their own debug runner commands. Field .spec.components.runner.configMaps Description Configmaps passed to the chaos runner pod Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.configMaps provides for a means to insert config information into the runner pod. Field .spec.components.runner.secrets Description Kubernetes secrets passed to the chaos runner pod. Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The .spec.components.runner.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the chaos runner pod. These are especially useful in case of platform-level/infra-level chaos experiments. Field .spec.components.runner.nodeSelector Description Node selectors for the runner pod Type Optional Range Labels in the from of label key=value Default n/a Notes The .spec.components.runner.nodeSelector The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.components.runner.resources Description Specify the resource requirements for the ChaosRunner pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The .spec.components.runner.resources contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. Field .spec.components.runner.tolerations Description Toleration for the runner pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.runner.tolerations Provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos.","title":"Runner Specification"},{"location":"experiments/chaos-resources/chaos-engine/contents/#experiment-specification","text":"View the experiment specification schema Field .spec.experiments[].spec.components.configMaps Description Configmaps passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.configMaps provides for a means to insert config information into the experiment. The configmaps definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.secrets Description Kubernetes secrets passed to the chaos experiment Type Optional Range user-defined (type: {name: string, mountPath: string}) Default n/a Notes The experiment[].spec.components.secrets provides for a means to push secrets (typically project ids, access credentials etc.,) into the experiment pods. These are especially useful in case of platform-level/infra-level chaos experiments. The secrets definition is validated for correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Field .spec.experiments[].spec.components.experimentImage Description Override the image of the chaos experiment Type Optional Range string Default n/a Notes The experiment[].spec.components.experimentImage overrides the experiment image for the chaoexperiment. Field .spec.experiments[].spec.components.experimentImagePullSecrets Description Flag to specify imagePullSecrets for the ChaosExperiment Type Optional Range user-defined (type: []corev1.LocalObjectReference) Default n/a Notes The .components.runner.experimentImagePullSecrets allows developers to specify the imagePullSecret name for ChaosExperiment. Field .spec.experiments[].spec.components.nodeSelector Description Provide the node selector for the experiment pod Type Optional Range Labels in the from of label key=value Default n/a Notes The experiment[].spec.components.nodeSelector The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. Field .spec.experiments[].spec.components.statusCheckTimeouts Description Provides the timeout and retry values for the status checks. Defaults to 180s & 90 retries (2s per retry) Type Optional Range It contains values in the form {delay: int, timeout: int} Default delay: 2s and timeout: 180s Notes The experiment[].spec.components.statusCheckTimeouts The statusCheckTimeouts override the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. Field .spec.experiments[].spec.components.resources Description Specify the resource requirements for the ChaosExperiment pod Type Optional Range user-defined (type: corev1.ResourceRequirements) Default n/a Notes The experiment[].spec.components.resources contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. Field .spec.experiments[].spec.components.experimentAnnotations Description Annotations that needs to be provided in the pod which will be created (experiment-pod) Type Optional Range user-defined (type: label key=value) Default n/a Notes The .spec.components.experimentAnnotation allows developers to specify the custom annotations for the experiment pod. Field .spec.experiments[].spec.components.tolerations Description Toleration for the experiment pod Type Optional Range user-defined (type: []corev1.Toleration) Default n/a Notes The .spec.components.tolerations Tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos.","title":"Experiment Specification"},{"location":"experiments/chaos-resources/chaos-engine/engine-state/","text":"It is a user-defined flag to trigger chaos. Setting it to active ensures the successful execution of chaos. Patching it with stop aborts ongoing experiments. It has a corresponding flag in the chaosengine status field, called engineStatus which is updated by the controller based on the actual state of the ChaosEngine. It can be tuned via engineState field. It supports active and stop values. Use the following example to tune this: # contains the chaosengine state # supports: active and stop states apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : # contains the state of engine engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"State Specifications"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/","text":"It contains all the experiment tunables provided at .spec.experiments[].spec.components inside chaosengine. Experiment Annotations \u00b6 It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment Experiment Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. - configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" Experiment Image \u00b6 It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\" Experiment ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred Experiment NodeSelectors \u00b6 The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos Experiment Resource Requirements \u00b6 It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" Experiment Tolerations \u00b6 It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" Experiment Status Check Timeout \u00b6 It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Specifications"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-annotations","text":"It allows developers to specify the custom annotations for the experiment pod. It can be tuned via experimentAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # annotations for the experiment pod experimentAnnotations : name : chaos-experiment","title":"Experiment Annotations"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the experiment pod respectively. - configMaps : It provides for a means to insert config information into the experiment. The configmaps definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the experiment pods. These are especially useful in the case of platform-level/infra-level chaos experiments. The secrets definition is validated for the correctness and those specified are checked for availability (in the cluster/namespace) before being mounted into the experiment pods. Use the following example to tune this: # contains configmaps and secrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # configmaps details mounted to the experiment pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the experiment pod secrets : - name : \"secret-01\" mountPath : \"/tmp\"","title":"Experiment Configmaps And Secrets"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-image","text":"It overrides the experiment image for the chaosexperiment. It allows developers to specify the experiment image. It can be tuned via experimentImage field. Use the following example to tune this: # contains the custom image for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # override the image of the experiment pod experimentImage : \"litmuschaos/go-runner:ci\"","title":"Experiment Image"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for ChaosExperiment. It can be tuned via experimentImagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # secret name for the experiment image, if using private registry imagePullSecrets : - name : regcred","title":"Experiment ImagePullSecrets"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-nodeselectors","text":"The nodeselector contains labels of the node on which experiment pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the experiment pod # it will schedule the experiment pod on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # nodeselector for the experiment pod nodeSelector : context : chaos","title":"Experiment NodeSelectors"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-resource-requirements","text":"It contains the resource requirements for the ChaosExperiment Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the experiment pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # resource requirements for the experiment pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\"","title":"Experiment Resource Requirements"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-tolerations","text":"It provides tolerations for the experiment pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the experiment pod # it will schedule the experiment pod on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # tolerations for the experiment pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\"","title":"Experiment Tolerations"},{"location":"experiments/chaos-resources/chaos-engine/experiment-components/#experiment-status-check-timeout","text":"It overrides the status timeouts inside chaosexperiments. It contains timeout & delay in seconds. It can be tuned via statusCheckTimeouts field. Use the following example to tune this: # contains status check timeout for the experiment pod # it will set this timeout as upper bound while checking application status, node status in experiments apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : components : # status check timeout for the experiment pod statusCheckTimeouts : delay : 2 timeout : 180","title":"Experiment Status Check Timeout"},{"location":"experiments/chaos-resources/chaos-engine/rbac-details/","text":"It specifies the name of the serviceaccount mapped to a role/clusterRole with enough permissions to execute the desired chaos experiment. The minimum permissions needed for any given experiment are provided in the .spec.definition.permissions field of the respective chaosexperiment CR. It can be tuned via chaosServiceAccount field. Use the following example to tune this: # contains name of the serviceAccount which contains all the RBAC permissions required for the experiment apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" # name of the service account w/ sufficient permissions chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"RBAC Specifications"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/","text":"It contains all the chaos-runner tunables provided at .spec.components.runner inside chaosengine. ChaosRunner Annotations \u00b6 It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Args And Command \u00b6 It defines the args and command to set the args and command of the chaos-runner respectively. - args : It allows developers to specify their own debug runner args. - command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Configmaps And Secrets \u00b6 It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. - configMaps : It provides for a means to insert config information into the runner pod. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Image and ImagePullPoicy \u00b6 It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. - image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . - imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner ImagePullSecrets \u00b6 It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner NodeSelectors \u00b6 The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Resource Requirements \u00b6 It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete ChaosRunner Tolerations \u00b6 It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runner Specifications"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-annotations","text":"It allows developers to specify the custom annotations for the runner pod. It can be tuned via runnerAnnotations field. Use the following example to tune this: # contains annotations for the chaos runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # annotations for the chaos-runner runnerAnnotations : name : chaos-runner appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Annotations"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-args-and-command","text":"It defines the args and command to set the args and command of the chaos-runner respectively. - args : It allows developers to specify their own debug runner args. - command : It allows developers to specify their own debug runner commands. Use the following example to tune this: # contains args and command for the chaos runner # it will be useful for the cases where custom image of the chaos-runner is used, which supports args and commands apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : # override the args and command for the chaos-runner runner : # name of the custom image image : \"<your repo>/chaos-runner:ci\" # args for the image args : - \"/bin/sh\" # command for the image command : - \"-c\" - \"<custom-command>\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Args And Command"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-configmaps-and-secrets","text":"It defines the configMaps and secrets to set the configmaps and secrets mounted to the chaos-runner respectively. - configMaps : It provides for a means to insert config information into the runner pod. - secrets : It provides for a means to push secrets (typically project ids, access credentials, etc.,) into the chaos runner pod. These are especially useful in the case of platform-level/infra-level chaos experiments. Use the following example to tune this: # contains configmaps and secrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # configmaps details mounted to the runner pod configMaps : - name : \"configmap-01\" mountPath : \"/mnt\" # secrets details mounted to the runner pod secrets : - name : \"secret-01\" mountPath : \"/tmp\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Configmaps And Secrets"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-image-and-imagepullpoicy","text":"It defines the image and imagePullPolicy to set the image and imagePullPolicy for the chaos-runner respectively. - image : It allows developers to specify their own debug runner images. Defaults for the runner image can be enforced via the operator env CHAOS_RUNNER_IMAGE . - imagePullPolicy : It allows developers to specify the pull policy for chaos-runner. Set to Always during debug/test. Use the following example to tune this: # contains the image and imagePullPolicy of the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # override the image of the chaos-runner # by default it is used the image based on the litmus version image : \"litmuschaos/chaos-runner:latest\" # imagePullPolicy for the runner image # supports: Always, IfNotPresent. default: IfNotPresent imagePullPolicy : \"Always\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Image and ImagePullPoicy"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-imagepullsecrets","text":"It allows developers to specify the imagePullSecret name for the ChaosRunner. It can be tuned via imagePullSecrets field. Use the following example to tune this: # contains the imagePullSecrets for the chaos-runner apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # secret name for the runner image, if using private registry imagePullSecrets : - name : regcred appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner ImagePullSecrets"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-nodeselectors","text":"The nodeselector contains labels of the node on which runner pod should be scheduled. Typically used in case of infra/node level chaos. It can be tuned via nodeSelector field. Use the following example to tune this: # contains the node-selector for the chaos-runner # it will schedule the chaos-runner on the coresponding node with matching labels apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # nodeselector for the runner pod nodeSelector : context : chaos appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner NodeSelectors"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-resource-requirements","text":"It contains the resource requirements for the ChaosRunner Pod, where we can provide resource requests and limits for the pod. It can be tuned via resources field. Use the following example to tune this: # contains the resource requirements for the runner pod apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # resource requirements for the runner pod resources : requests : cpu : \"250m\" memory : \"64Mi\" limits : cpu : \"500m\" memory : \"128Mi\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Resource Requirements"},{"location":"experiments/chaos-resources/chaos-engine/runner-components/#chaosrunner-tolerations","text":"It provides tolerations for the runner pod so that it can be scheduled on the respective tainted node. Typically used in case of infra/node level chaos. It can be tuned via tolerations field. Use the following example to tune this: # contains the tolerations for the chaos-runner # it will schedule the chaos-runner on the tainted node apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" components : runner : # tolerations for the runner pod tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"Schedule\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"ChaosRunner Tolerations"},{"location":"experiments/chaos-resources/chaos-engine/runtime-details/","text":"Annotation Check \u00b6 It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Jobcleanup Policy \u00b6 It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanupPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete Termination Grace Period Seconds \u00b6 It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Runtime Specifications"},{"location":"experiments/chaos-resources/chaos-engine/runtime-details/#annotation-check","text":"It controls whether or not the operator checks for the annotation litmuschaos.io/chaos to be set against the application under test (AUT). Setting it to true ensures the check is performed, with chaos being skipped if the app is not annotated while setting it to false suppresses this check and proceeds with chaos injection. It can be tuned via annotationCheck field. It supports the boolean value and the default value is false . Use the following example to tune this: # checks the AUT for the annoations. The AUT should be annotated with `litmuschaos.io/chaos: true` if provided as true # supports: true, false. default: false apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" # annotaionCheck details annotationCheck : \"true\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Annotation Check"},{"location":"experiments/chaos-resources/chaos-engine/runtime-details/#jobcleanup-policy","text":"It controls whether or not the experiment pods are removed once execution completes. Set to retain for debug purposes (in the absence of standard logging mechanisms). It can be tuned via jobCleanupPolicy fields. It supports retain and delete . The default value is retain . Use the following example to tune this: # flag to delete or retain the chaos resources after completions of chaosengine # supports: delete, retain. default: retain apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" jobCleanupPolicy : \"delete\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Jobcleanup Policy"},{"location":"experiments/chaos-resources/chaos-engine/runtime-details/#termination-grace-period-seconds","text":"It controls the terminationGracePeriodSeconds for the chaos resources in the abort case. Chaos pods contain chaos revert upon abortion steps, which continuously looking for the termination signals. The terminationGracePeriodSeconds should be provided in such a way that the chaos pods got enough time for the revert before being completely terminated. It can be tuned via terminationGracePeriodSeconds field. Use the following example to tune this: # contains flag to control the terminationGracePeriodSeconds for the chaos pod(abort case) apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" annotationCheck : \"false\" # contains terminationGracePeriodSeconds for the chaos pods terminationGracePeriodSeconds : 100 appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete","title":"Termination Grace Period Seconds"},{"location":"experiments/chaos-resources/chaos-experiment/contents/","text":"Chaos Experiment Specifications \u00b6 Granular definition of chaos intent specified via image, librar, necessary permissions, low-level chaos parameters (default values). This section describes the fields in the ChaosExperiment and the possible values that can be set against the same. Scope Specification \u00b6 View the scope schema Field .spec.definition.scope Description Flag to specify the scope of the ChaosExperiment Type Optional Range Namespaced , Cluster Default n/a (depends on experiment type) Notes The .spec.definition.scope specifies the scope of the experiment. It can be Namespaced scope for pod level experiments and Cluster for the experiments having a cluster wide impact. Field .spec.definition.permissions Description Flag to specify the minimum permission to run the ChaosExperiment Type Optional Range user-defined (type: list) Default n/a Notes The .spec.definition.permissions specify the minimum permission that is required to run the ChaosExperiment. It also helps to estimate the blast radius for the ChaosExperiment. Component Specification \u00b6 View the component schema Field .spec.definition.image Description Flag to specify the image to run the ChaosExperiment Type Mandatory Range user-defined (type: string) Default n/a (refer Notes) Notes The .spec.definition.image allows the developers to specify their experiment images. Typically set to the Litmus go-runner or the ansible-runner . This feature of the experiment enables BYOC (BringYourOwnChaos), where developers can implement their own variants of a standard chaos experiment Field .spec.definition.imagePullPolicy Description Flag that helps the developers to specify imagePullPolicy for the ChaosExperiment Type Mandatory Range IfNotPresent , Always (type: string) Default Always Notes The .spec.definition.imagePullPolicy allows developers to specify the pull policy for ChaosExperiment image. Set to Always during debug/test Field .spec.definition.args Description Flag to specify the entrypoint for the ChaosExperiment Type Mandatory Range user-defined (type:list of string) Default n/a Notes The .spec.definition.args specifies the entrypoint for the ChaosExperiment. It depends on the language used in the experiment. For litmus-go the .spec.definition.args contains a single binary of all experiments and managed via -name flag to indicate experiment to run( -name (exp-name) ). Field .spec.definition.command Description Flag to specify the shell on which the ChaosExperiment will execute Type Mandatory Range user-defined (type: list of string). Default /bin/bash Notes The .spec.definition.command specifies the shell used to run the experiment /bin/bash is the most common shell to be used. Experiment Tunables Specification \u00b6 View the experiment tunables Field .spec.definition.env Description Flag to specify env used for ChaosExperiment Type Mandatory Range user-defined (type: {name: string, value: string}) Default n/a Notes The .spec.definition.env specifies the array of tunables passed to the experiment pods as environment variables. It is used to manage the experiment execution. We can set the default values for all the variables (tunable) here which can be overridden by ChaosEngine from .spec.experiments[].spec.components.env if required. To know about the variables that need to be overridden check the list of \"mandatory\" & \"optional\" env for an experiment as provided within the respective experiment documentation. Configuration Specification \u00b6 View the configuration schema Field .spec.definition.securityContext.containerSecurityContext.privileged Description Flag to specify the security context for the ChaosExperiment pod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.securityContext.containerSecurityContext.privileged specify the securityContext params to the experiment container. Field .spec.definition.labels Description Flag to specify the label for the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.labels allow developers to specify the ChaosPod label for an experiment. Field .spec.definition.securityContext.podSecurityContext Description Flag to specify security context for ChaosPod Type Optional Range user-defined (type:corev1.PodSecurityContext) Default n/a Notes The .spec.definition.securityContext.podSecurityContext allows the developers to specify the security context for the ChaosPod which applies to all containers inside the Pod. Field .spec.definition.configMaps Description Flag to specify the configmap for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.configMaps allows the developers to mount the ConfigMap volume into the experiment pod. Field .spec.definition.secrets Description Flag to specify the secrets for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.secrets specify the secret data to be passed for the ChaosPod. The secrets typically contains confidential information like credentials. Field .spec.definition.experimentAnnotations Description Flag to specify the custom annotation to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.experimentAnnotations allows the developer to specify the Custom annotation for the chaos pod. Field .spec.definition.hostFileVolumes Description Flag to specify the host file volumes to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.hostFileVolumes allows the developer to specify the host file volumes to the ChaosPod. Field .spec.definition.hostPID Description Flag to specify the host PID for the ChaosPod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.hostPID allows the developer to specify the host PID for the ChaosPod.","title":"Contents"},{"location":"experiments/chaos-resources/chaos-experiment/contents/#chaos-experiment-specifications","text":"Granular definition of chaos intent specified via image, librar, necessary permissions, low-level chaos parameters (default values). This section describes the fields in the ChaosExperiment and the possible values that can be set against the same.","title":"Chaos Experiment Specifications"},{"location":"experiments/chaos-resources/chaos-experiment/contents/#scope-specification","text":"View the scope schema Field .spec.definition.scope Description Flag to specify the scope of the ChaosExperiment Type Optional Range Namespaced , Cluster Default n/a (depends on experiment type) Notes The .spec.definition.scope specifies the scope of the experiment. It can be Namespaced scope for pod level experiments and Cluster for the experiments having a cluster wide impact. Field .spec.definition.permissions Description Flag to specify the minimum permission to run the ChaosExperiment Type Optional Range user-defined (type: list) Default n/a Notes The .spec.definition.permissions specify the minimum permission that is required to run the ChaosExperiment. It also helps to estimate the blast radius for the ChaosExperiment.","title":"Scope Specification"},{"location":"experiments/chaos-resources/chaos-experiment/contents/#component-specification","text":"View the component schema Field .spec.definition.image Description Flag to specify the image to run the ChaosExperiment Type Mandatory Range user-defined (type: string) Default n/a (refer Notes) Notes The .spec.definition.image allows the developers to specify their experiment images. Typically set to the Litmus go-runner or the ansible-runner . This feature of the experiment enables BYOC (BringYourOwnChaos), where developers can implement their own variants of a standard chaos experiment Field .spec.definition.imagePullPolicy Description Flag that helps the developers to specify imagePullPolicy for the ChaosExperiment Type Mandatory Range IfNotPresent , Always (type: string) Default Always Notes The .spec.definition.imagePullPolicy allows developers to specify the pull policy for ChaosExperiment image. Set to Always during debug/test Field .spec.definition.args Description Flag to specify the entrypoint for the ChaosExperiment Type Mandatory Range user-defined (type:list of string) Default n/a Notes The .spec.definition.args specifies the entrypoint for the ChaosExperiment. It depends on the language used in the experiment. For litmus-go the .spec.definition.args contains a single binary of all experiments and managed via -name flag to indicate experiment to run( -name (exp-name) ). Field .spec.definition.command Description Flag to specify the shell on which the ChaosExperiment will execute Type Mandatory Range user-defined (type: list of string). Default /bin/bash Notes The .spec.definition.command specifies the shell used to run the experiment /bin/bash is the most common shell to be used.","title":"Component Specification"},{"location":"experiments/chaos-resources/chaos-experiment/contents/#experiment-tunables-specification","text":"View the experiment tunables Field .spec.definition.env Description Flag to specify env used for ChaosExperiment Type Mandatory Range user-defined (type: {name: string, value: string}) Default n/a Notes The .spec.definition.env specifies the array of tunables passed to the experiment pods as environment variables. It is used to manage the experiment execution. We can set the default values for all the variables (tunable) here which can be overridden by ChaosEngine from .spec.experiments[].spec.components.env if required. To know about the variables that need to be overridden check the list of \"mandatory\" & \"optional\" env for an experiment as provided within the respective experiment documentation.","title":"Experiment Tunables Specification"},{"location":"experiments/chaos-resources/chaos-experiment/contents/#configuration-specification","text":"View the configuration schema Field .spec.definition.securityContext.containerSecurityContext.privileged Description Flag to specify the security context for the ChaosExperiment pod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.securityContext.containerSecurityContext.privileged specify the securityContext params to the experiment container. Field .spec.definition.labels Description Flag to specify the label for the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.labels allow developers to specify the ChaosPod label for an experiment. Field .spec.definition.securityContext.podSecurityContext Description Flag to specify security context for ChaosPod Type Optional Range user-defined (type:corev1.PodSecurityContext) Default n/a Notes The .spec.definition.securityContext.podSecurityContext allows the developers to specify the security context for the ChaosPod which applies to all containers inside the Pod. Field .spec.definition.configMaps Description Flag to specify the configmap for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.configMaps allows the developers to mount the ConfigMap volume into the experiment pod. Field .spec.definition.secrets Description Flag to specify the secrets for ChaosPod Type Optional Range user-defined Default n/a Notes The .spec.definition.secrets specify the secret data to be passed for the ChaosPod. The secrets typically contains confidential information like credentials. Field .spec.definition.experimentAnnotations Description Flag to specify the custom annotation to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.experimentAnnotations allows the developer to specify the Custom annotation for the chaos pod. Field .spec.definition.hostFileVolumes Description Flag to specify the host file volumes to the ChaosPod Type Optional Range user-defined (type:map[string]string) Default n/a Notes The .spec.definition.hostFileVolumes allows the developer to specify the host file volumes to the ChaosPod. Field .spec.definition.hostPID Description Flag to specify the host PID for the ChaosPod Type Optional Range true, false (type:bool) Default n/a Notes The .spec.definition.hostPID allows the developer to specify the host PID for the ChaosPod.","title":"Configuration Specification"},{"location":"experiments/chaos-resources/chaos-result/contents/","text":"Chaos Result Specifications \u00b6 Hold engine reference, experiment state, verdict(on complete), salient application/result attributes, sources for metrics collection This section describes the fields in the ChaosResult and the possible values that can be set against the same. Component Details \u00b6 View the components schema Field .spec.engine Description Flag to hold the ChaosEngine name for the experiment Type Optional Range n/a (type: string) Notes The .spec.engine holds the engine name for the current course of the experiment. Field .spec.experiment Description Flag to hold the ChaosExperiment name which induces chaos. Type Optional Range n/a (type: string) Notes The .spec.experiment holds the ChaosExperiment name for the current course of the experiment. Status Details \u00b6 View the status schema Field .status.experimentStatus.failstep Description Flag to show the failure step of the ChaosExperiment Type Mandatory Range n/a (type: string) Notes The .status.experimentStatus.failstep Show the step at which the experiment failed. It helps in faster debugging of failures in the experiment execution. Field .status.experimentStatus.phase Description Flag to show the current phase of the experiment Type Mandatory Range Awaited,Running,Completed,Aborted (type: string) Notes The .status.experimentStatus.phase shows the current phase in which the experiment is. It gets updated as the experiment proceeds.If the experiment is aborted then the status will be Aborted. Field .status.experimentStatus.probesuccesspercentage Description Flag to show the probe success percentage Type Mandatory Range 1 to 100 (type: int) Notes The .status.experimentStatus.probesuccesspercentage shows the probe success percentage which is a ratio of successful checks v/s total probes. Field .status.experimentStatus.verdict Description Flag to show the verdict of the experiment. Type Mandatory Range Awaited,Pass,Fail,Stopped (type: string) Notes The .status.experimentStatus.verdict shows the verdict of the experiment. It is Awaited when the experiment is in progress and ends up with Pass or Fail according to the experiment result. Field .status.history.passedRuns Description It contains cumulative passed run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.passedRuns contains cumulative passed run counts for a specific ChaosResult. Field .status.history.failedRuns Description It contains cumulative failed run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.failedRuns contains cumulative failed run counts for a specific ChaosResult. Field .status.history.stoppedRuns Description It contains cumulative stopped run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.stoppedRuns contains cumulative stopped run counts for a specific ChaosResult. Probe Details \u00b6 View the probe schema Field .status.probestatus.name Description Flag to show the name of probe used in the experiment Type Mandatory Range n/a n/a (type: string) Notes The .status.probestatus.name shows the name of the probe used in the experiment. Field .status.probestatus.status.continuous Description Flag to show the result of probe in continuous mode Type Optional Range Awaited,Passed,Better Luck Next Time (type: string) Notes The .status.probestatus.status.continuous helps to get the result of the probe in the continuous mode. The httpProbe is better used in the Continuous mode. Field .status.probestatus.status.postchaos Description Flag to show the probe result post chaos Type Optional Range Awaited,Passed,Better Luck Next Time (type:map[string]string) Notes The .status.probestatus.status.postchaos shows the result of probe setup in EOT mode executed at the End of Test as a post-chaos check. Field .status.probestatus.status.prechaos Description Flag to show the probe result pre chaos Range Awaited,Passed,Better Luck Next Time (type:string) Notes The .status.probestatus.status.prechaos shows the result of probe setup in SOT mode executed at the Start of Test as a pre-chaos check. Field .status.probestatus.type Description Flag to show the type of probe used Range HTTPProbe,K8sProbe,CmdProbe (type:string) Notes The .status.probestatus.type shows the type of probe used.","title":"Contents"},{"location":"experiments/chaos-resources/chaos-result/contents/#chaos-result-specifications","text":"Hold engine reference, experiment state, verdict(on complete), salient application/result attributes, sources for metrics collection This section describes the fields in the ChaosResult and the possible values that can be set against the same.","title":"Chaos Result Specifications"},{"location":"experiments/chaos-resources/chaos-result/contents/#component-details","text":"View the components schema Field .spec.engine Description Flag to hold the ChaosEngine name for the experiment Type Optional Range n/a (type: string) Notes The .spec.engine holds the engine name for the current course of the experiment. Field .spec.experiment Description Flag to hold the ChaosExperiment name which induces chaos. Type Optional Range n/a (type: string) Notes The .spec.experiment holds the ChaosExperiment name for the current course of the experiment.","title":"Component Details"},{"location":"experiments/chaos-resources/chaos-result/contents/#status-details","text":"View the status schema Field .status.experimentStatus.failstep Description Flag to show the failure step of the ChaosExperiment Type Mandatory Range n/a (type: string) Notes The .status.experimentStatus.failstep Show the step at which the experiment failed. It helps in faster debugging of failures in the experiment execution. Field .status.experimentStatus.phase Description Flag to show the current phase of the experiment Type Mandatory Range Awaited,Running,Completed,Aborted (type: string) Notes The .status.experimentStatus.phase shows the current phase in which the experiment is. It gets updated as the experiment proceeds.If the experiment is aborted then the status will be Aborted. Field .status.experimentStatus.probesuccesspercentage Description Flag to show the probe success percentage Type Mandatory Range 1 to 100 (type: int) Notes The .status.experimentStatus.probesuccesspercentage shows the probe success percentage which is a ratio of successful checks v/s total probes. Field .status.experimentStatus.verdict Description Flag to show the verdict of the experiment. Type Mandatory Range Awaited,Pass,Fail,Stopped (type: string) Notes The .status.experimentStatus.verdict shows the verdict of the experiment. It is Awaited when the experiment is in progress and ends up with Pass or Fail according to the experiment result. Field .status.history.passedRuns Description It contains cumulative passed run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.passedRuns contains cumulative passed run counts for a specific ChaosResult. Field .status.history.failedRuns Description It contains cumulative failed run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.failedRuns contains cumulative failed run counts for a specific ChaosResult. Field .status.history.stoppedRuns Description It contains cumulative stopped run count Type Mandatory Range ANY NON NEGATIVE INTEGER Notes The .status.history.stoppedRuns contains cumulative stopped run counts for a specific ChaosResult.","title":"Status Details"},{"location":"experiments/chaos-resources/chaos-result/contents/#probe-details","text":"View the probe schema Field .status.probestatus.name Description Flag to show the name of probe used in the experiment Type Mandatory Range n/a n/a (type: string) Notes The .status.probestatus.name shows the name of the probe used in the experiment. Field .status.probestatus.status.continuous Description Flag to show the result of probe in continuous mode Type Optional Range Awaited,Passed,Better Luck Next Time (type: string) Notes The .status.probestatus.status.continuous helps to get the result of the probe in the continuous mode. The httpProbe is better used in the Continuous mode. Field .status.probestatus.status.postchaos Description Flag to show the probe result post chaos Type Optional Range Awaited,Passed,Better Luck Next Time (type:map[string]string) Notes The .status.probestatus.status.postchaos shows the result of probe setup in EOT mode executed at the End of Test as a post-chaos check. Field .status.probestatus.status.prechaos Description Flag to show the probe result pre chaos Range Awaited,Passed,Better Luck Next Time (type:string) Notes The .status.probestatus.status.prechaos shows the result of probe setup in SOT mode executed at the Start of Test as a pre-chaos check. Field .status.probestatus.type Description Flag to show the type of probe used Range HTTPProbe,K8sProbe,CmdProbe (type:string) Notes The .status.probestatus.type shows the type of probe used.","title":"Probe Details"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/","text":"Chaos Scheduler Specifications \u00b6 Hold attributes for repeated execution (run now, once@timestamp, b/w start-end timestamp@ interval). Embeds the ChaosEngine as template This section describes the fields in the ChaosScheduler and the possible values that can be set against the same. Schedule NOW \u00b6 View the schedule now schema Field .spec.schedule.now Description Flag to control the type of scheduling Type Mandatory Range true , false Default n/a Notes The now in the spec.schedule ensures immediate creation of chaosengine, i.e., injection of chaos. Schedule Once \u00b6 View the schedule once schema Field .spec.schedule.once.executionTime Description Flag to specify execution timestamp at which chaos is injected, when the policy is once . The chaosengine is created exactly at this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes .spec.schedule.once refers to a single-instance execution of chaos at a particular timestamp specified by .spec.schedule.once.executionTime Schedule Repeat \u00b6 View the schedule repeat schema Field .spec.schedule.repeat.timeRange.startTime Description Flag to specify start timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created before this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When startTime is specified against the policy repeat , ChaosEngine will not be formed before this time, no matter when it was created. Field .spec.schedule.repeat.timeRange.endTime Description Flag to specify end timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created after this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When endTime is specified against the policy repeat , ChaosEngine will not be formed after this time. Field .spec.schedule.repeat.properties.minChaosInterval Description Flag to specify the minimum interval between two chaosengines to be formed. Type Mandatory Range user-defined (type: string)(pattern: \"{number}m\", \"{number}h\"). Default n/a Notes The minChaosInterval in the spec specifies a time interval that must be taken care of while repeatedly forming the chaosengines i.e. This much duration of time should be there as interval between the formation of two chaosengines. Field .spec.schedule.repeat.workDays.includedDays Description Flag to specify the days at which chaos is allowed to take place Type Mandatory Range user-defined (type: string)(pattern: [{day_name},{day_name}...]). Default n/a Notes The includedDays in the spec specifies a (comma-separated) list of days of the week at which chaos is allowed to take place. {day_name} is to be specified with the first 3 letters of the name of day such as Mon , Tue etc. Field .spec.schedule.repeat.workHours.includedHours Description Flag to specify the hours at which chaos is allowed to take place Type Mandatory Range {hour_number} will range from 0 to 23 (type: string)(pattern: {hour_number}-{hour_number}). Default n/a Notes The includedHours in the spec specifies a range of hours of the day at which chaos is allowed to take place. 24 hour format is followed Engine Specification \u00b6 View the engine details Field .spec.engineTemplateSpec Description Flag to control chaosengine to be formed Type Mandatory Range n/a Default n/a Notes The engineTemplateSpec is the ChaosEngineSpec of ChaosEngine that is to be formed. State Specification \u00b6 View the state schema Field .spec.scheduleState Description Flag to control chaosshedule state Type Optional Range active , halt , complete Default active Notes The scheduleState is the current state of ChaosSchedule. If the schedule is running its state will be active , if the schedule is halted its state will be halt and if the schedule is completed it state will be complete .","title":"Contents"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#chaos-scheduler-specifications","text":"Hold attributes for repeated execution (run now, once@timestamp, b/w start-end timestamp@ interval). Embeds the ChaosEngine as template This section describes the fields in the ChaosScheduler and the possible values that can be set against the same.","title":"Chaos Scheduler Specifications"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#schedule-now","text":"View the schedule now schema Field .spec.schedule.now Description Flag to control the type of scheduling Type Mandatory Range true , false Default n/a Notes The now in the spec.schedule ensures immediate creation of chaosengine, i.e., injection of chaos.","title":"Schedule NOW"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#schedule-once","text":"View the schedule once schema Field .spec.schedule.once.executionTime Description Flag to specify execution timestamp at which chaos is injected, when the policy is once . The chaosengine is created exactly at this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes .spec.schedule.once refers to a single-instance execution of chaos at a particular timestamp specified by .spec.schedule.once.executionTime","title":"Schedule Once"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#schedule-repeat","text":"View the schedule repeat schema Field .spec.schedule.repeat.timeRange.startTime Description Flag to specify start timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created before this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When startTime is specified against the policy repeat , ChaosEngine will not be formed before this time, no matter when it was created. Field .spec.schedule.repeat.timeRange.endTime Description Flag to specify end timestamp of the range within which chaos is injected, when the policy is repeat . The chaosengine is not created after this timestamp. Type Mandatory Range user-defined (type: UTC Timeformat) Default n/a Notes When endTime is specified against the policy repeat , ChaosEngine will not be formed after this time. Field .spec.schedule.repeat.properties.minChaosInterval Description Flag to specify the minimum interval between two chaosengines to be formed. Type Mandatory Range user-defined (type: string)(pattern: \"{number}m\", \"{number}h\"). Default n/a Notes The minChaosInterval in the spec specifies a time interval that must be taken care of while repeatedly forming the chaosengines i.e. This much duration of time should be there as interval between the formation of two chaosengines. Field .spec.schedule.repeat.workDays.includedDays Description Flag to specify the days at which chaos is allowed to take place Type Mandatory Range user-defined (type: string)(pattern: [{day_name},{day_name}...]). Default n/a Notes The includedDays in the spec specifies a (comma-separated) list of days of the week at which chaos is allowed to take place. {day_name} is to be specified with the first 3 letters of the name of day such as Mon , Tue etc. Field .spec.schedule.repeat.workHours.includedHours Description Flag to specify the hours at which chaos is allowed to take place Type Mandatory Range {hour_number} will range from 0 to 23 (type: string)(pattern: {hour_number}-{hour_number}). Default n/a Notes The includedHours in the spec specifies a range of hours of the day at which chaos is allowed to take place. 24 hour format is followed","title":"Schedule Repeat"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#engine-specification","text":"View the engine details Field .spec.engineTemplateSpec Description Flag to control chaosengine to be formed Type Mandatory Range n/a Default n/a Notes The engineTemplateSpec is the ChaosEngineSpec of ChaosEngine that is to be formed.","title":"Engine Specification"},{"location":"experiments/chaos-resources/chaos-scheduler/contents/#state-specification","text":"View the state schema Field .spec.scheduleState Description Flag to control chaosshedule state Type Optional Range active , halt , complete Default active Notes The scheduleState is the current state of ChaosSchedule. If the schedule is running its state will be active , if the schedule is halted its state will be halt and if the schedule is completed it state will be complete .","title":"State Specification"},{"location":"experiments/chaos-resources/probes/cmdProbe/","text":"The command probe allows developers to run shell commands and match the resulting output as part of the entry/exit criteria. The intent behind this probe was to allow users to implement a non-standard & imperative way of expressing their hypothesis. For example, the cmdProbe enables you to check for specific data within a database, parse the value out of a JSON blob being dumped into a certain path, or check for the existence of a particular string in the service logs. It can be executed by setting type as cmdProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Inline Mode \u00b6 In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5 Source Mode \u00b6 In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Command Probe"},{"location":"experiments/chaos-resources/probes/cmdProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/cmdProbe/#inline-mode","text":"In inline mode, the command probe is executed from within the experiment image. It is preferred for simple shell commands. It can be tuned by setting source as inline . Use the following example to tune this: # execute the command inside the experiment pod itself # cases where command doesn't need any extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"inline\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Inline Mode"},{"location":"experiments/chaos-resources/probes/cmdProbe/#source-mode","text":"In source mode, the command execution is carried out from within a new pod whose image can be specified. It can be used when application-specific binaries are required. It can be tuned by setting source as <source-image> . Use the following example to tune this: # it launches the external pod with the source image and run the command inside the same pod # cases where command needs an extra binaries which is not available in litmsuchaos/go-runner image apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-database-integrity\" type : \"cmdProbe\" cmdProbe/inputs : # command which needs to run in cmdProbe command : \"<command>\" comparator : # output type for the above command # supports: string, int, float type : \"string\" # criteria which should be followed by the actual output and the expected output #supports [>=, <=, >, <, ==, !=] for int and float # supports [contains, equal, notEqual, matches, notMatches] for string values criteria : \"contains\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" # source for the cmdProbe # it can be \u201cinline\u201d or any image source : \"<source-image>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 initialDelaySeconds : 5","title":"Source Mode"},{"location":"experiments/chaos-resources/probes/contents/","text":"Probes Specifications \u00b6 Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe Basic Details \u00b6 View the basic schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .data Description Flag to hold the data for the create operation of the k8sProbe Type Optional Range n/a {type: string} Notes The .data contains the manifest/data for the resource, which need to be created. It supported for create operation of k8sProbe only Command Probe \u00b6 View the command probe schema Field .cmdProbe/inputs.command Description Flag to hold the command for the cmdProbe Type Mandatory Range n/a {type: string} Notes The .cmdProbe/inputs.command contains the shell command, which should be run as part of cmdProbe Field .cmdProbe/inputs.source Description Flag to hold the source for the cmdProbe Type Mandatory Range inline , any source docker image Notes The .cmdProbe/inputs.source It supports inline value when command can be run from within the experiment image. Otherwise provide the source image which can be used to launch a external pod where the command execution is carried out. HTTP Probe \u00b6 View the http probe schema Field .httpProbe/inputs.url Description Flag to hold the URL for the httpProbe Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.url contains the URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. Field .httpProbe/inputs.insecureSkipVerify Description Flag to hold the flag to skip certificate checks for the httpProbe Type Optional Range true , false Notes The .httpProbe/inputs.insecureSkipVerify contains flag to skip certificate checks. Field .httpProbe/inputs.responseTimeout Description Flag to hold the flag to response timeout for the httpProbe Type Optional Range n/a {type: integer} Notes The .httpProbe/inputs.responseTimeout contains flag to provide the response timeout for the http Get/Post request. Field .httpProbe/inputs.method.get.criteria Description Flag to hold the criteria for the http get request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.get.criteria contains criteria to match the http get request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.get.responseCode Description Flag to hold the expected response code for the get request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.get.responseCode contains the expected response code for the http get request as part of httpProbe run Field .httpProbe/inputs.method.post.contentType Description Flag to hold the content type of the post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.contentType contains the content type of the http body data, which need to be passed for the http post request Field .httpProbe/inputs.method.post.body Description Flag to hold the body of the http post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.body contains the http body, which is required for the http post request. It is used for the simple http body. If the http body is complex then use .httpProbe/inputs.method.post.bodyPath field. Field .httpProbe/inputs.method.post.bodyPath Description Flag to hold the path of the http body, required for the http post request Type Optional Range n/a {type: string} Notes The .httpProbe/inputs.method.post.bodyPath This field is used in case of complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. Field .httpProbe/inputs.method.post.criteria Description Flag to hold the criteria for the http post request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.post.criteria contains criteria to match the http post request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.post.responseCode Description Flag to hold the expected response code for the post request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.post.responseCode contains the expected response code for the http post request as part of httpProbe run K8S Probe \u00b6 View the k8s probe schema Field .k8sProbe/inputs.group Description Flag to hold the group of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.group contains group of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.version Description Flag to hold the apiVersion of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.version contains apiVersion of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.resource Description Flag to hold the kubernetes resource name for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.resource contains the kubernetes resource name on which k8sProbe performs the specified operation Field .k8sProbe/inputs.namespace Description Flag to hold the namespace of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.namespace contains namespace of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.fieldSelector Description Flag to hold the fieldSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.fieldSelector contains fieldSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.labelSelector Description Flag to hold the labelSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.labelSelector contains labelSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.operation Description Flag to hold the operation type for the k8sProbe Type Mandatory Range create , delete , present , absent Notes The .k8sProbe/inputs.operation contains operation which should be applied on the kubernetes resource as part of k8sProbe. It supports four type of operation. It can be one of create , delete , present , absent . Prometheus Probe \u00b6 View the prometheus probe schema Field .promProbe/inputs.endpoint Description Flag to hold the prometheus endpoints for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.endpoint contains the prometheus endpoints Field .promProbe/inputs.query Description Flag to hold the promql query for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.query contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint Field .promProbe/inputs.queryPath Description Flag to hold the path of the promql query for the promProbe Type Optional Range n/a {type: string} Notes The .promProbe/inputs.queryPath This field is used in case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. Runproperties \u00b6 View the run-properties schema Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails Comparator \u00b6 View the comparator schema Field type Description Flag to hold type of the data used for comparision Type Mandatory Range string , int , float Notes The type contains type of data, which should be compare as part of comparision operation Field criteria Description Flag to hold criteria for the comparision Type Mandatory Range it supports {>=, <=, ==, >, <, !=, oneOf, between} for int & float type. And {equal, notEqual, contains, matches, notMatches, oneOf} for string type. Notes The criteria contains criteria of the comparision, which should be fulfill as part of comparision operation. Field value Description Flag to hold value for the comparision Type Mandatory Range n/a {type: string} Notes The value contains value of the comparision, which should follow the given criteria as part of comparision operation.","title":"Contents"},{"location":"experiments/chaos-resources/probes/contents/#probes-specifications","text":"Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). Probe Name Description User Guide Command Probe It defines the command probes Command Probe HTTP Probe It defines the http probes HTTP Probe K8S Probe It defines the k8s probes K8S Probe Prometheus Probe It defines the prometheus probes Prometheus Probe","title":"Probes Specifications"},{"location":"experiments/chaos-resources/probes/contents/#basic-details","text":"View the basic schema Field .name Description Flag to hold the name of the probe Type Mandatory Range n/a (type: string) Notes The .name holds the name of the probe. It can be set based on the usecase Field .type Description Flag to hold the type of the probe Type Mandatory Range httpProbe , k8sProbe , cmdProbe , promProbe Notes The .type supports four type of probes. It can one of the httpProbe , k8sProbe , cmdProbe , promProbe Field .mode Description Flag to hold the mode of the probe Type Mandatory Range SOT , EOT , Edge , Continuous , OnChaos Notes The .mode supports five modes of probes. It can one of the SOT , EOT , Edge , Continuous , OnChaos Field .data Description Flag to hold the data for the create operation of the k8sProbe Type Optional Range n/a {type: string} Notes The .data contains the manifest/data for the resource, which need to be created. It supported for create operation of k8sProbe only","title":"Basic Details"},{"location":"experiments/chaos-resources/probes/contents/#command-probe","text":"View the command probe schema Field .cmdProbe/inputs.command Description Flag to hold the command for the cmdProbe Type Mandatory Range n/a {type: string} Notes The .cmdProbe/inputs.command contains the shell command, which should be run as part of cmdProbe Field .cmdProbe/inputs.source Description Flag to hold the source for the cmdProbe Type Mandatory Range inline , any source docker image Notes The .cmdProbe/inputs.source It supports inline value when command can be run from within the experiment image. Otherwise provide the source image which can be used to launch a external pod where the command execution is carried out.","title":"Command Probe"},{"location":"experiments/chaos-resources/probes/contents/#http-probe","text":"View the http probe schema Field .httpProbe/inputs.url Description Flag to hold the URL for the httpProbe Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.url contains the URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. Field .httpProbe/inputs.insecureSkipVerify Description Flag to hold the flag to skip certificate checks for the httpProbe Type Optional Range true , false Notes The .httpProbe/inputs.insecureSkipVerify contains flag to skip certificate checks. Field .httpProbe/inputs.responseTimeout Description Flag to hold the flag to response timeout for the httpProbe Type Optional Range n/a {type: integer} Notes The .httpProbe/inputs.responseTimeout contains flag to provide the response timeout for the http Get/Post request. Field .httpProbe/inputs.method.get.criteria Description Flag to hold the criteria for the http get request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.get.criteria contains criteria to match the http get request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.get.responseCode Description Flag to hold the expected response code for the get request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.get.responseCode contains the expected response code for the http get request as part of httpProbe run Field .httpProbe/inputs.method.post.contentType Description Flag to hold the content type of the post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.contentType contains the content type of the http body data, which need to be passed for the http post request Field .httpProbe/inputs.method.post.body Description Flag to hold the body of the http post request Type Mandatory Range n/a {type: string} Notes The .httpProbe/inputs.method.post.body contains the http body, which is required for the http post request. It is used for the simple http body. If the http body is complex then use .httpProbe/inputs.method.post.bodyPath field. Field .httpProbe/inputs.method.post.bodyPath Description Flag to hold the path of the http body, required for the http post request Type Optional Range n/a {type: string} Notes The .httpProbe/inputs.method.post.bodyPath This field is used in case of complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. Field .httpProbe/inputs.method.post.criteria Description Flag to hold the criteria for the http post request Type Mandatory Range == , != , oneOf Notes The .httpProbe/inputs.method.post.criteria contains criteria to match the http post request's response code with the expected responseCode, which need to be fulfill as part of httpProbe run Field .httpProbe/inputs.method.post.responseCode Description Flag to hold the expected response code for the post request Type Mandatory Range HTTP_RESPONSE_CODE Notes The .httpProbe/inputs.method.post.responseCode contains the expected response code for the http post request as part of httpProbe run","title":"HTTP Probe"},{"location":"experiments/chaos-resources/probes/contents/#k8s-probe","text":"View the k8s probe schema Field .k8sProbe/inputs.group Description Flag to hold the group of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.group contains group of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.version Description Flag to hold the apiVersion of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.version contains apiVersion of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.resource Description Flag to hold the kubernetes resource name for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.resource contains the kubernetes resource name on which k8sProbe performs the specified operation Field .k8sProbe/inputs.namespace Description Flag to hold the namespace of the kubernetes resource for the k8sProbe Type Mandatory Range n/a {type: string} Notes The .k8sProbe/inputs.namespace contains namespace of the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.fieldSelector Description Flag to hold the fieldSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.fieldSelector contains fieldSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.labelSelector Description Flag to hold the labelSelectors of the kubernetes resource for the k8sProbe Type Optional Range n/a {type: string} Notes The .k8sProbe/inputs.labelSelector contains labelSelector to derived the kubernetes resource on which k8sProbe performs the specified operation Field .k8sProbe/inputs.operation Description Flag to hold the operation type for the k8sProbe Type Mandatory Range create , delete , present , absent Notes The .k8sProbe/inputs.operation contains operation which should be applied on the kubernetes resource as part of k8sProbe. It supports four type of operation. It can be one of create , delete , present , absent .","title":"K8S Probe"},{"location":"experiments/chaos-resources/probes/contents/#prometheus-probe","text":"View the prometheus probe schema Field .promProbe/inputs.endpoint Description Flag to hold the prometheus endpoints for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.endpoint contains the prometheus endpoints Field .promProbe/inputs.query Description Flag to hold the promql query for the promProbe Type Mandatory Range n/a {type: string} Notes The .promProbe/inputs.query contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint Field .promProbe/inputs.queryPath Description Flag to hold the path of the promql query for the promProbe Type Optional Range n/a {type: string} Notes The .promProbe/inputs.queryPath This field is used in case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR.","title":"Prometheus Probe"},{"location":"experiments/chaos-resources/probes/contents/#runproperties","text":"View the run-properties schema Field .runProperties.probeTimeout Description Flag to hold the timeout for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.probeTimeout represents the time limit for the probe to execute the specified check and return the expected data Field .runProperties.retry Description Flag to hold the retry count for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.retry contains the number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. Field .runProperties.interval Description Flag to hold the interval for the probes Type Mandatory Range n/a {type: integer} Notes The .runProperties.interval contains the interval for which probes waits between subsequent retries Field .runProperties.probePollingInterval Description Flag to hold the polling interval for the probes(applicable for Continuous mode only) Type Optional Range n/a {type: integer} Notes The .runProperties.probePollingInterval contains the time interval for which continuous probe should be sleep after each iteration Field .runProperties.initialDelaySeconds Description Flag to hold the initial delay interval for the probes Type Optional Range n/a {type: integer} Notes The .runProperties.initialDelaySeconds represents the initial waiting time interval for the probes. Field .runProperties.stopOnFailure Description Flags to hold the stop or continue the experiment on probe failure Type Optional Range false {type: boolean} Notes The .runProperties.stopOnFailure can be set to true/false to stop or continue the experiment execution after probe fails","title":"Runproperties"},{"location":"experiments/chaos-resources/probes/contents/#comparator","text":"View the comparator schema Field type Description Flag to hold type of the data used for comparision Type Mandatory Range string , int , float Notes The type contains type of data, which should be compare as part of comparision operation Field criteria Description Flag to hold criteria for the comparision Type Mandatory Range it supports {>=, <=, ==, >, <, !=, oneOf, between} for int & float type. And {equal, notEqual, contains, matches, notMatches, oneOf} for string type. Notes The criteria contains criteria of the comparision, which should be fulfill as part of comparision operation. Field value Description Flag to hold value for the comparision Type Mandatory Range n/a {type: string} Notes The value contains value of the comparision, which should follow the given criteria as part of comparision operation.","title":"Comparator"},{"location":"experiments/chaos-resources/probes/httpProbe/","text":"The http probe allows developers to specify a URL which the experiment uses to gauge health/service availability (or other custom conditions) as part of the entry/exit criteria. The received status code is mapped against an expected status. It supports http Get and Post methods. It can be executed by setting type as httpProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. HTTP Get Request \u00b6 In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a simple) \u00b6 It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 HTTP Post Request(http body is a complex) \u00b6 In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Response Timout \u00b6 It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Skip Certification Check \u00b6 It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Probe"},{"location":"experiments/chaos-resources/probes/httpProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-get-request","text":"In HTTP Get method, it sends an http GET request to the provided URL and matches the response code based on the given criteria(==, !=, oneOf). It can be executed by setting httpProbe/inputs.method.get field. Use the following example to tune this: # contains the http probes with get method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http get method and verify the response code get : # criteria which should be matched criteria : == # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Get Request"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-simple","text":"It contains the http body, which is required for the http post request. It is used for the simple http body. The http body can be provided in the body field. It can be executed by setting httpProbe/inputs.method.post.body field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # value of the http body, used for the post request body : \"<http-body>\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a simple)"},{"location":"experiments/chaos-resources/probes/httpProbe/#http-post-requesthttp-body-is-a-complex","text":"In the case of a complex POST request in which the body spans multiple lines, the bodyPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting httpProbe/inputs.method.post.body field. NOTE : It is mutually exclusive with the body field. If body is set then it will use the body field for the post request otherwise, it will use the bodyPath field. Use the following example to tune this: # contains the http probes with post method and verify the response code apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" method : # call http post method and verify the response code post : # the configMap should be mounted to the experiment which contains http body # use the mounted path here bodyPath : \"/mnt/body.yml\" # http body content type contentType : \"application/json; charset=UTF-8\" # criteria which should be matched criteria : \"==\" # ==, !=, oneof # exepected response code for the http request, which should follow the specified criteria responseCode : \"200\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"HTTP Post Request(http body is a complex)"},{"location":"experiments/chaos-resources/probes/httpProbe/#response-timout","text":"It contains a flag to provide the response timeout for the http Get/Post request. It can be tuned via .httpProbe/inputs.responseTimeout field. It is an optional field and its unit is milliseconds. Use the following example to tune this: # defines the response timeout for the http probe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # timeout for the http requests responseTimeout : 100 #in ms method : get : criteria : == # ==, !=, oneof responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Response Timout"},{"location":"experiments/chaos-resources/probes/httpProbe/#skip-certification-check","text":"It contains flag to skip certificate checks. It can bed tuned via .httpProbe/inputs.insecureSkipVerify field. It supports boolean values. Provide it to true to skip the certificate checks. Its default value is false. Use the following example to tune this: # skip the certificate checks for the httpProbe apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" # skip certificate checks for the httpProbe # supports: true, false. default: false insecureSkipVerify : \"true\" method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Skip Certification Check"},{"location":"experiments/chaos-resources/probes/k8sProbe/","text":"With the proliferation of custom resources & operators, especially in the case of stateful applications, the steady-state is manifested as status parameters/flags within Kubernetes resources. k8sProbe addresses verification of the desired resource state by allowing users to define the Kubernetes GVR (group-version-resource) with appropriate filters (field selectors/label selectors). The experiment makes use of the Kubernetes Dynamic Client to achieve this. It supports CRUD operations which can be defined at probe.k8sProbe/inputs.operation . It can be executed by setting type as k8sProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Create Operation \u00b6 It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi Delete Operation \u00b6 It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Present Operation \u00b6 It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 Absent Operation \u00b6 It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"K8S Probe"},{"location":"experiments/chaos-resources/probes/k8sProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/k8sProbe/#create-operation","text":"It creates kubernetes resource based on the data provided inside probe.data field. It can be defined by setting operation to create operation. Use the following example to tune this: # create the given resource provided inside data field apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"create-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource should be created namespace : \"default\" # type of operation # supports: create, delete, present, absent operation : \"create\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1 # contains manifest, which can be used to create the resource data : | kind: PersistentVolumeClaim apiVersion: v1 metadata: name: percona-mysql-claim labels: openebs.io/target-affinity: percona spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 100Mi","title":"Create Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#delete-operation","text":"It deletes matching kubernetes resources via GVR and filters (field selectors/label selectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to delete operation. Use the following example to tune this: # delete the resource matched with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"delete-percona-pvc\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace of the instance, which needs to be deleted namespace : \"default\" # labels selectors for the k8s resource, which needs to be deleted labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource, which needs to be deleted fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"delete\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Delete Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#present-operation","text":"It checks for the presence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to present operation. Use the following example to tune this: # verify the existance of the resource matched with the given inputs inside cluster apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-presence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"present\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Present Operation"},{"location":"experiments/chaos-resources/probes/k8sProbe/#absent-operation","text":"It checks for the absence of kubernetes resource based on GVR and filters (field selectors/labelselectors) provided at probe.k8sProbe/inputs . It can be defined by setting operation to absent operation. Use the following example to tune this: # verify that the no resource should be present in cluster with the given inputs apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-percona-pvc-absence\" type : \"k8sProbe\" k8sProbe/inputs : # group of the resource group : \"\" # version of the resource version : \"v1\" # name of the resource resource : \"persistentvolumeclaims\" # namespace where the instance of resource namespace : \"default\" # labels selectors for the k8s resource labelSelector : \"openebs.io/target-affinity=percona\" # fieldselector for the k8s resource fieldSelector : \"\" # type of operation # supports: create, delete, present, absent operation : \"absent\" mode : \"EOT\" runProperties : probeTimeout : 5 interval : 2 retry : 1","title":"Absent Operation"},{"location":"experiments/chaos-resources/probes/litmus-probes/","text":"Litmus probes are pluggable checks that can be defined within the ChaosEngine for any chaos experiment. The experiment pods execute these checks based on the mode they are defined in & factor their success as necessary conditions in determining the verdict of the experiment (along with the standard \u201cin-built\u201d checks). It can be provided at .spec.experiments[].spec.probe inside chaosengine. It supports four types: cmdProbe , k8sProbe , httpProbe , and promProbe . Probe Modes \u00b6 The probes can be set up to run in five different modes. Which can be tuned via mode ENV. - SOT : Executed at the Start of the Test as a pre-chaos check - EOT : Executed at the End of the Test as a post-chaos check - Edge : Executed both, before and after the chaos - Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. - OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 Run Properties \u00b6 All probes share some common attributes. Which can be tuned via runProperties ENV. - probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. - retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. - interval : The period between subsequent retries - probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2 Initial Delay Seconds \u00b6 It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds Stop/Continue Experiment On Probe Failure \u00b6 It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true Probe Chaining \u00b6 Probe chaining enables reuse of probe a result (represented by the template function {{ .&gt;probeName&lt;.probeArtifact.Register}}) in subsequent \"downstream\" probes defined in the ChaosEngine. Note : The order of execution of probes in the experiment depends purely on the order in which they are defined in the ChaosEngine. Use the following example to tune this: # chaining enables reuse of probe's result (represented by the template function {{ <probeName>.probeArtifact.Register}}) #-- in subsequent \"downstream\" probes defined in the ChaosEngine. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"probe1\" type : \"cmdProbe\" cmdProbe/inputs : command : \"<command>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1 - name : \"probe2\" type : \"cmdProbe\" cmdProbe/inputs : ## probe1's result being used as one of the args in probe2 command : \"<commmand> {{ .probe1.ProbeArtifacts.Register }} <arg2>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Litmus probes"},{"location":"experiments/chaos-resources/probes/litmus-probes/#probe-modes","text":"The probes can be set up to run in five different modes. Which can be tuned via mode ENV. - SOT : Executed at the Start of the Test as a pre-chaos check - EOT : Executed at the End of the Test as a post-chaos check - Edge : Executed both, before and after the chaos - Continuous : The probe is executed continuously, with a specified polling interval during the chaos injection. - OnChaos : The probe is executed continuously, with a specified polling interval strictly for chaos duration of chaos Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" # modes for the probes # supports: [SOT, EOT, Edge, Continuous, OnChaos] mode : \"Continuous\" runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2","title":"Probe Modes"},{"location":"experiments/chaos-resources/probes/litmus-probes/#run-properties","text":"All probes share some common attributes. Which can be tuned via runProperties ENV. - probeTimeout : Represents the time limit for the probe to execute the check specified and return the expected data. - retry : The number of times a check is re-run upon failure in the first attempt before declaring the probe status as failed. - interval : The period between subsequent retries - probePollingInterval : The time interval for which continuous/onchaos probes should be sleep after each iteration. Use the following example to tune this: # contains the common attributes or run properties apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : # time limit for the probe to execute the specified check probeTimeout : 5 #in seconds # the time period between subsequent retries interval : 2 #in seconds # number of times a check is re-run upon failure before declaring the probe status as failed retry : 1 #time interval for which continuous probe should wait after each iteration # applicable for onChaos and Continuous probes probePollingInterval : 2","title":"Run Properties"},{"location":"experiments/chaos-resources/probes/litmus-probes/#initial-delay-seconds","text":"It Represents the initial waiting time interval for the probes. It can be tuned via initialDelaySeconds ENV. Use the following example to tune this: # contains the initial delay seconds for the probes apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes RunProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #initial waiting time interval for the probes initialDelaySeconds : 30 #in seconds","title":"Initial Delay Seconds"},{"location":"experiments/chaos-resources/probes/litmus-probes/#stopcontinue-experiment-on-probe-failure","text":"It can be set to true/false to stop or continue the experiment execution after the probe fails. It can be tuned via stopOnFailure ENV. It supports boolean values. The default value is false . Use the following example to tune this: # contains the flag to stop/continue experiment based on the specified flag apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-frontend-access-url\" type : \"httpProbe\" httpProbe/inputs : url : \"<url>\" insecureSkipVerify : false responseTimeout : <value> method : get : criteria : == responseCode : \"<response code>\" mode : \"Continuous\" # contains runProperties for the probes runProperties : probeTimeout : 5 interval : 2 retry : 1 probePollingInterval : 2 #it can be set to true/false to stop or continue the experiment execution after probe fails # supports: true, false. default: false stopOnFailure : true","title":"Stop/Continue Experiment On Probe Failure"},{"location":"experiments/chaos-resources/probes/litmus-probes/#probe-chaining","text":"Probe chaining enables reuse of probe a result (represented by the template function {{ .&gt;probeName&lt;.probeArtifact.Register}}) in subsequent \"downstream\" probes defined in the ChaosEngine. Note : The order of execution of probes in the experiment depends purely on the order in which they are defined in the ChaosEngine. Use the following example to tune this: # chaining enables reuse of probe's result (represented by the template function {{ <probeName>.probeArtifact.Register}}) #-- in subsequent \"downstream\" probes defined in the ChaosEngine. apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"probe1\" type : \"cmdProbe\" cmdProbe/inputs : command : \"<command>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1 - name : \"probe2\" type : \"cmdProbe\" cmdProbe/inputs : ## probe1's result being used as one of the args in probe2 command : \"<commmand> {{ .probe1.ProbeArtifacts.Register }} <arg2>\" comparator : type : \"string\" criteria : \"equals\" value : \"<value-for-criteria-match>\" source : \"inline\" mode : \"SOT\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Probe Chaining"},{"location":"experiments/chaos-resources/probes/promProbe/","text":"The prometheus probe allows users to run Prometheus queries and match the resulting output against specific conditions. The intent behind this probe is to allow users to define metrics-based SLOs in a declarative way and determine the experiment verdict based on its success. The probe runs the query on a Prometheus server defined by the endpoint, and checks whether the output satisfies the specified criteria. It can be executed by setting type as promProbe inside .spec.experiments[].spec.probe . Common Probe Tunables \u00b6 Refer the common attributes to tune the common tunables for all the probes. Prometheus Query(query is a simple) \u00b6 It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1 Prometheus Query(query is a complex \u00b6 In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Probe"},{"location":"experiments/chaos-resources/probes/promProbe/#common-probe-tunables","text":"Refer the common attributes to tune the common tunables for all the probes.","title":"Common Probe Tunables"},{"location":"experiments/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-simple","text":"It contains the promql query to extract out the desired prometheus metrics via running it on the given prometheus endpoint. The prometheus query can be provided in the query field. It can be executed by setting .promProbe/inputs.query field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # promql query, which should be executed query : \"<promql-query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a simple)"},{"location":"experiments/chaos-resources/probes/promProbe/#prometheus-queryquery-is-a-complex","text":"In case of complex queries that spans multiple lines, the queryPath attribute can be used to provide the path to a file consisting of the same. This file can be made available to the experiment pod via a ConfigMap resource, with the ConfigMap name being defined in the ChaosEngine OR the ChaosExperiment CR. It can be executed by setting promProbe/inputs.queryPath field. NOTE : It is mutually exclusive with the query field. If query is set then it will use the query field otherwise, it will use the queryPath field. Use the following example to tune this: # contains the prom probe which execute the query and match for the expected criteria apiVersion : litmuschaos.io/v1alpha1 kind : ChaosEngine metadata : name : engine-nginx spec : engineState : \"active\" appinfo : appns : \"default\" applabel : \"app=nginx\" appkind : \"deployment\" chaosServiceAccount : pod-delete-sa experiments : - name : pod-delete spec : probe : - name : \"check-probe-success\" type : \"promProbe\" promProbe/inputs : # endpoint for the promethus service endpoint : \"<prometheus-endpoint>\" # the configMap should be mounted to the experiment which contains promql query # use the mounted path here queryPath : \"<path of the query>\" comparator : # criteria which should be followed by the actual output and the expected output #supports >=,<=,>,<,==,!= comparision criteria : \"==\" # expected value, which should follow the specified criteria value : \"<value-for-criteria-match>\" mode : \"Edge\" runProperties : probeTimeout : 5 interval : 5 retry : 1","title":"Prometheus Query(query is a complex"},{"location":"experiments/faq/experiments/","text":"The What, Why & How of Litmus \u00b6 Table of Contents \u00b6 Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? What are the typical probes used for pod-network related experiments? Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? How to run chaos experiment programatically using apis? Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? The experiment is not targeting all pods with the given label, it just selects only one pod by default Do we have a way to see what pods are targeted when users use percentages? What is the function of spec.definition.scope of a ChaosExperiment CR? Pod network latency -- I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? How to check the NETWORK_INTERFACE and SOCKET_PATH variable? What are the different ways to target the pods and nodes for chaos? Does the pod affected perc select the random set of pods from the total pods under chaos? How to extract the chaos start time and end time? How do we check the MTTR (Mean time to recovery) for an application post chaos? What is the difference between Ramp Time and Chaos Interval? When I\u2019m executing an experiment the experiment's pod failed with the exec format error Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? \u00b6 The experiment takes a percentage of the total memory capacity of the Node. The helper pod runs on the target node to stress the resources of that node. So The experiment will not consume/hog the memory resources greater than the total memory available on Node. In other words there will always be an upper limit for the amount of memory to be consumed, which equal to the total available memory. Please refer to this blog for more details. Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? \u00b6 Yes, ultimately these are different ways to simulate a degraded network. Both cases are expected to typically cause retransmissions (for tcp). The extent of degradation depends on the percentage of loss/corruption How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? \u00b6 The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in different availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The args passed to the tc netem command run against the target container changes depending on the type of n/w fault What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? \u00b6 The pod cpu and memory chaos experiment till now (version 1.13.7) was using an exec mode of execution which means - we were execing inside the specified target container and launching process like md5sum and dd to consume the cpu and memory respectively. This is done by providing CHAOS_INJECT_COMMAND and CHAOS-KILL-COMMAND in chaosengine CR. But we have some limitations of using this method. Those were: - The chaos inject and kill command are highly dependent on the base image of the target container and may work for some and for others you may have to derive it manually and use it. - For scratch images that don't expose shells we couldn't execute the chaos. To overcome this - The stress-chaos experiments (cpu, memory and io) are enhanced to use a non exec mode of chaos execution. It makes use of target container cgroup for the resource allocation and container pid namespace for showing the stress-ng process in target container. This stress-ng process will consume the resources on the target container without doing an exec. The new enhanced experiments are available from litmus 1.13.8 version. What are the typical probes used for pod-network related experiments? \u00b6 Precisely the role of the experiment. Cause n/w degradation w/o the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in diff availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? \u00b6 The optional libs (like Pumba) is more of an illustration of how you can use 3 rd party tools with litmus. Called the BYOC (Bring Your Own Chaos). The preferred LIB is litmus . How to run chaos experiment programatically using apis? \u00b6 To directly consume/manipulate the chaos resources (i.e., chaosexperiment, chaosengine or chaosresults) via API - you can directly use the kube API. The CRDs by default provide us with an API endpoint. You can use any generic client implementation (go/python are most used ones) to access them. In case you use go, there is a clientset available as well: go-client Here are some simple CRUD ops against chaosresources you could construct with curl (I have used kubectl proxy, one could use an auth token instead)- just for illustration purposes. Create ChaosEngine: \u00b6 For example, assume this is the engine spec curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines -XPOST -H 'Content-Type: application/json' -d@pod-delete-chaosengine-trigger.json Read ChaosEngine status: \u00b6 curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos | jq '.status.engineStatus, .status.experiments[].verdict' Update ChaosEngine Spec: \u00b6 (say, this is the patch: https://gist.github.com/ksatchit/be54955a1f4231314797f25361ac488d ) curl --header \"Content-Type: application/json-patch+json\" --request PATCH --data '[{\"op\": \"replace\", \"path\": \"/spec/engineState\", \"value\": \"stop\"}]' http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos Delete the ChaosEngine resource: \u00b6 curl -X DELETE localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\" Similarly, to check the results/verdict of the experiment from ChaosResult, you could use: \u00b6 curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/nginx-chaos-pod-delete | jq '.status.experimentStatus.verdict, .status.experimentStatus.probeSuccessPercentage' Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? \u00b6 There are some scenarios that can still occur despite whatever availability aids K8s provides. For example, take disk usage or CPU hogs -- problems you would generally refer to as \"Noisy Neighbour\" problems. Stressing the disk w/ continuous and heavy I/O for example can cause degradation in reads and writes performed by other microservices that use this shared disk - for example. (modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out). Another issue is the amount of scratch space eaten up on a node - leading to lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes. Similarly w/ CPU chaos -- by injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in app traffic OR in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods. The experiment is not targeting all pods with the given label, it just selects only one pod by default. \u00b6 Yes. You can use either the PODS_AFFECTED_PERCENTAGE or TARGET_PODS env to select multiple pods. Refer: experiment tunable envs . Do we have a way to see what pods are targeted when users use percentages? \u00b6 We can view the target pods from the experiment logs or inside chaos results. What is the function of spec.definition.scope of a ChaosExperiment CR? \u00b6 The spec.definition.scope & .spec.definition.permissions is mostly for indicative/illustration purposes (for external tools to identify and validate what are the permissions associated to run the exp). By itself, it doesn't influence how and where an exp can be used.One could remove these fields if needed (of course along w/ the crd validation) and store these manifests if desired. In Pod network latency - I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? \u00b6 It will target the [1:total_replicas] (based on PODS_AFFECTED_PERC) numbers of random pods with matching labels(appinfo.applabel) and namespace(appinfo.appns). But if you want to target a specific pod then you can provide their names as a comma separated list inside TARGET_PODS . Yes, you can provide service B details inside DESTINATION_IPS or DESTINATION_HOSTS . The NETWORK_INTERFACE should be eth0 . How to check the NETWORK_INTERFACE and SOCKET_PATH variable? \u00b6 The NETWORK_INTERFACE is the interface name inside the pod/container that needs to be targeted. You can find it by execing into the target pod and checking the available interfaces. You can try ip link , iwconfig , ifconfig depending on the tools installed in the pod either of those could work. The SOCKET_PATH by default takes the docker socket path. If you are using something else like containerd, crio or have a different socket path by any chance you can specify it. This is required to communicate with the container runtime of your cluster. In addition to this if container-runtime is different then provide the name of container runtime inside CONTAINER_RUNTIME ENV. It supports docker , containerd , and crio runtimes. What are the different ways to target the pods and nodes for chaos? \u00b6 The different ways are: Pod Chaos: - Appinfo : Provide the target pod labels in the chaos engine appinfo section. - TARGET_PODS : You can provide the target pod names as a Comma Separated Variable. Like pod1,pod2. Node Chaos: - TARGET_NODE or TARGET_NODES : Provide the target node or nodes in these envs. - NODE_LABEL : Provide the label of the target nodes. Does the pod affected percentage select the random set of pods from the total pods under chaos? \u00b6 Yes, it selects the random pods based on the POD_AFFACTED_PERC ENV. In pod-delete experiment it selects random pods for each iterations of chaos. But for rest of the experiments(if it supports iterations) then it will select random pods once and use the same set of pods for remaining iterations. How to extract the chaos start time and end time? \u00b6 We can use the Chaos exporter metrics for the same. One can also visualise these events along with time in chaos engine events. How do we check the MTTR (Mean time to recovery) for an application post chaos? \u00b6 The MTTR can be validated by using statusCheck Timeout in the chaos engine. By default its value will be 180 seconds. We can also overwrite this using ChaosEngine. For more details refer this What is the difference between Ramp Time and Chaos Interval? \u00b6 The ramp time is the time duration to wait before and after injection of chaos in seconds. While the chaos interval is the time interval (in second) between successive chaos iterations. When I\u2019m executing an experiment the experiment's pod failed with the exec format error \u00b6 View the error message standard_init_linux.go:211: exec user process caused \"exec format error\": There could be multiple reasons for this. The most common one is mismatched in the binary and the platform on which it is running, try to check out the image binary you're using should have the support for the platform on which you\u2019re trying to run the experiment.","title":"Experiments"},{"location":"experiments/faq/experiments/#the-what-why-how-of-litmus","text":"","title":"The What, Why &amp; How of Litmus"},{"location":"experiments/faq/experiments/#table-of-contents","text":"Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress? Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test? How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it? What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec? What are the typical probes used for pod-network related experiments? Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use? How to run chaos experiment programatically using apis? Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments? The experiment is not targeting all pods with the given label, it just selects only one pod by default Do we have a way to see what pods are targeted when users use percentages? What is the function of spec.definition.scope of a ChaosExperiment CR? Pod network latency -- I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS? How to check the NETWORK_INTERFACE and SOCKET_PATH variable? What are the different ways to target the pods and nodes for chaos? Does the pod affected perc select the random set of pods from the total pods under chaos? How to extract the chaos start time and end time? How do we check the MTTR (Mean time to recovery) for an application post chaos? What is the difference between Ramp Time and Chaos Interval? When I\u2019m executing an experiment the experiment's pod failed with the exec format error","title":"Table of Contents"},{"location":"experiments/faq/experiments/#node-memory-hog-experiments-pod-oom-killed-even-before-the-kubelet-sees-the-memory-stress","text":"The experiment takes a percentage of the total memory capacity of the Node. The helper pod runs on the target node to stress the resources of that node. So The experiment will not consume/hog the memory resources greater than the total memory available on Node. In other words there will always be an upper limit for the amount of memory to be consumed, which equal to the total available memory. Please refer to this blog for more details.","title":"Node memory hog experiment's pod OOM Killed even before the kubelet sees the memory stress?"},{"location":"experiments/faq/experiments/#pod-network-corruption-and-pod-network-loss-both-experiments-force-network-packet-loss-is-it-worthwhile-trying-out-both-experiments-in-a-scheduled-chaos-test","text":"Yes, ultimately these are different ways to simulate a degraded network. Both cases are expected to typically cause retransmissions (for tcp). The extent of degradation depends on the percentage of loss/corruption","title":"Pod-network-corruption and pod-network-loss both experiments force network packet loss - is it worthwhile trying out both experiments in a scheduled chaos test?"},{"location":"experiments/faq/experiments/#how-is-the-packet-loss-achieved-in-pod-network-loss-and-corruption-experiments-what-are-the-internals-of-it","text":"The experiment causes network degradation without the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in different availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree. The args passed to the tc netem command run against the target container changes depending on the type of n/w fault","title":"How is the packet loss achieved in pod-network loss and corruption experiments? What are the internals of it?"},{"location":"experiments/faq/experiments/#whats-the-difference-between-pod-memorycpu-hog-vs-pod-memorycpu-hog-exec","text":"The pod cpu and memory chaos experiment till now (version 1.13.7) was using an exec mode of execution which means - we were execing inside the specified target container and launching process like md5sum and dd to consume the cpu and memory respectively. This is done by providing CHAOS_INJECT_COMMAND and CHAOS-KILL-COMMAND in chaosengine CR. But we have some limitations of using this method. Those were: - The chaos inject and kill command are highly dependent on the base image of the target container and may work for some and for others you may have to derive it manually and use it. - For scratch images that don't expose shells we couldn't execute the chaos. To overcome this - The stress-chaos experiments (cpu, memory and io) are enhanced to use a non exec mode of chaos execution. It makes use of target container cgroup for the resource allocation and container pid namespace for showing the stress-ng process in target container. This stress-ng process will consume the resources on the target container without doing an exec. The new enhanced experiments are available from litmus 1.13.8 version.","title":"What's the difference between pod-memory/cpu-hog vs pod-memory/cpu-hog-exec?"},{"location":"experiments/faq/experiments/#what-are-the-typical-probes-used-for-pod-network-related-experiments","text":"Precisely the role of the experiment. Cause n/w degradation w/o the pod being marked unhealthy/unworthy of traffic by kube-proxy (unless you have a liveness probe of sorts that measures latency and restarts/crashes the container) The idea of this exp is to simulate issues within your pod-network OR microservice communication across services in diff availability zones/regions etc.., Mitigation (in this case keep the timeout i.e., access latency low) could be via some middleware that can switch traffic based on some SLOs/perf parameters. If such an arrangement is not available - the next best thing would be to verify if such a degradation is highlighted via notification/alerts etc,. so the admin/SRE has the opportunity to investigate and fix things. Another utility of the test would be to see what the extent of impact caused to the end-user OR the last point in the app stack on account of degradation in access to a downstream/dependent microservice. Whether it is acceptable OR breaks the system to an unacceptable degree","title":"What are the typical probes used for pod-network related experiments?"},{"location":"experiments/faq/experiments/#litmus-provides-multiple-libs-to-run-some-chaos-experiments-like-stress-chaos-and-network-chaos-so-which-library-should-be-preferred-to-use","text":"The optional libs (like Pumba) is more of an illustration of how you can use 3 rd party tools with litmus. Called the BYOC (Bring Your Own Chaos). The preferred LIB is litmus .","title":"Litmus provides multiple libs to run some chaos experiments like stress-chaos and network chaos so which library should be preferred to use?"},{"location":"experiments/faq/experiments/#how-to-run-chaos-experiment-programatically-using-apis","text":"To directly consume/manipulate the chaos resources (i.e., chaosexperiment, chaosengine or chaosresults) via API - you can directly use the kube API. The CRDs by default provide us with an API endpoint. You can use any generic client implementation (go/python are most used ones) to access them. In case you use go, there is a clientset available as well: go-client Here are some simple CRUD ops against chaosresources you could construct with curl (I have used kubectl proxy, one could use an auth token instead)- just for illustration purposes.","title":"How to run chaos experiment programatically using apis?"},{"location":"experiments/faq/experiments/#create-chaosengine","text":"For example, assume this is the engine spec curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines -XPOST -H 'Content-Type: application/json' -d@pod-delete-chaosengine-trigger.json","title":"Create ChaosEngine:"},{"location":"experiments/faq/experiments/#read-chaosengine-status","text":"curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos | jq '.status.engineStatus, .status.experiments[].verdict'","title":"Read ChaosEngine status:"},{"location":"experiments/faq/experiments/#update-chaosengine-spec","text":"(say, this is the patch: https://gist.github.com/ksatchit/be54955a1f4231314797f25361ac488d ) curl --header \"Content-Type: application/json-patch+json\" --request PATCH --data '[{\"op\": \"replace\", \"path\": \"/spec/engineState\", \"value\": \"stop\"}]' http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos","title":"Update ChaosEngine Spec:"},{"location":"experiments/faq/experiments/#delete-the-chaosengine-resource","text":"curl -X DELETE localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosengines/nginx-chaos \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\"","title":"Delete the ChaosEngine resource:"},{"location":"experiments/faq/experiments/#similarly-to-check-the-resultsverdict-of-the-experiment-from-chaosresult-you-could-use","text":"curl -s http://localhost:8001/apis/litmuschaos.io/v1alpha1/namespaces/default/chaosresults/nginx-chaos-pod-delete | jq '.status.experimentStatus.verdict, .status.experimentStatus.probeSuccessPercentage'","title":"Similarly, to check the results/verdict of the experiment from ChaosResult, you could use:"},{"location":"experiments/faq/experiments/#kubernetes-by-default-has-built-in-features-like-replicasetdeployment-to-prevent-service-unavailability-continuous-curl-from-the-httpprobe-on-litmus-should-not-fail-in-case-of-container-kill-pod-delete-and-oom-due-to-pod-memory-hog-then-why-do-we-need-cpu-io-and-network-related-chaos-experiments","text":"There are some scenarios that can still occur despite whatever availability aids K8s provides. For example, take disk usage or CPU hogs -- problems you would generally refer to as \"Noisy Neighbour\" problems. Stressing the disk w/ continuous and heavy I/O for example can cause degradation in reads and writes performed by other microservices that use this shared disk - for example. (modern storage solutions for Kubernetes use the concept of storage pools out of which virtual volumes/devices are carved out). Another issue is the amount of scratch space eaten up on a node - leading to lack of space for newer containers to get scheduled (kubernetes too gives up by applying an \"eviction\" taint like \"disk-pressure\") and causes a wholesale movement of all pods to other nodes. Similarly w/ CPU chaos -- by injecting a rogue process into a target container, we starve the main microservice process (typically pid 1) of the resources allocated to it (where limits are defined) causing slowness in app traffic OR in other cases unrestrained use can cause node to exhaust resources leading to eviction of all pods.","title":"Kubernetes by default has built-in features like replicaset/deployment to prevent service unavailability (continuous curl from the httpProbe on litmus should not fail) in case of container kill, pod delete and OOM due to pod-memory-hog then why do we need CPU, IO and network related chaos experiments?"},{"location":"experiments/faq/experiments/#the-experiment-is-not-targeting-all-pods-with-the-given-label-it-just-selects-only-one-pod-by-default","text":"Yes. You can use either the PODS_AFFECTED_PERCENTAGE or TARGET_PODS env to select multiple pods. Refer: experiment tunable envs .","title":"The experiment is not targeting all pods with the given label, it just selects only one pod by default."},{"location":"experiments/faq/experiments/#do-we-have-a-way-to-see-what-pods-are-targeted-when-users-use-percentages","text":"We can view the target pods from the experiment logs or inside chaos results.","title":"Do we have a way to see what pods are targeted when users use percentages?"},{"location":"experiments/faq/experiments/#what-is-the-function-of-specdefinitionscope-of-a-chaosexperiment-cr","text":"The spec.definition.scope & .spec.definition.permissions is mostly for indicative/illustration purposes (for external tools to identify and validate what are the permissions associated to run the exp). By itself, it doesn't influence how and where an exp can be used.One could remove these fields if needed (of course along w/ the crd validation) and store these manifests if desired.","title":"What is the function of spec.definition.scope of a ChaosExperiment CR?"},{"location":"experiments/faq/experiments/#in-pod-network-latency-i-have-pod-a-talking-to-pod-b-over-service-b-and-i-want-to-introduce-latency-between-pod-a-and-service-b-what-would-go-into-specappinfo-section-pod-a-namespace-label-selector-and-kind-what-will-go-into-destination_ip-and-destination_host-service-b-details-what-are-the-target_pods","text":"It will target the [1:total_replicas] (based on PODS_AFFECTED_PERC) numbers of random pods with matching labels(appinfo.applabel) and namespace(appinfo.appns). But if you want to target a specific pod then you can provide their names as a comma separated list inside TARGET_PODS . Yes, you can provide service B details inside DESTINATION_IPS or DESTINATION_HOSTS . The NETWORK_INTERFACE should be eth0 .","title":"In Pod network latency - I have pod A talking to Pod B over Service B. and I want to introduce latency between Pod A and Service B. What would go into spec.appInfo section? Pod A namespace, label selector and kind? What will go into DESTINATION_IP and DESTINATION_HOST? Service B details? What are the TARGET_PODS?"},{"location":"experiments/faq/experiments/#how-to-check-the-network_interface-and-socket_path-variable","text":"The NETWORK_INTERFACE is the interface name inside the pod/container that needs to be targeted. You can find it by execing into the target pod and checking the available interfaces. You can try ip link , iwconfig , ifconfig depending on the tools installed in the pod either of those could work. The SOCKET_PATH by default takes the docker socket path. If you are using something else like containerd, crio or have a different socket path by any chance you can specify it. This is required to communicate with the container runtime of your cluster. In addition to this if container-runtime is different then provide the name of container runtime inside CONTAINER_RUNTIME ENV. It supports docker , containerd , and crio runtimes.","title":"How to check the NETWORK_INTERFACE and SOCKET_PATH variable?"},{"location":"experiments/faq/experiments/#what-are-the-different-ways-to-target-the-pods-and-nodes-for-chaos","text":"The different ways are: Pod Chaos: - Appinfo : Provide the target pod labels in the chaos engine appinfo section. - TARGET_PODS : You can provide the target pod names as a Comma Separated Variable. Like pod1,pod2. Node Chaos: - TARGET_NODE or TARGET_NODES : Provide the target node or nodes in these envs. - NODE_LABEL : Provide the label of the target nodes.","title":"What are the different ways to target the pods and nodes for chaos?"},{"location":"experiments/faq/experiments/#does-the-pod-affected-percentage-select-the-random-set-of-pods-from-the-total-pods-under-chaos","text":"Yes, it selects the random pods based on the POD_AFFACTED_PERC ENV. In pod-delete experiment it selects random pods for each iterations of chaos. But for rest of the experiments(if it supports iterations) then it will select random pods once and use the same set of pods for remaining iterations.","title":"Does the pod affected percentage select the random set of pods from the total pods under chaos?"},{"location":"experiments/faq/experiments/#how-to-extract-the-chaos-start-time-and-end-time","text":"We can use the Chaos exporter metrics for the same. One can also visualise these events along with time in chaos engine events.","title":"How to extract the chaos start time and end time?"},{"location":"experiments/faq/experiments/#how-do-we-check-the-mttr-mean-time-to-recovery-for-an-application-post-chaos","text":"The MTTR can be validated by using statusCheck Timeout in the chaos engine. By default its value will be 180 seconds. We can also overwrite this using ChaosEngine. For more details refer this","title":"How do we check the MTTR (Mean time to recovery) for an application post chaos?"},{"location":"experiments/faq/experiments/#what-is-the-difference-between-ramp-time-and-chaos-interval","text":"The ramp time is the time duration to wait before and after injection of chaos in seconds. While the chaos interval is the time interval (in second) between successive chaos iterations.","title":"What is the difference between Ramp Time and Chaos Interval?"},{"location":"experiments/faq/experiments/#when-im-executing-an-experiment-the-experiments-pod-failed-with-the-exec-format-error","text":"View the error message standard_init_linux.go:211: exec user process caused \"exec format error\": There could be multiple reasons for this. The most common one is mismatched in the binary and the platform on which it is running, try to check out the image binary you're using should have the support for the platform on which you\u2019re trying to run the experiment.","title":"When I\u2019m executing an experiment the experiment's pod failed with the exec format error"},{"location":"experiments/faq/install/","text":"","title":"Install"},{"location":"experiments/faq/portal/","text":"","title":"Portal"},{"location":"experiments/faq/security/","text":"","title":"Security"},{"location":"experiments/troubleshooting/experiments/","text":"Troubleshooting Litmus \u00b6 Table of Contents \u00b6 The Litmus chaos operator is seen to be in CrashLoopBackOff state immediately after installation? Nothing happens (no pods created) when the chaosengine resource is created? The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? The experiment pod enters completed state w/o the desired chaos being injected? Scheduler not forming chaosengines for type-repeat? Litmus uninstallation is not successful and namespace is stuck in terminating state? Observing experiment results using describe chaosresult is showing NotFound error? The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation? \u00b6 Verify if the ChaosEngine custom resource definition (CRD) has been installed in the cluster. This can be verified with the following commands: kubectl get crds | grep chaos kubectl api-resources | grep chaos If not created, install it from here Nothing happens (no pods created) when the chaosengine resource is created? \u00b6 If the ChaosEngine creation results in no action at all, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Specifically look for the event reason ChaosResourcesOperationFailed . Typically, these events consist of messages pointing to the problem. Some of the common messages include: Unable to filter app by specified info Unable to get chaos resources Unable to update chaosengine Check the logs of the chaos-operator pod using the following command to get more details (on failed creation of chaos resources). The below example uses litmus namespace, which is the default mode of installation. Please provide the namespace into which the operator has been deployed: kubectl logs -f <chaos-operator-(hash)-(hash)>-runner -n litmus Some of the possible reasons for these errors include: The annotationCheck is set to true in the ChaosEngine spec, but the application deployment (AUT) has not been annotated for chaos. If so, please add it using the following command: kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos=\"true\" The annotationCheck is set to true in the ChaosEngine spec and there are multiple chaos candidates that share the same label (as provided in the .spec.appinfo of the ChaosEngine) and are also annotated for chaos. If so, please provide a unique label for the AUT, or remove annotations on other applications with the same label. Litmus, by default, doesn't allow selection of multiple applications. If this is a requirement, set the annotationCheck to false . kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos- - The ChaosEngine has the .spec.engineState set to stop , which causes the operator to refrain from creating chaos resources. While it is an unlikely scenario, it is possible to reuse a previously modified ChaosEngine manifest. Verify if the service account used by the Litmus ChaosOperator has enough permissions to launch pods/services (this is available by default if the manifests suggested by the docs have been used). The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? \u00b6 If the chaos-runner enters completed state immediately post creation, i.e., the creation of experiment resources is unsuccessful, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for one of these events: ExperimentNotFound , ExperimentDependencyCheck , EnvParseError Check the logs of the chaos-runner pod logs. kubectl logs -f <chaosengine_name>-runner -n <namespace> Some of the possible reasons may include: The ChaosExperiment CR for the experiment (name) specified in the ChaosEngine .spec.experiments list is not installed. If so, please install the desired experiment from the chaoshub The dependent resources for the ChaosExperiment, such as ConfigMap & secret volumes (as specified in the ChaosExperiment CR or the ChaosEngine CR) may not be present in the cluster (or in the desired namespace). The runner pod doesn\u2019t proceed with creation of experiment resources if the dependencies are unavailable. The values provided for the ENV variables in the ChaosExperiment or the ChaosEngines might be invalid The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment resources (For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs). The experiment pod enters completed state w/o the desired chaos being injected? \u00b6 If the experiment pod enters completed state immediately (or in a few seconds) after creation w/o injecting the desired chaos, perform the following checks: Check the Kubernetes events generated against the ChaosEngine resource kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for the event with reason Summary with message experiment has been failed Check the logs of the chaos-experiment pod. kubectl logs -f <experiment_name_(hash)_(hash)> -n <namespace> Some of the possible reasons may include: The ChaosExperiment CR or the ChaosEngine CR doesn\u2019t include mandatory ENVs (or consists of incorrect values/info) needed by the experiment. Note that each experiment (see docs) specifies a mandatory set of ENVs along with some optional ones, which are necessary for successful execution of the experiment. The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment helper-resources (i.e., some experiments in turn create other K8s resources like Jobs/Daemonsets/Deployments etc.., For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs). The application's (AUT) unique label provided in the ChaosEngine is set only at the parent resource metadata but not propagated to the pod template spec. Note that the Operator uses this label to filter chaos candidates at the parent resource level (deployment/statefulset/daemonset) but the experiment pod uses this to pick application pods into which the chaos is injected. The experiment pre-chaos checks have failed on account of application (AUT) or auxiliary application unavailability Scheduler not forming chaosengines for type=repeat? \u00b6 If the ChaosSchedule has been created successfully created in the cluster and ChaosEngine is not being formed, the most common problem is that either start or end time has been wrongly specified. We should verify the times. We can identify if this is the problem or not by changing to type=now . If the ChaosEngine is formed successfully then the problem is with the specified time ranges, if ChaosEngine is still not formed, then the problem is with engineSpec . Litmus uninstallation is not successful and namespace is stuck in terminating state? \u00b6 Under typical operating conditions, the ChaosOperator makes use of finalizers to ensure that the ChaosEngine is deleted only after chaos resources (chaos-runner, experiment pod, any other helper pods) are removed. When uninstalling Litmus via the operator manifest (which contains the namespace, operator, crd specifictions in a single YAML) without deleting the existing chaosengine resources first, the ChaosOperator deployment may get deleted before the CRD removal is attempted. Since the stale chaosengines have the finalizer present on them, their deletion (triggered by the CRD delete) and by consequence, the deletion of the chaosengine CRD itself is \"stuck\". In such cases, manually remove the finalizer entries on the stale chaosengines to facilitate their successful delete. To get the chaosengine, run: kubectl get chaosengine -n <namespace> followed by: kubectl edit chaosengine <chaosengine-name> -n <namespace> and remove the finalizer entry chaosengine.litmuschaos.io/finalizer Repeat this on all the stale chaosengine CRs to remove the CRDs successfully & complete uninstallation process. If however, the litmus namespace deletion remains stuck despite the above actions, follow the procedure described here to complete the uninstallation. Observing experiment results using describe chaosresult is showing NotFound error? \u00b6 Upon observing the ChaosResults by executing the describe command given below, it may give a NotFound error. kubectl describe chaosresult <chaos-engine-name>-<chaos-experiment-name> -n <namespace> Alternatively, running the describe command without specifying the expected ChaosResult name might execute successfully, but does may not show any output. kubectl describe chaosresult -n <namespace>` This can occur sometimes due to the time taken in pulling the image starting the experiment pod (note that the ChaosResult resource is generated by the experiment). For the above commands to execute successfully, you should simply wait for the experiment pod to be created. The waiting time will be based upon resource available (network bandwidth, space availability on the node filesyste","title":"Experiments"},{"location":"experiments/troubleshooting/experiments/#troubleshooting-litmus","text":"","title":"Troubleshooting Litmus"},{"location":"experiments/troubleshooting/experiments/#table-of-contents","text":"The Litmus chaos operator is seen to be in CrashLoopBackOff state immediately after installation? Nothing happens (no pods created) when the chaosengine resource is created? The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created? The experiment pod enters completed state w/o the desired chaos being injected? Scheduler not forming chaosengines for type-repeat? Litmus uninstallation is not successful and namespace is stuck in terminating state? Observing experiment results using describe chaosresult is showing NotFound error?","title":"Table of Contents"},{"location":"experiments/troubleshooting/experiments/#the-litmus-chaosoperator-is-seen-to-be-in-crashloopbackoff-state-immediately-after-installation","text":"Verify if the ChaosEngine custom resource definition (CRD) has been installed in the cluster. This can be verified with the following commands: kubectl get crds | grep chaos kubectl api-resources | grep chaos If not created, install it from here","title":"The Litmus ChaosOperator is seen to be in CrashLoopBackOff state immediately after installation?"},{"location":"experiments/troubleshooting/experiments/#nothing-happens-no-pods-created-when-the-chaosengine-resource-is-created","text":"If the ChaosEngine creation results in no action at all, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Specifically look for the event reason ChaosResourcesOperationFailed . Typically, these events consist of messages pointing to the problem. Some of the common messages include: Unable to filter app by specified info Unable to get chaos resources Unable to update chaosengine Check the logs of the chaos-operator pod using the following command to get more details (on failed creation of chaos resources). The below example uses litmus namespace, which is the default mode of installation. Please provide the namespace into which the operator has been deployed: kubectl logs -f <chaos-operator-(hash)-(hash)>-runner -n litmus Some of the possible reasons for these errors include: The annotationCheck is set to true in the ChaosEngine spec, but the application deployment (AUT) has not been annotated for chaos. If so, please add it using the following command: kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos=\"true\" The annotationCheck is set to true in the ChaosEngine spec and there are multiple chaos candidates that share the same label (as provided in the .spec.appinfo of the ChaosEngine) and are also annotated for chaos. If so, please provide a unique label for the AUT, or remove annotations on other applications with the same label. Litmus, by default, doesn't allow selection of multiple applications. If this is a requirement, set the annotationCheck to false . kubectl annotate <deploy-type>/<application_name> litmuschaos.io/chaos- - The ChaosEngine has the .spec.engineState set to stop , which causes the operator to refrain from creating chaos resources. While it is an unlikely scenario, it is possible to reuse a previously modified ChaosEngine manifest. Verify if the service account used by the Litmus ChaosOperator has enough permissions to launch pods/services (this is available by default if the manifests suggested by the docs have been used).","title":"Nothing happens (no pods created) when the chaosengine resource is created?"},{"location":"experiments/troubleshooting/experiments/#the-chaos-runner-pod-enters-completed-state-seconds-after-getting-created-no-experiment-jobs-are-created","text":"If the chaos-runner enters completed state immediately post creation, i.e., the creation of experiment resources is unsuccessful, perform the following checks: Check the Kubernetes events generated against the chaosengine resource. kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for one of these events: ExperimentNotFound , ExperimentDependencyCheck , EnvParseError Check the logs of the chaos-runner pod logs. kubectl logs -f <chaosengine_name>-runner -n <namespace> Some of the possible reasons may include: The ChaosExperiment CR for the experiment (name) specified in the ChaosEngine .spec.experiments list is not installed. If so, please install the desired experiment from the chaoshub The dependent resources for the ChaosExperiment, such as ConfigMap & secret volumes (as specified in the ChaosExperiment CR or the ChaosEngine CR) may not be present in the cluster (or in the desired namespace). The runner pod doesn\u2019t proceed with creation of experiment resources if the dependencies are unavailable. The values provided for the ENV variables in the ChaosExperiment or the ChaosEngines might be invalid The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment resources (For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs).","title":"The chaos-runner pod enters completed state seconds after getting created. No experiment jobs are created?"},{"location":"experiments/troubleshooting/experiments/#the-experiment-pod-enters-completed-state-wo-the-desired-chaos-being-injected","text":"If the experiment pod enters completed state immediately (or in a few seconds) after creation w/o injecting the desired chaos, perform the following checks: Check the Kubernetes events generated against the ChaosEngine resource kubectl describe chaosengine <chaosengine-name> -n <namespace> Look for the event with reason Summary with message experiment has been failed Check the logs of the chaos-experiment pod. kubectl logs -f <experiment_name_(hash)_(hash)> -n <namespace> Some of the possible reasons may include: The ChaosExperiment CR or the ChaosEngine CR doesn\u2019t include mandatory ENVs (or consists of incorrect values/info) needed by the experiment. Note that each experiment (see docs) specifies a mandatory set of ENVs along with some optional ones, which are necessary for successful execution of the experiment. The chaosServiceAccount specified in the ChaosEngine CR doesn\u2019t have sufficient permissions to create the experiment helper-resources (i.e., some experiments in turn create other K8s resources like Jobs/Daemonsets/Deployments etc.., For existing experiments, appropriate rbac manifests are already provided in chaos-charts/docs). The application's (AUT) unique label provided in the ChaosEngine is set only at the parent resource metadata but not propagated to the pod template spec. Note that the Operator uses this label to filter chaos candidates at the parent resource level (deployment/statefulset/daemonset) but the experiment pod uses this to pick application pods into which the chaos is injected. The experiment pre-chaos checks have failed on account of application (AUT) or auxiliary application unavailability","title":"The experiment pod enters completed state w/o the desired chaos being injected?"},{"location":"experiments/troubleshooting/experiments/#scheduler-not-forming-chaosengines-for-typerepeat","text":"If the ChaosSchedule has been created successfully created in the cluster and ChaosEngine is not being formed, the most common problem is that either start or end time has been wrongly specified. We should verify the times. We can identify if this is the problem or not by changing to type=now . If the ChaosEngine is formed successfully then the problem is with the specified time ranges, if ChaosEngine is still not formed, then the problem is with engineSpec .","title":"Scheduler not forming chaosengines for type=repeat?"},{"location":"experiments/troubleshooting/experiments/#litmus-uninstallation-is-not-successful-and-namespace-is-stuck-in-terminating-state","text":"Under typical operating conditions, the ChaosOperator makes use of finalizers to ensure that the ChaosEngine is deleted only after chaos resources (chaos-runner, experiment pod, any other helper pods) are removed. When uninstalling Litmus via the operator manifest (which contains the namespace, operator, crd specifictions in a single YAML) without deleting the existing chaosengine resources first, the ChaosOperator deployment may get deleted before the CRD removal is attempted. Since the stale chaosengines have the finalizer present on them, their deletion (triggered by the CRD delete) and by consequence, the deletion of the chaosengine CRD itself is \"stuck\". In such cases, manually remove the finalizer entries on the stale chaosengines to facilitate their successful delete. To get the chaosengine, run: kubectl get chaosengine -n <namespace> followed by: kubectl edit chaosengine <chaosengine-name> -n <namespace> and remove the finalizer entry chaosengine.litmuschaos.io/finalizer Repeat this on all the stale chaosengine CRs to remove the CRDs successfully & complete uninstallation process. If however, the litmus namespace deletion remains stuck despite the above actions, follow the procedure described here to complete the uninstallation.","title":"Litmus uninstallation is not successful and namespace is stuck in terminating state?"},{"location":"experiments/troubleshooting/experiments/#observing-experiment-results-using-describe-chaosresult-is-showing-notfound-error","text":"Upon observing the ChaosResults by executing the describe command given below, it may give a NotFound error. kubectl describe chaosresult <chaos-engine-name>-<chaos-experiment-name> -n <namespace> Alternatively, running the describe command without specifying the expected ChaosResult name might execute successfully, but does may not show any output. kubectl describe chaosresult -n <namespace>` This can occur sometimes due to the time taken in pulling the image starting the experiment pod (note that the ChaosResult resource is generated by the experiment). For the above commands to execute successfully, you should simply wait for the experiment pod to be created. The waiting time will be based upon resource available (network bandwidth, space availability on the node filesyste","title":"Observing experiment results using describe chaosresult is showing NotFound error?"},{"location":"experiments/troubleshooting/install/","text":"","title":"Install"},{"location":"experiments/troubleshooting/portal/","text":"","title":"Portal"}]}